[
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/vi/",
	"title": "Báo cáo thực tập",
	"tags": [],
	"description": "",
	"content": "Báo cáo thực tập Thông tin sinh viên: Họ và tên: Lê Minh Tuấn\nSố điện thoại: 0981 500 154\nEmail: tuanlmse184475@fpt.edu.vn\nTrường: Đại học FPT TP.Hồ Chí Minh\nNgành: Trí tuệ nhân tạo\nCông ty thực tập: Công ty TNHH Amazon Web Services Vietnam\nVị trí thực tập: FCJ Cloud Intern\nThời gian thực tập: Từ ngày 08/09/2025 đến ngày 09/12/2025\nNội dung báo cáo Worklog Proposal Các bài blogs đã dịch Các events đã tham gia Workshop Tự đánh giá Chia sẻ, đóng góp ý kiến "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/vi/3-blogstranslated/3.1-blog1/",
	"title": "Blog 1",
	"tags": [],
	"description": "",
	"content": "Tùy chỉnh mô hình, RAG, hay cả hai: Nghiên cứu điển hình với Amazon Nova by Flora Wang, Anila Joshi, Baishali Chaudhury, Sungmin Hong, Jae Oh Woo, and Rahul Ghosh on 10 APR 2025 in Advanced (300), Amazon Bedrock, Amazon Machine Learning, Amazon Nova, Amazon SageMaker, Generative AI, Technical How-to\nKhi các doanh nghiệp và nhà phát triển ngày càng tìm cách tối ưu hóa các mô hình ngôn ngữ của họ cho các nhiệm vụ cụ thể, quyết định giữa tùy chỉnh mô hình và Retrieval Augmented Generation (RAG) trở nên quan trọng. Trong bài đăng này, chúng tôi tìm cách giải quyết nhu cầu ngày càng tăng này bằng cách đưa ra các hướng dẫn rõ ràng, có thể hành động và các phương pháp hay nhất về thời điểm sử dụng từng phương pháp, giúp bạn đưa ra quyết định sáng suốt phù hợp với các yêu cầu và mục tiêu riêng của mình.\nSự ra đời của các mô hình Amazon Nova thể hiện một bước tiến đáng kể trong lĩnh vực AI, mang đến những cơ hội mới để tối ưu hóa mô hình ngôn ngữ lớn (LLM). Trong bài đăng này, chúng tôi trình bày cách thực hiện tùy chỉnh mô hình và RAG một cách hiệu quả với các mô hình Amazon Nova làm cơ sở. Chúng tôi đã tiến hành nghiên cứu so sánh toàn diện giữa tùy chỉnh mô hình và RAG bằng cách sử dụng các mô hình Amazon Nova mới nhất và chia sẻ những thông tin chi tiết có giá trị này.\nTổng quan về cách tiếp cận và mô hình cơ sở Trong phần này, chúng tôi thảo luận về sự khác biệt giữa phương pháp tinh chỉnh và RAG, trình bày các trường hợp sử dụng phổ biến cho từng cách tiếp cận và cung cấp tổng quan về mô hình cơ sở được sử dụng cho các thử nghiệm.\nLàm sáng tỏ RAG và tùy chỉnh mô hình RAG là một kỹ thuật để nâng cao khả năng của các mô hình được đào tạo trước bằng cách cho phép mô hình truy cập vào các nguồn dữ liệu miền cụ thể bên ngoài. Nó kết hợp hai thành phần: truy xuất kiến thức bên ngoài và tạo phản hồi. Nó cho phép các mô hình ngôn ngữ được đào tạo trước kết hợp động dữ liệu bên ngoài trong quá trình tạo phản hồi, cho phép kết quả đầu ra được cập nhật và chính xác hơn theo ngữ cảnh. Không giống như tinh chỉnh, trong RAG, mô hình không trải qua bất kỳ khóa đào tạo nào và trọng số mô hình không được cập nhật để tìm hiểu kiến thức miền. Mặc dù tinh chỉnh ngầm sử dụng thông tin miền cụ thể bằng cách nhúng kiến thức cần thiết trực tiếp vào mô hình, RAG sử dụng rõ ràng thông tin miền cụ thể thông qua truy xuất bên ngoài.\nTùy chỉnh mô hình đề cập đến việc điều chỉnh mô hình ngôn ngữ được đào tạo trước để phù hợp hơn với các tác vụ, miền hoặc tập dữ liệu cụ thể. Tinh chỉnh là một trong những kỹ thuật như vậy, giúp đưa kiến thức cụ thể về nhiệm vụ hoặc lĩnh vực cụ thể để cải thiện hiệu suất mô hình. Nó điều chỉnh các thông số của mô hình để phù hợp hơn với các sắc thái của nhiệm vụ mục tiêu trong khi sử dụng kiến thức chung của nó.\nCác trường hợp sử dụng phổ biến cho từng cách tiếp cận RAG tối ưu cho các trường hợp sử dụng yêu cầu dữ liệu động hoặc cập nhật thường xuyên (chẳng hạn như Câu hỏi thường gặp về hỗ trợ khách hàng và danh mục thương mại điện tử), thông tin chi tiết về miền cụ thể (chẳng hạn như Hỏi \u0026amp; Đáp pháp lý hoặc y tế), các giải pháp có thể mở rộng cho các ứng dụng rộng (chẳng hạn như nền tảng phần mềm dưới dạng dịch vụ (SaaS)), truy xuất dữ liệu đa phương thức (chẳng hạn như tóm tắt tài liệu) và tuân thủ nghiêm ngặt dữ liệu an toàn hoặc nhạy cảm (chẳng hạn như hệ thống tài chính và quy định).\nNgược lại, tinh chỉnh phát triển mạnh trong các tình huống yêu cầu tùy chỉnh chính xác (chẳng hạn như chatbot được cá nhân hóa hoặc viết sáng tạo), độ chính xác cao cho các tác vụ hẹp (chẳng hạn như tạo mã hoặc tóm tắt chuyên biệt), độ trễ cực thấp (chẳng hạn như tương tác với khách hàng theo thời gian thực), ổn định với bộ dữ liệu tĩnh (chẳng hạn như bảng thuật ngữ dành riêng cho miền) và mở rộng quy mô hiệu quả về chi phí cho các tác vụ khối lượng lớn (chẳng hạn như tự động hóa trung tâm cuộc gọi).\nMặc dù RAG vượt trội về nền tảng thời gian thực trong dữ liệu bên ngoài và tinh chỉnh chuyên về quy trình làm việc tĩnh, có cấu trúc và được cá nhân hóa, nhưng việc lựa chọn giữa chúng thường phụ thuộc vào các yếu tố sắc thái. Bài đăng này cung cấp so sánh toàn diện về RAG và tinh chỉnh, làm rõ điểm mạnh, hạn chế và bối cảnh của chúng mà mỗi cách tiếp cận mang lại hiệu suất tốt nhất.\nGiới thiệu về các mẫu Amazon Nova Amazon Nova là một thế hệ mô hình nền tảng (FM) mới cung cấp thông tin tiên tiến và hiệu suất giá hàng đầu trong ngành. Amazon Nova Pro và Amazon Nova Lite là những mô hình đa phương thức vượt trội về độ chính xác và tốc độ, với Amazon Nova Lite được tối ưu hóa để xử lý nhanh và chi phí thấp. Amazon Nova Micro tập trung vào các tác vụ văn bản có độ trễ cực thấp. Chúng cung cấp khả năng suy luận nhanh, hỗ trợ quy trình làm việc tác nhân với Cơ sở tri thức Amazon Bedrock và RAG, đồng thời cho phép tinh chỉnh dữ liệu văn bản và đa phương thức. Được tối ưu hóa để có hiệu suất hiệu quả về chi phí, chúng được đào tạo trên dữ liệu bằng hơn 200 ngôn ngữ.\nTổng quan về giải pháp Để đánh giá hiệu quả của RAG so với tùy chỉnh mô hình, chúng tôi đã thiết kế một khung thử nghiệm toàn diện bằng cách sử dụng một tập hợp các câu hỏi dành riêng cho AWS. Nghiên cứu của chúng tôi đã sử dụng Amazon Nova Micro và Amazon Nova Lite làm FM cơ bản và kiểm tra hiệu suất của chúng trên các cấu hình khác nhau.\nChúng tôi đã cấu trúc đánh giá của mình như sau:\nMô hình cơ sở:\nAmazon Nova Micro và Amazon Nova Lite đã qua sử dụng\nCâu trả lời được tạo cho các câu hỏi dành riêng cho AWS mà không cần ngữ cảnh bổ sung\nMô hình cơ sở với RAG:\nKết nối các mô hình cơ sở với Cơ sở tri thức Amazon Bedrock\nĐược cung cấp quyền truy cập vào tài liệu và blog có liên quan của AWS\nTùy chỉnh mô hình:\nTinh chỉnh cả hai mô hình Amazon Nova bằng cách sử dụng 1.000 cặp câu hỏi-câu trả lời dành riêng cho AWS được tạo từ cùng một bộ bài viết AWS\nTriển khai các mô hình tùy chỉnh thông qua thông lượng được cung cấp\nTạo câu trả lời cho các câu hỏi dành riêng cho AWS với các mô hình được tinh chỉnh\nTùy chỉnh mô hình và cách tiếp cận kết hợp RAG:\nKết nối các mô hình tinh chỉnh với Cơ sở tri thức Amazon Bedrock\nCung cấp các mô hình tinh chỉnh quyền truy cập vào các bài viết AWS có liên quan tại thời điểm suy luận\nTrong các phần sau, chúng ta sẽ hướng dẫn cách thiết lập phương pháp tiếp cận thứ hai và thứ ba (mô hình cơ sở với RAG và tùy chỉnh mô hình với tinh chỉnh) trong Amazon Bedrock.\nĐiều kiện tiên quyết Để làm theo bài đăng này, bạn cần các điều kiện tiên quyết sau:\nTài khoản AWS và các quyền thích hợp\nVùng lưu trữ Amazon Simple Storage Service (Amazon S3) với hai thư mục: một thư mục chứa dữ liệu đào tạo của bạn và một thư mục cho đầu ra mô hình và chỉ số đào tạo của bạn\nTriển khai RAG với mô hình Amazon Nova cơ bản Trong phần này, chúng ta sẽ hướng dẫn các bước để triển khai RAG với mô hình cơ sở. Để làm như vậy, chúng tôi tạo ra một cơ sở tri thức. Hoàn thành các bước sau:\nTrên bảng điều khiển Amazon Bedrock, chọn Cơ sở kiến thức trong ngăn điều hướng.\nTrong Cơ sở kiến thức, chọn Tạo.\nTrên trang Đặt cấu hình nguồn dữ liệu, hãy cung cấp thông tin sau:\nChỉ định vị trí Amazon S3 của tài liệu.\nChỉ định chiến lược phân đoạn.\nChọn Tiếp theo.\nTrên trang Chọn mô hình nhúng và đặt cấu hình kho vectơ, hãy cung cấp thông tin sau:\nTrong phần Mô hình nhúng, chọn mô hình nhúng được sử dụng để nhúng các khối.\nTrong phần Cơ sở dữ liệu vectơ, hãy tạo một kho lưu trữ vectơ mới hoặc sử dụng kho lưu trữ vectơ hiện có, nơi các phần nhúng sẽ được lưu trữ để truy xuất.\nChọn Tiếp theo.\nTrên trang Xem lại và tạo, xem lại cài đặt và chọn Tạo cơ sở kiến thức. Tinh chỉnh mô hình Amazon Nova bằng API Amazon Bedrock Trong phần này, chúng tôi cung cấp hướng dẫn chi tiết về cách tinh chỉnh và lưu trữ các mô hình Amazon Nova tùy chỉnh bằng Amazon Bedrock. Sơ đồ sau đây minh họa kiến trúc giải pháp.\nTạo công việc tinh chỉnh Tinh chỉnh các mô hình Amazon Nova thông qua API Amazon Bedrock là một quy trình được sắp xếp hợp lý:\nTrên bảng điều khiển Amazon Bedrock, chọn us-east-1 làm Khu vực AWS của bạn.Tại thời điểm viết bài, tinh chỉnh mô hình Amazon Nova chỉ có sẵn ở us-east-1.\nChọn Mô hình tùy chỉnh trong Mô hình nền tảng trong ngăn điều hướng.\nTrong Phương pháp tùy chỉnh, chọn Tạo công việc tinh chỉnh.\nĐối với Mô hình nguồn, chọn Chọn mô hình.\nChọn Amazon làm nhà cung cấp và mô hình Amazon Nova mà bạn chọn.\nChọn Áp dụng.\nĐối với Tên kiểu máy được tinh chỉnh, hãy nhập tên duy nhất cho kiểu máy được tinh chỉnh.\nĐối với Tên công việc, nhập tên cho công việc tinh chỉnh.\nTrong Dữ liệu đầu vào, nhập vị trí của vùng lưu trữ S3 nguồn (dữ liệu đào tạo) và vùng lưu trữ S3 đích (đầu ra mô hình và chỉ số đào tạo) và tùy chọn vị trí của tập dữ liệu xác thực của bạn.\nĐịnh cấu hình siêu tham số Đối với các mô hình Amazon Nova, bạn có thể tùy chỉnh các siêu tham số sau:\nThông số Phạm vi/Ràng buộc Kỷ nguyên 1–5 Kích thước lô Cố định ở mức 1 Tốc độ học tập 0.000001–0.0001 Các bước khởi động tốc độ học tập 0–100 Chuẩn bị tập dữ liệu để tương thích với các mô hình Amazon Nova Tương tự như các LLM khác, Amazon Nova yêu cầu các cặp hoàn thành lời nhắc, còn được gọi là cặp câu hỏi và câu trả lời (Q\u0026amp;A), để tinh chỉnh có giám sát (SFT). Tập dữ liệu này phải chứa các kết quả lý tưởng mà bạn muốn mô hình ngôn ngữ tạo ra cho các tác vụ hoặc lời nhắc cụ thể. Tham khảo Hướng dẫn chuẩn bị dữ liệu của bạn cho Amazon Nova về các phương pháp thực hành tốt nhất và định dạng ví dụ khi chuẩn bị bộ dữ liệu để tinh chỉnh các mô hình Amazon Nova.\nKiểm tra tinh chỉnh tình trạng công việc và hiện vật đào tạo\nSau khi bạn tạo công việc tinh chỉnh, hãy chọn Mô hình tùy chỉnh trong Mô hình nền tảng trong ngăn điều hướng. Bạn sẽ tìm thấy công việc tinh chỉnh hiện tại được liệt kê trong Công việc. Bạn có thể sử dụng trang này để theo dõi trạng thái công việc tinh chỉnh của mình.\nKhi trạng thái công việc tinh chỉnh của bạn thay đổi thành Hoàn thành, bạn có thể chọn tên công việc và điều hướng đến trang Tổng quan về công việc đào tạo. Bạn sẽ tìm thấy các thông tin sau:\nThông số kỹ thuật công việc đào tạo\nVị trí Amazon S3 cho dữ liệu đầu vào được sử dụng để tinh chỉnh\nSiêu tham số được sử dụng trong quá trình tinh chỉnh\nVị trí Amazon S3 cho đầu ra đào tạo\nLưu trữ mô hình tinh chỉnh với thông lượng được cung cấp Sau khi công việc tinh chỉnh hoàn tất thành công, bạn có thể truy cập mô hình tùy chỉnh của mình thông qua các bước sau:\nTrên bảng điều khiển Amazon Bedrock, chọn Mô hình tùy chỉnh trong Mô hình nền tảng trong ngăn điều hướng.\nTrong Mô hình, chọn mô hình tùy chỉnh của bạn.\nTrang chi tiết kiểu máy hiển thị các thông tin sau:\nChi tiết mô hình được tinh chỉnh\nVị trí Amazon S3 cho dữ liệu đầu vào được sử dụng để tinh chỉnh\nSiêu tham số được sử dụng trong quá trình tinh chỉnh\nVị trí Amazon S3 cho đầu ra đào tạo\nĐể cung cấp mô hình tinh chỉnh của bạn để suy luận, hãy chọn Mua thông lượng được cung cấp.\nChọn thời hạn cam kết (không cam kết, 1 tháng hoặc 6 tháng) và xem lại chi phí liên quan để lưu trữ các mô hình được tinh chỉnh.\nSau khi mô hình tùy chỉnh được lưu trữ thông qua thông lượng được cung cấp, ID mô hình sẽ được gán và có thể được sử dụng để suy luận.\nCác bước tinh chỉnh và suy luận nói trên cũng có thể được thực hiện theo chương trình. Để biết thêm thông tin, hãy tham khảo kho lưu trữ GitHub sau đây, trong đó chứa mã mẫu.\nKhung đánh giá và kết quả Trong phần này, trước tiên chúng tôi giới thiệu khung đánh giá nhiều thẩm phán LLM của chúng tôi, được thiết lập để giảm thiểu sự thiên vị của từng giám khảo LLM. Sau đó, chúng tôi so sánh RAG và kết quả tinh chỉnh về chất lượng phản hồi cũng như độ trễ và ý nghĩa của token.\nNhiều LLM làm thẩm phán để giảm thiểu thành kiến Sơ đồ sau đây minh họa quy trình làm việc của chúng tôi bằng cách sử dụng nhiều LLM làm giám khảo.\nSử dụng LLM làm giám khảo đã trở thành một cách tiếp cận ngày càng phổ biến để đánh giá các nhiệm vụ khó đánh giá thông qua các phương pháp truyền thống hoặc đánh giá của con người. Đối với khung đánh giá của mình, chúng tôi đã xây dựng 10 câu hỏi kiểm tra theo lĩnh vực cụ thể bao gồm các khía cạnh chính của các dịch vụ và tính năng AWS, được thiết kế để kiểm tra cả độ chính xác thực tế và chiều sâu hiểu biết. Mỗi câu trả lời do mô hình tạo ra được đánh giá bằng cách sử dụng hệ thống tính điểm tiêu chuẩn trên thang điểm 0–10, trong đó 0–3 biểu thị thông tin không chính xác hoặc gây hiểu lầm, 4–6 đại diện cho câu trả lời đúng một phần nhưng không đầy đủ, 7–8 biểu thị hầu hết đúng với những điểm không chính xác nhỏ và 9–10 biểu thị hoàn toàn chính xác với lời giải thích toàn diện.\nChúng tôi sử dụng lời nhắc đánh giá thẩm phán LLM sau:\n{\n\u0026quot;system\\_prompt\u0026quot;: \u0026quot;You are a helpful assistant.\u0026quot;, \u0026quot;prompt\\_template\u0026quot;: \u0026quot;\\[Instruction\\] Please act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the user question displayed below. Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of the response. Begin your evaluation by providing a short explanation. Be as objective as possible. After providing your explanation, you must rate the response on a scale of 1 to 10 by strictly following this format: \\\\\u0026quot;\\[\\[rating\\]\\]\\\\\u0026quot;, for example: \\\\\u0026quot;Rating: \\[\\[5\\]\\]\\\\\u0026quot;.\\\\n\\\\n\\[Question\\]\\\\n{question}\\\\n\\\\n\\[The Start of Assistant's Answer\\]\\\\n{answer}\\\\n\\[The End of Assistant's Answer\\]\u0026quot;, \u0026quot;description\u0026quot;: \u0026quot;Prompt for general questions\u0026quot;, \u0026quot;category\u0026quot;: \u0026quot;general\u0026quot;, \u0026quot;output\\_format\u0026quot;: \u0026quot;\\[\\[rating\\]\\]\u0026quot; }\nChúng tôi sử dụng câu hỏi đánh giá mẫu sau đây và sự thật cơ bản:\n{\n\u0026quot;question\\_id\u0026quot;: 9161, \u0026quot;category\u0026quot;: \u0026quot;AWS\u0026quot;, \u0026quot;turns\u0026quot;: \\[ \u0026quot; \\\\\u0026quot;What specific details are collected and sent to AWS when anonymous operational metrics are enabled for an Amazon EFS file system?\u0026quot;, \u0026quot;What's required for a successful AWS CloudFormation launch?\u0026quot; \\], \u0026quot;reference\u0026quot;: \\[ \u0026quot;When anonymous operational metrics are enabled for an Amazon EFS file system, the following specific details are collected and sent to AWS: Solution ID, Unique ID, Timestamp, Backup ID, Backup Start Time, Backup Stop Time, Backup Window, Source EFS Size, Destination EFS Size, Instance Type, Retain, S3 Bucket Size, Source Burst Credit Balance, Source Burst Credit Balance Post Backup, Source Performance Mode, Destination Performance Mode, Number of Files, Number of Files Transferred, Total File Size, Total Transferred File Size, Region, Create Hard Links Start Time, Create Hard Links Stop Time, Remove Snapshot Start Time, Remove Snapshot Stop Time, Rsync Delete Start Time, Rsync Delete Stop Time.\u0026quot;, \u0026quot;For a successful AWS CloudFormation launch, you need to sign in to the AWS Management Console, choose the correct AWS Region, use the button to launch the template, verify the correct template URL, assign a name to your solution stack, review and modify the parameters as necessary, review and confirm the settings, check the boxes acknowledging that the template creates AWS Identity and Access Management resources and may require an AWS CloudFormation capability, and choose Create stack to deploy the stack. You should receive a CREATE\\_COMPLETE status in approximately 15 minutes.\u0026quot; \\] }\nĐể giảm thiểu những thành kiến nội tại tiềm ẩn giữa các giám khảo LLM khác nhau, chúng tôi đã sử dụng hai giám khảo LLM để đánh giá các phản hồi do mô hình tạo ra: Claude Sonnet 3.5 của Anthropic và Llama 3.1 70B của Meta. Mỗi giám khảo được cung cấp câu hỏi kiểm tra ban đầu, câu trả lời do mô hình tạo ra và các tiêu chí chấm điểm cụ thể tập trung vào độ chính xác, đầy đủ, liên quan và rõ ràng thực tế. Nhìn chung, chúng tôi quan sát thấy mức độ tương quan xếp hạng cao giữa các giám khảo LLM trong việc đánh giá các cách tiếp cận khác nhau, với các mô hình đánh giá nhất quán trên tất cả các trường hợp thử nghiệm.\nSo sánh chất lượng phản hồi Cả tinh chỉnh và RAG đều cải thiện đáng kể chất lượng câu trả lời được tạo cho các câu hỏi dành riêng cho AWS so với mô hình cơ sở. Sử dụng Amazon Nova Lite làm mô hình cơ sở, chúng tôi quan sát thấy rằng cả tinh chỉnh và RAG đều cải thiện 30% điểm trung bình của giám khảo LLM về chất lượng phản hồi, trong khi kết hợp tinh chỉnh với RAG đã cải thiện tổng cộng 83% chất lượng phản hồi, như thể hiện trong hình sau.\nĐáng chú ý, đánh giá của chúng tôi đã tiết lộ một phát hiện thú vị (như thể hiện trong hình sau): khi kết hợp các phương pháp tinh chỉnh và RAG, các mô hình nhỏ hơn như Amazon Nova Micro cho thấy những cải thiện đáng kể về hiệu suất trong các tác vụ theo miền cụ thể, gần như phù hợp với hiệu suất của các mô hình lớn hơn. Điều này cho thấy rằng đối với các trường hợp sử dụng chuyên biệt với phạm vi được xác định rõ ràng, sử dụng các mô hình nhỏ hơn với cả tinh chỉnh và RAG có thể là một giải pháp hiệu quả hơn về chi phí so với việc triển khai các mô hình lớn hơn.\nĐộ trễ và ý nghĩa của token Ngoài việc nâng cao chất lượng phản hồi, cả tinh chỉnh và RAG đều giúp giảm độ trễ tạo phản hồi so với mô hình cơ sở. Đối với cả Amazon Nova Micro và Amazon Nova Lite, tinh chỉnh đã giảm độ trễ của mô hình cơ sở khoảng 50%, trong khi RAG giảm khoảng 30%, như thể hiện trong hình sau.\nTinh chỉnh cũng thể hiện lợi thế độc đáo của việc cải thiện giọng điệu và phong cách của các câu trả lời được tạo để phù hợp hơn với dữ liệu đào tạo. Trong các thử nghiệm của chúng tôi, tổng số token trung bình (token đầu vào và đầu ra) giảm hơn 60% với cả hai mô hình được tinh chỉnh. Tuy nhiên, tổng số token trung bình tăng hơn gấp đôi với cách tiếp cận RAG do chuyển ngữ cảnh, như thể hiện trong hình sau. Phát hiện này cho thấy rằng đối với các trường hợp sử dụng nhạy cảm với độ trễ hoặc khi mục tiêu là điều chỉnh phản hồi của mô hình theo giọng điệu, phong cách hoặc giọng nói thương hiệu cụ thể, tùy chỉnh mô hình có thể mang lại nhiều giá trị kinh doanh hơn.\nKết thúc Trong bài đăng này, chúng tôi đã so sánh tùy chỉnh mô hình (tinh chỉnh) và RAG cho các tác vụ theo miền cụ thể với Amazon Nova. Trước tiên, chúng tôi cung cấp hướng dẫn chi tiết về cách tinh chỉnh, lưu trữ và tiến hành suy luận với Amazon Nova tùy chỉnh thông qua API Amazon Bedrock. Sau đó, chúng tôi áp dụng phương pháp LLM-as-a-judge để đánh giá chất lượng phản hồi từ các cách tiếp cận khác nhau. Ngoài ra, chúng tôi đã kiểm tra độ trễ và ý nghĩa mã thông báo của các thiết lập khác nhau.\nCả tinh chỉnh và RAG đều cải thiện hiệu suất của mô hình. Tùy thuộc vào nhiệm vụ và tiêu chí đánh giá, tùy chỉnh mô hình cho thấy hiệu suất tương tự hoặc đôi khi tốt hơn so với RAG. Tùy chỉnh mô hình cũng có thể hữu ích để cải thiện phong cách và giọng điệu của câu trả lời được tạo. Trong thử nghiệm này, phản hồi của mô hình tùy chỉnh tuân theo kiểu câu trả lời ngắn gọn của dữ liệu đào tạo đã cho, dẫn đến độ trễ thấp hơn so với đối tác cơ sở. Ngoài ra, tùy chỉnh mô hình cũng có thể được sử dụng cho nhiều trường hợp sử dụng mà RAG không đơn giản để sử dụng, chẳng hạn như gọi công cụ, phân tích cảm xúc, trích xuất thực thể, v.v. Nhìn chung, chúng tôi khuyên bạn nên kết hợp tùy chỉnh mô hình và RAG để trả lời câu hỏi hoặc các tác vụ tương tự để tối đa hóa hiệu suất.\nĐể biết thêm thông tin về Amazon Bedrock và các mẫu Amazon Nova mới nhất, hãy tham khảo Hướng dẫn sử dụng Amazon Bedrock và Hướng dẫn sử dụng Amazon Nova. Trung tâm đổi mới AI tổng quát AWS có một nhóm các chuyên gia khoa học và chiến lược AWS có chuyên môn toàn diện trong hành trình AI tổng quát, giúp khách hàng ưu tiên các trường hợp sử dụng, xây dựng lộ trình và đưa các giải pháp vào sản xuất. Hãy xem Trung tâm Đổi mới AI Tổng quát để biết những câu chuyện thành công mới nhất của chúng tôi về công việc và khách hàng.\nGiới thiệu về các tác giả\nẢnh đại diện Giới thiệu về các tác giả Mengdie (Flora) Wang là Nhà khoa học dữ liệu tại Trung tâm đổi mới AI tổng quát AWS, nơi cô làm việc với khách hàng để kiến trúc và triển khai các giải pháp AI tổng quát có thể mở rộng nhằm giải quyết những thách thức kinh doanh riêng của họ. Cô chuyên về các kỹ thuật tùy chỉnh mô hình và hệ thống AI dựa trên tác nhân, giúp các tổ chức khai thác toàn bộ tiềm năng của công nghệ AI tổng quát. Trước khi gia nhập AWS, Flora lấy bằng Thạc sĩ Khoa học Máy tính tại Đại học Minnesota, nơi cô phát triển chuyên môn về máy học và trí tuệ nhân tạo. Sungmin Hong là Nhà khoa học ứng dụng cấp cao tại Trung tâm đổi mới AI tổng quát của Amazon, nơi ông giúp đẩy nhanh sự đa dạng của các trường hợp sử dụng của khách hàng AWS. Trước khi gia nhập Amazon, Sungmin là nghiên cứu sinh sau tiến sĩ tại Trường Y Harvard. Ông có bằng Tiến sĩ Khoa học Máy tính tại Đại học New York. Ngoài công việc, anh ấy tự hào về việc giữ cho cây trồng trong nhà của mình tồn tại trong 3+ năm. Jae Oh Woo là Nhà khoa học ứng dụng cấp cao tại Trung tâm đổi mới AI tổng quát AWS, nơi anh chuyên phát triển các giải pháp tùy chỉnh và tùy chỉnh mô hình cho nhiều trường hợp sử dụng khác nhau. Ông có niềm đam mê mãnh liệt đối với nghiên cứu liên ngành kết nối nền tảng lý thuyết với các ứng dụng thực tế trong lĩnh vực AI tổng quát đang phát triển nhanh chóng. Trước khi gia nhập Amazon, Jae Oh là nghiên cứu sinh sau tiến sĩ Simons tại Đại học Texas ở Austin, nơi ông tiến hành nghiên cứu trên các khoa Toán học và Kỹ thuật Điện và Máy tính. Ông có bằng Tiến sĩ về Toán ứng dụng tại Đại học Yale. Rahul Ghosh là Nhà khoa học ứng dụng tại Trung tâm đổi mới AI tổng quát của Amazon, nơi ông làm việc với khách hàng AWS trên các ngành dọc khác nhau để đẩy nhanh việc sử dụng AI tổng quát. Rahul có bằng Tiến sĩ Khoa học Máy tính tại Đại học Minnesota. Baishali Chaudhury là Nhà khoa học ứng dụng tại Trung tâm Đổi mới AI Tổng quát tại AWS, nơi cô tập trung vào việc thúc đẩy các giải pháp AI tổng quát cho các ứng dụng trong thế giới thực. Cô có nền tảng vững chắc về thị giác máy tính, học máy và AI cho chăm sóc sức khỏe. Baishali có bằng Tiến sĩ Khoa học Máy tính của Đại học Nam Florida và PostDoc của Trung tâm Ung thư Moffitt. Anila Joshi có hơn một thập kỷ kinh nghiệm xây dựng các giải pháp AI. Với tư cách là Nhà lãnh đạo địa lý AWSI tại Trung tâm đổi mới AI tổng quát AWS, Anila đi tiên phong trong các ứng dụng sáng tạo của AI nhằm vượt qua ranh giới khả năng và đẩy nhanh việc áp dụng các dịch vụ AWS với khách hàng bằng cách giúp khách hàng lên ý tưởng, xác định và triển khai các giải pháp AI tổng quát bảo mật. "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/vi/3-blogstranslated/3.2-blog2/",
	"title": "Blog 2",
	"tags": [],
	"description": "",
	"content": "Di chuyển ứng dụng CDK phiên bản 1 sang CDK phiên bản 2 với Amazon Q Developer bởi Tiến sĩ Rahul Sharad Gaikwad, Tamilselvan P và Vinodkumar Mandalapu vào ngày 30 tháng 4 năm 2025 trên Amazon Q.\nGiới thiệu: Bộ công cụ phát triển đám mây AWS (AWS CDK) là một khung phát triển phần mềm mã nguồn mở để xác định cơ sở hạ tầng đám mây trong mã và cung cấp thông qua AWS CloudFormation. Kể từ ngày 1 tháng 6 năm 2023, AWS CDK phiên bản 1 không còn được hỗ trợ. Để tránh các vấn đề tiềm ẩn khi sử dụng phiên bản lỗi thời và tận dụng các tính năng và cải tiến mới nhất, chúng tôi khuyên bạn nên nâng cấp lên AWS CDK phiên bản 2.\nAmazon Q Developer, một trợ lý tổng quát được hỗ trợ bởi AI để phát triển phần mềm, nâng cao hiệu quả của các nhóm phát triển phần mềm. Nó tạo điều kiện thuận lợi cho việc tạo cơ sở hạ tầng sẵn sàng triển khai dưới dạng mã (IaC) cho AWS CloudFormation, AWS CDK và Terraform. Bằng cách sử dụng Amazon Q, các nhà phát triển có thể tăng tốc phát triển IaC, nâng cao chất lượng mã và giảm khả năng xảy ra lỗi cấu hình.\nBài đăng này trình bày cách Amazon Q Developer hỗ trợ nâng cấp ứng dụng AWS CDK v1 hiện có lên AWS CDK v2.\nĐiều kiện tiên quyết ID AWS Builder hoặc thông tin đăng nhập Trung tâm nhận dạng AWS IAM do tổ chức của bạn kiểm soát\nIDE được hỗ trợ, chẳng hạn như Visual Studio Code\nTiện ích mở rộng IDE của Bộ công cụ AWS\nXác thực và kết nối\nNodejs\nAWS CDK phiên bản 1\nAWS CDK phiên bản 2\nKế hoạch Trong bài đăng trên blog này, tôi sẽ khám phá một ví dụ về mã mà tôi đã tạo VPC, Mạng con và cụm ECS Fargate bằng AWS CDK phiên bản 1. Sau đó, tôi sẽ giải thích cách bạn có thể sử dụng Amazon Q để chuyển đổi mã từ CDK v1 sang CDK v2.\n1. Để bắt đầu quá trình này, tôi đã bắt đầu bằng cách yêu cầu Nhà phát triển Amazon Q cung cấp các bước cần thiết để di chuyển từ CDK phiên bản 1 sang phiên bản 2, được nêu dưới đây.\nCan you provide the steps to migrate from cdk version 1 to version 2?\n2. Trong ảnh chụp màn hình trên, Amazon Q Developer đã phác thảo một số bước chúng tôi có thể thực hiện để thực hiện các thay đổi cần thiết. Bước đầu tiên là cập nhật các phần phụ thuộc. Nếu tôi cần hướng dẫn về cách cập nhật các phần phụ thuộc, tôi có thể yêu cầu Nhà phát triển Amazon Q trợ giúp một lần nữa bằng cách yêu cầu các bước liên quan đến cập nhật các phần phụ thuộc như bên dưới.\nCan you provide the steps to update dependencies?\n3. Sau khi cập nhật các phần phụ thuộc, bước tiếp theo là cập nhật các câu lệnh nhập. Để được hướng dẫn về cách cập nhật các câu lệnh nhập, tôi có thể yêu cầu trợ lý nhà phát triển Amazon Q trợ giúp một lần nữa bằng cách hỏi các bước liên quan đến cách nhập các câu lệnh như hình dưới đây.\n@workspace Can you provide the steps to update import statements?\nTrong ảnh chụp màn hình ở trên, nếu bạn nhận thấy, tôi đã thêm trước câu hỏi tự động bao gồm các phần có liên quan nhất của mã không gian làm việc của tôi dưới dạng ngữ cảnh.@workspace\n4. Nếu có bất kỳ lỗi nào xảy ra trong khi cập nhật mã theo khuyến nghị của Amazon Q Developer, tôi có thể sử dụng Amazon Q Developer để gỡ lỗi sự cố và cung cấp thông tin đầu vào cần thiết để giải quyết.\n5. Sau khi hoàn thành các bước bắt buộc, tôi có thể triển khai ứng dụng bằng phiên bản 2 của AWS CDK bằng cách chạy lệnh.cdk deploy\n6. Ngoài các khả năng khác, Amazon Q còn cung cấp chức năng xem xét mã. Để bắt đầu xem xét mã, chỉ cần chọn Amazon Q và sử dụng lệnh. Sau đó, tôi sẽ có tùy chọn để xem lại các tệp đang hoạt động hoặc toàn bộ không gian làm việc đang mở. Chọn tùy chọn của bạn và Amazon Q sẽ phân tích dự án của bạn và cung cấp kết quả đánh giá toàn diện./review\n7. Amazon Q Developer cũng có thể tạo tài liệu, bao gồm các tệp README. Để tạo tài liệu, hãy chọn Amazon Q và nhập lệnh. Amazon Q sẽ tự động tạo tệp README cho dự án của bạn. Sau đó, tôi có thể xem lại tài liệu đã tạo, chấp nhận các thay đổi hoặc cung cấp hướng dẫn cụ thể để sửa đổi thêm./doc\nKết thúc Trong blog này, tôi đã trình bày cách Amazon Q Developer có thể đơn giản hóa và đẩy nhanh quá trình nâng cấp từ AWS CDK phiên bản 1 lên phiên bản 2, đảm bảo cơ sở hạ tầng đám mây của bạn luôn bảo mật, hiệu quả và phù hợp với những cải tiến mới nhất của AWS. AWS CDK phiên bản 2 cung cấp một thư viện hợp nhất, hợp nhất được sắp xếp hợp lý với hiệu suất được cải thiện và hỗ trợ liên tục, giúp việc quản lý cơ sở hạ tầng trở nên dễ dàng và đáng tin cậy hơn.\nBằng cách tận dụng Amazon Q Developer, một trợ lý tổng quát được hỗ trợ bởi AI, các nhóm có thể tự động hóa việc phát triển Cơ sở hạ tầng dưới dạng mã, nâng cao chất lượng mã và giảm thiểu lỗi cấu hình. Cùng với nhau, các công cụ này hỗ trợ các nhóm phát triển tự tin hiện đại hóa và mở rộng quy mô môi trường AWS của họ, biến quá trình nâng cấp thành cơ hội liền mạch để đổi mới và tăng trưởng.\nTài nguyên Để tìm hiểu thêm về Amazon Q Developer, hãy xem các tài nguyên sau:\nHội thảo dành cho nhà phát triển Amazon Q\nHướng dẫn sử dụng nhà phát triển Amazon Q\nĐể tìm hiểu thêm về AWS CDK, hãy xem các tài nguyên sau:\nHội thảo AWS CDK\nCách sử dụng Amazon Q Developer để triển khai ứng dụng web phi máy chủ với AWS CDK\nGiới thiệu về các tác giả:\nẢnh đại diện Giới thiệu về các tác giả Tiến sĩ Rahul Sharad Gaikwad là Kiến trúc sư giải pháp tại AWS, thúc đẩy đổi mới đám mây thông qua việc di chuyển và hiện đại hóa khối lượng công việc của khách hàng. Là một người đam mê Generative AI và DevOps, anh ấy kiến trúc các giải pháp tiên tiến và được công nhận là Đại sứ APJC HashiCorp. Ông lấy bằng tiến sĩ về AIOps và ông đã nhận được Giải thưởng Người đàn ông xuất sắc, Giải thưởng Người thành tựu Ấn Độ, Giải thưởng Luận án Tiến sĩ Xuất sắc nhất, Giải thưởng Học giả Nghiên cứu của Năm và Giải thưởng Nhà nghiên cứu trẻ. Vinodkumar Mandalapu là Chuyên gia tư vấn Devops tại AWS, chuyên thiết kế và triển khai cơ sở hạ tầng dựa trên đám mây và đường ống triển khai trên AWS. Với kinh nghiệm dày dặn trong việc tự động hóa và hợp lý hóa việc phân phối phần mềm, ông đã giúp các tổ chức thuộc mọi quy mô tận dụng sức mạnh của đám mây để thúc đẩy đổi mới, cải thiện khả năng mở rộng và nâng cao hiệu quả hoạt động. Trong thời gian rảnh rỗi, anh ấy thích đi du lịch và dành thời gian chất lượng cho con trai mình. Tamilselvan P là Chuyên gia tư vấn Devops tại AWS, tập trung vào việc kiến trúc và triển khai các hệ thống gốc đám mây cũng như phân phối liên tục trong hệ sinh thái. Tận dụng chuyên môn toàn diện của mình trong việc điều phối và tinh chỉnh các quy trình phát hành phần mềm, ông đã hỗ trợ khách hàng trong nhiều ngành và quy mô khác nhau trong việc khai thác công nghệ đám mây để đổi mới nhanh hơn, tăng khả năng mở rộng và nâng cao hiệu suất hoạt động. Trong thời gian rảnh rỗi, anh ấy thích chơi cricket. "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/vi/3-blogstranslated/3.3-blog3/",
	"title": "Blog 3",
	"tags": [],
	"description": "",
	"content": "Melting The Ice - Cách Natural Intelligence đơn giản hóa việc chuyển đổi Data lake sang Apache Iceberg bởi Yonatan Dolan và Haya Axelrod Stern, Zion Rubin, Michal Urbanowicz vào 28 tháng 4 năm 2025 trong Advanced (300), Analytics, AWS Glue, Best Practices, Case Study, Technical How-to Permalink\nBài viết này được đồng tác giả bởi Haya Axelrod Stern, Zion Rubin và Michal Urbanowicz từ Natural Intelligence..\nNhiều tổ chức chọn data lake vì tính linh hoạt và khả năng mở rộng trong việc quản lý dữ liệu có cấu trúc và không cấu trúc. Tuy nhiên, việc di chuyển một data lake hiện hữu sang định dạng bảng mới như Apache Iceberg có thể gặp nhiều thách thức về kỹ thuật lẫn tổ chức.\nNatural Intelligence (NI) là một đơn vị hàng đầu trong lĩnh vực thị trường đa danh mục. Với các thương hiệu nổi bật như Top10.com và BestMoney.com, NI hỗ trợ hàng triệu người mỗi ngày trong việc đưa ra quyết định thông minh. Gần đây, NI đã khởi động hành trình chuyển đổi data lake truyền thống từ Apache Hive sang Apache Iceberg.\nTrong bài viết này, NI chia sẻ hành trình của họ, các giải pháp sáng tạo đã phát triển, và những bài học chính có thể hướng dẫn các tổ chức khác muốn thực hiện con đường tương tự. Nội dung tập trung nhiều vào thách thức thực tế và cách giải quyết trong quá trình chuyển đổi, hơn là các đặc tả kỹ thuật phức tạp của Apache Iceberg.\nTại sao chọn Apache Iceberg? Kiến trúc dữ liệu tại NI tuân theo mô hình Medallion Architecture (kiến trúc phân tầng đồng – bạc – vàng), được mô tả như sau:\nLớp Đồng (Bronze layer):\nDữ liệu thô chưa qua xử lý được thu thập từ nhiều nguồn khác nhau, lưu trữ ở định dạng gốc trong Amazon Simple Storage Service (Amazon S3) và được nạp thông qua Apache Kafka brokers.\nLớp Bạc (Silver layer):\nChứa dữ liệu đã được làm sạch và làm giàu (enriched data), được xử lý bằng Apache Flink.\nLớp Vàng (Gold layer):\nLưu trữ các tập dữ liệu sẵn sàng cho phân tích (analytics-ready datasets) được thiết kế cho Business Intelligence (BI) và báo cáo.\nDữ liệu ở lớp này được tạo ra thông qua các pipeline Apache Spark và được sử dụng bởi các dịch vụ như Snowflake, Amazon Athena, Tableau, và Apache Druid.\nDữ liệu được lưu ở định dạng Apache Parquet, với AWS Glue Catalog chịu trách nhiệm quản lý siêu dữ liệu (metadata management).\nMặc dù kiến trúc này đáp ứng được các nhu cầu phân tích dữ liệu của NI, nhưng nó thiếu tính linh hoạt cần thiết cho một nền tảng dữ liệu mở và có khả năng thích ứng thực sự.Lớp Gold chỉ có thể hoạt động với các công cụ truy vấn (query engines) hỗ trợ Hive và AWS Glue Data Catalog. Dù có thể sử dụng Amazon Athena, nhưng với Snowflake, NI phải duy trì một catalog riêng biệt để có thể truy vấn các bảng external (ngoại bảng). Vấn đề này khiến cho việc đánh giá hoặc áp dụng các công cụ và engine thay thế trở nên khó khăn — nếu không muốn nhân bản dữ liệu, viết lại truy vấn, hoặc đồng bộ lại catalog tốn kém chi phí. Khi quy mô kinh doanh mở rộng, NI cần một nền tảng dữ liệu có thể hỗ trợ đồng thời nhiều công cụ truy vấn khác nhau chỉ với một data catalog duy nhất, đồng thời tránh bị phụ thuộc vào bất kỳ nhà cung cấp nào (vendor lock-in).\nSức mạnh của Apache Iceberg Apache Iceberg nổi lên như một giải pháp hoàn hảo — một định dạng bảng mở, linh hoạt phù hợp với cách tiếp cận Data Lake First của NI. Iceberg cung cấp một số lợi thế quan trọng như giao dịch ACID, phát triển lược đồ, du hành thời gian, cải thiện hiệu suất và hơn thế nữa. Nhưng lợi ích chiến lược chính nằm ở khả năng hỗ trợ nhiều công cụ truy vấn đồng thời. Nó cũng có những ưu điểm sau:\nTách biệt giữa lưu trữ và xử lý (Decoupling of storage and compute): Định dạng bảng mở cho phép bạn tách lớp lưu trữ khỏi công cụ truy vấn, cho phép dễ dàng hoán đổi và hỗ trợ đồng thời nhiều công cụ mà không trùng lặp dữ liệu.\nĐộc lập với nhà cung cấp (Vendor independence) : Là một định dạng bảng mở, Apache Iceberg ngăn chặn khóa nhà cung cấp, giúp bạn linh hoạt thích ứng với nhu cầu phân tích thay đổi.\nĐược nhiều nền tảng hỗ trợ (Vendor adoption): Apache Iceberg được hỗ trợ rộng rãi bởi các nền tảng và công cụ chính, cung cấp khả năng tích hợp liền mạch và khả năng tương thích lâu dài với hệ sinh thái.\nBằng cách chuyển đổi sang Iceberg, NI đã có thể nắm bắt một nền tảng dữ liệu mở thực sự, cung cấp tính linh hoạt lâu dài, khả năng mở rộng và khả năng tương tác trong khi vẫn duy trì một nguồn tin cậy thống nhất cho tất cả các nhu cầu phân tích và báo cáo.\nNhững thách thức phải đối mặt Việc di chuyển hồ dữ liệu sản xuất trực tiếp sang Iceberg là một thách thức vì sự phức tạp trong hoạt động và các hạn chế kế thừa. Dịch vụ dữ liệu tại NI chạy hàng trăm quy trình Spark và machine learning, quản lý hàng nghìn bảng và hỗ trợ hơn 400 bảng thông tin—tất cả đều hoạt động 24/7. Bất kỳ quá trình di chuyển nào cũng cần được thực hiện mà không bị gián đoạn sản xuất; và việc điều phối một cuộc di cư như vậy trong khi các hoạt động tiếp tục liền mạch là điều khó khăn.\nNI cần đáp ứng những người dùng đa dạng với các yêu cầu và thời gian khác nhau từ kỹ sư dữ liệu đến nhà phân tích dữ liệu cho đến các nhà khoa học dữ liệu và nhóm BI.\nThêm vào thách thức là những hạn chế kế thừa. Một số công cụ hiện có không hỗ trợ đầy đủ Iceberg, vì vậy cần phải duy trì các bảng được hỗ trợ bởi Hive để tương thích. Khi NI nhận ra rằng không phải tất cả người tiêu dùng đều có thể chấp nhận Iceberg ngay lập tức. Cần có một kế hoạch để cho phép chuyển đổi gia tăng mà không có thời gian ngừng hoạt động hoặc gián đoạn các hoạt động đang diễn ra.\nCác thành phần chính cho quá trình di chuyển Để đảm bảo quá trình chuyển đổi diễn ra suôn sẻ và thành công, sáu thành phần quan trọng đã được xác định như sau:\nDuy trì hoạt động liên tục (Support ongoing operations):\nĐảm bảo khả năng tương thích không bị gián đoạn với các hệ thống và quy trình hiện có trong suốt quá trình di chuyển.\nTính minh bạch với người dùng (User transparency):\nGiảm thiểu gián đoạn cho người dùng bằng cách giữ nguyên tên bảng và cách truy cập như cũ.\nChuyển đổi người dùng dần dần (Gradual consumer migration):\nCho phép người dùng chuyển sang Iceberg theo tiến độ riêng, tránh việc phải chuyển đổi đồng loạt cùng lúc.\nTính linh hoạt trong ETL (ETL flexibility):\nCho phép chuyển các pipeline ETL sang Iceberg mà không áp đặt các ràng buộc trong quá trình phát triển hoặc triển khai.\nHiệu quả chi phí (Cost effectiveness):\nGiảm thiểu nhân bản dữ liệu lưu trữ và xử lý, cũng như chi phí phát sinh trong giai đoạn chuyển đổi.\nGiảm thiểu bảo trì (Minimize maintenance):\nGiảm gánh nặng vận hành trong việc duy trì song song hai định dạng bảng (Hive và Iceberg) trong quá trình chuyển đổi.\nĐánh giá các phương pháp di chuyển truyền thống Apache Iceberg hỗ trợ hai phương pháp chính để di chuyển dữ liệu In-place migration (Chuyển đổi trực tiếp) và Rewrite-based migration (Chuyển đổi bằng cách ghi lại dữ liệu).\nChuyển đổi trực tiếp (In-place migration)\nCách hoạt động: Phương pháp này chuyển đổi bộ dữ liệu hiện có sang bảng Iceberg mà không cần nhân bản dữ liệu, bằng cách tạo metadata của Iceberg dựa trên các tệp dữ liệu hiện có, đồng thời giữ nguyên bố cục và định dạng ban đầu.\nƯu điểm:\nTiết kiệm chi phí lưu trữ, vì không có sự nhân bản dữ liệu.\nDễ triển khai, quá trình thực hiện đơn giản.\nGiữ nguyên tên và vị trí bảng hiện tại, giúp người dùng không phải thay đổi truy cập.\nKhông cần di chuyển dữ liệu, yêu cầu tính toán tối thiểu → chi phí thấp hơn.\nNhược điểm:\nCần downtime: Tất cả các thao tác ghi phải tạm dừng trong khi chuyển đổi, điều này không thể chấp nhận được với NI, vì các quy trình dữ liệu và phân tích rất quan trọng và vận hành 24/7.\nKhông thể chuyển đổi dần dần: Tất cả người dùng phải chuyển sang Iceberg cùng lúc, làm tăng nguy cơ gián đoạn hệ thống.\nGiới hạn xác thực: Không có cơ hội kiểm tra tính đúng đắn của dữ liệu trước khi hoàn tất chuyển đổi; nếu có lỗi, cần phục hồi từ bản sao lưu.\nHạn chế kỹ thuật: Việc tiến hóa schema trong quá trình chuyển đổi có thể khó khăn; xung đột kiểu dữ liệu có thể khiến toàn bộ quy trình thất bại.\nRewrite-based migration (Chuyển đổi bằng cách ghi lại dữ liệu) Cách hoạt động: Phương pháp này tạo một bảng Iceberg mới bằng cách viết lại và tái tổ chức các tệp dữ liệu hiện có theo định dạng và cấu trúc tối ưu của Iceberg, giúp cải thiện hiệu năng và quản lý dữ liệu.\nƯu điểm:\nKhông cần downtime trong suốt quá trình di chuyển.\nHỗ trợ chuyển đổi người dùng dần dần, cho phép từng nhóm áp dụng Iceberg theo nhịp riêng.\nCho phép xác thực dữ liệu kỹ lưỡng trước khi chuyển đổi hoàn toàn.\nCơ chế rollback đơn giản, có thể dễ dàng quay lại nếu có lỗi.\nNhược điểm:\nTăng chi phí tài nguyên: Cần gấp đôi dung lượng lưu trữ và công suất xử lý trong thời gian di chuyển.\nPhức tạp trong bảo trì: Cần duy trì hai pipeline dữ liệu song song, tăng gánh nặng vận hành.\nThách thức về tính nhất quán: Khó đảm bảo hai hệ thống luôn đồng bộ hoàn toàn trong thời gian chuyển đổi.\nẢnh hưởng đến hiệu năng: Ghi dữ liệu song song (dual writes) có thể tăng độ trễ và làm chậm pipeline.\nTại sao không phương án nào là đủ tốt NI nhận thấy rằng cả hai phương pháp (In-place và Rewrite-based) đều không thể đáp ứng đầy đủ các yêu cầu quan trọng:\nIn-place migration không phù hợp vì yêu cầu downtime (ngừng hoạt động) là không thể chấp nhận được, và không hỗ trợ quá trình chuyển đổi dần dần.\nRewrite-based migration lại tốn kém chi phí và phức tạp trong quản lý vận hành do phải duy trì hai pipeline song song.\nTừ những phân tích đó, NI đã phát triển một giải pháp lai (hybrid approach) — kết hợp ưu điểm của cả hai phương pháp, đồng thời giảm thiểu và khắc phục các hạn chế của chúng.\nGiải pháp Hybrid Chiến lược di chuyển lai được thiết kế dựa trên 5 thành phần cốt lõi, tận dụng các dịch vụ phân tích của AWS để điều phối, xử lý và quản lý trạng thái.\n1. Hive-to-Iceberg CDC (Đồng bộ thay đổi từ Hive sang Iceberg): Hệ thống tự động đồng bộ các bảng Hive sang Iceberg bằng quy trình CDC (Change Data Capture) tùy chỉnh để hỗ trợ người dùng hiện có. Không giống CDC truyền thống theo mức hàng (row-level), NI áp dụng CDC ở mức phân vùng (partition-level) — vì Hive thường cập nhật dữ liệu bằng cách ghi đè (overwrite) toàn bộ phân vùng. Cách này giúp duy trì tính nhất quán dữ liệu giữa Hive và Iceberg mà không cần thay đổi logic ghi dữ liệu, đảm bảo rằng hai bảng chứa cùng dữ liệu trong giai đoạn di chuyển. 2. Continuous schema synchronization (Đồng bộ lược đồ liên tục): Trong quá trình di chuyển, việc tiến hóa schema (schema evolution) gây ra nhiều thách thức trong bảo trì. NI triển khai quy trình đồng bộ lược đồ tự động, so sánh schema giữa Hive và Iceberg, và điều chỉnh sự khác biệt trong khi vẫn giữ tương thích kiểu dữ liệu (type compatibility). 3. Iceberg-to-Hive reverse CDC (Đồng bộ ngược từ Iceberg sang Hive): Cho phép nhóm dữ liệu chuyển các job ETL (Extract, Transform, Load) để ghi trực tiếp vào Iceberg, đồng thời vẫn duy trì tính tương thích với các quy trình cũ dùng Hive. Reverse CDC giúp tự động cập nhật dữ liệu từ Iceberg ngược trở lại Hive, đảm bảo các pipeline downstream (phía sau) chưa di chuyển vẫn hoạt động bình thường. Nhờ đó, hệ thống có thể chuyển đổi dần dần, không làm gián đoạn quy trình hiện có. 4. Alias management in Snowflake (Quản lý bí danh trong Snowflake): Sử dụng alias (bí danh) trong Snowflake để đảm bảo rằng các bảng Iceberg giữ nguyên tên gốc, giúp quá trình chuyển đổi trở nên trong suốt (transparent) đối với người dùng. Cách này giúp giảm thiểu việc cấu hình lại trên các nhóm phụ thuộc và các workflow hiện có. 5. Table replacement (Thay thế bảng sản xuất): Sau khi toàn bộ hệ thống đã chuyển sang Iceberg, NI hoán đổi (swap) các bảng sản xuất, giữ nguyên tên bảng ban đầu, và hoàn tất quá trình di chuyển. Tìm hiểu sâu về kỹ thuật Quá trình di chuyển từ Hive đến Iceberg được xây dựng từ một số bước:\n1. Sơ đồ Hive-to-Iceberg CDC: Mục tiêu: Giữ cho bảng Hive và Iceberg được đồng bộ hóa mà không cần nỗ lực trùng lặp.\nHình trước cho thấy cách mọi phân vùng được ghi vào bảng Hive được sao chép tự động và minh bạch vào bảng Iceberg bằng cách sử dụng quy trình CDC. Quá trình này đảm bảo rằng cả hai bảng đều được đồng bộ hóa, cho phép di chuyển liền mạch và gia tăng mà không làm gián đoạn hệ thống xuôi dòng. NI đã chọn đồng bộ hóa cấp phân vùng vì các công việc Hive ETL cũ đã ghi các bản cập nhật bằng cách ghi đè lên toàn bộ phân vùng và cập nhật vị trí phân vùng. Việc áp dụng cách tiếp cận tương tự trong quy trình CDC đã giúp đảm bảo rằng nó vẫn nhất quán với cách dữ liệu được quản lý ban đầu, giúp quá trình di chuyển suôn sẻ hơn và tránh phải làm lại logic cấp hàng.\nThực hiện:\nĐể giữ cho bảng Hive và Iceberg được đồng bộ hóa mà không cần nỗ lực trùng lặp, một quy trình hợp lý đã được triển khai. Bất cứ khi nào các phân vùng trong bảng Hive được cập nhật, AWS Glue Catalog sẽ phát ra các sự kiện như . Amazon EventBridge đã nắm bắt các sự kiện này, lọc chúng cho các cơ sở dữ liệu và bảng có liên quan theo quy tắc cầu nối sự kiện, đồng thời kích hoạt AWS Lambda Hàm này phân tích siêu dữ liệu sự kiện và gửi các bản cập nhật phân vùng đến chủ đề Apache Kafka.UpdatePartition\nMột tác vụ Spark chạy trên Amazon EMR đã sử dụng các tin nhắn từ Kafka, chứa các chi tiết phân vùng được cập nhật từ các sự kiện Danh mục dữ liệu. Sử dụng siêu dữ liệu sự kiện đó, tác vụ Spark truy vấn bảng Hive có liên quan và ghi vào bảng Iceberg trong Amazon S3 bằng API Spark Iceberg, như được hiển thị trong ví dụ sau:overwritePartitions\n{\n\u0026quot;id\u0026quot;: \u0026quot;10397e54-c049-fc7b-76c8-59e148c7cbfc\u0026quot;, \u0026quot;detail-type\u0026quot;: \u0026quot;Glue Data Catalog Table State Change\u0026quot;, \u0026quot;source\u0026quot;: \u0026quot;aws.glue\u0026quot;, \u0026quot;time\u0026quot;: \u0026quot;2024-10-27T17:16:21Z\u0026quot;, \u0026quot;region\u0026quot;: \u0026quot;us-east-1\u0026quot;, \u0026quot;detail\u0026quot;: { \u0026quot;databaseName\u0026quot;: \u0026quot;dlk\\_visitor\\_funnel\\_dwh\\_production\u0026quot;, \u0026quot;changedPartitions\u0026quot;: \\[ \u0026quot;2024-10-27\u0026quot; \\], \u0026quot;typeOfChange\u0026quot;: \u0026quot;UpdatePartition\u0026quot;, \u0026quot;tableName\u0026quot;: \u0026quot;fact\\_events\u0026quot; } }\nBằng cách chỉ nhắm mục tiêu các phân vùng đã sửa đổi, quy trình (được hiển thị trong hình sau) đã giảm đáng kể nhu cầu viết lại toàn bảng tốn kém. Các lớp siêu dữ liệu mạnh mẽ của Iceberg, bao gồm ảnh chụp nhanh và tệp kê khai, đã được cập nhật liền mạch để nắm bắt những thay đổi này, cung cấp đồng bộ hóa hiệu quả và chính xác giữa bảng Hive và Iceberg. 2. Sơ đồ ceberg-to-Hive reverse CDC Mục tiêu: Hỗ trợ khách hàng Hive đồng thời cho phép các quy trình ETL chuyển sang Iceberg.\nHình trước cho thấy quá trình ngược lại, trong đó mọi phân vùng được ghi vào bảng Iceberg được sao chép tự động và minh bạch vào bảng Hive bằng cơ chế CDC. Quá trình này giúp đảm bảo đồng bộ hóa giữa hai hệ thống, cho phép cập nhật dữ liệu liền mạch cho các hệ thống cũ vẫn dựa vào Hive trong khi chuyển sang Iceberg.\nThực hiện:\nĐồng bộ hóa dữ liệu từ bảng Iceberg trở lại bảng Hive đưa ra một thách thức khác. Không giống như bảng Hive, Danh mục dữ liệu không theo dõi các bản cập nhật phân vùng cho bảng Iceberg vì các phân vùng trong Iceberg được quản lý nội bộ chứ không phải trong danh mục. Điều này có nghĩa là NI không thể dựa vào các sự kiện Glue Catalog để phát hiện các thay đổi phân vùng.\nĐể giải quyết vấn đề này, NI đã triển khai một giải pháp tương tự như quy trình trước đó nhưng thích ứng với kiến trúc của Iceberg. Apache Spark được sử dụng để truy vấn các bảng siêu dữ liệu của Iceberg - cụ thể là các bảng ảnh chụp nhanh và mục nhập - để xác định các phân vùng được sửa đổi kể từ lần đồng bộ hóa cuối cùng. Truy vấn được sử dụng là:\nSELECT e.data_file.partition, MAX(s.committed_at) AS last_modified_time\nFROM $target_table.snapshots JOIN $target_table.entries e ON s.snapshot_id = e.snapshot_id\nWHERE s.committed_at \u0026amp;gt; \u0026lsquo;$last_sync_time\u0026rsquo;\nGROUP BY e.data_file.partition;\nTruy vấn này chỉ trả về các phân vùng đã được cập nhật kể từ lần đồng bộ hóa cuối cùng, cho phép nó tập trung hoàn toàn vào dữ liệu đã thay đổi. Sử dụng thông tin này, tương tự như quy trình trước đó, một công việc Spark đã truy xuất các phân vùng được cập nhật từ Iceberg và ghi chúng trở lại bảng Hive tương ứng, cung cấp sự đồng bộ hóa liền mạch giữa cả hai bảng.\n3. Đồng bộ hóa lược đồ liên tục Mục tiêu: tự động cập nhật lược đồ để duy trì tính nhất quán trên Hive và Iceberg.\nHình trước cho thấy quy trình đồng bộ hóa lược đồ tự động giúp đảm bảo tính nhất quán giữa lược đồ bảng Hive và Iceberg bằng cách tự động đồng bộ hóa các thay đổi lược đồ. Trong ví dụ này, thêm cột, giảm thiểu công việc thủ công và bảo trì kép trong thời gian di chuyển kéo dài.Channel\nThực hiện:\nĐể xử lý các thay đổi lược đồ giữa Hive và Iceberg, một quy trình đã được thực hiện để phát hiện và đối chiếu sự khác biệt một cách tự động. Khi thay đổi lược đồ xảy ra trong bảng Hive, Danh mục dữ liệu sẽ phát ra một sự kiện. Sự kiện này kích hoạt hàm Lambda (được định tuyến qua EventBridge), hàm này truy xuất lược đồ cập nhật từ Danh mục dữ liệu cho bảng Hive và so sánh với lược đồ Iceberg. Điều quan trọng cần lưu ý là trong thiết lập của NI, các thay đổi lược đồ bắt nguồn từ Hive vì bảng Iceberg được ẩn đằng sau các bí danh trên toàn hệ thống. Bởi vì Iceberg chủ yếu được sử dụng cho Snowflake, đồng bộ hóa một chiều từ Hive đến Iceberg là đủ. Do đó, không có cơ chế để phát hiện hoặc xử lý các thay đổi lược đồ được thực hiện trực tiếp trong Iceberg, vì chúng không cần thiết trong quy trình làm việc hiện tại.UpdateTable\nTrong quá trình đối chiếu lược đồ (hiển thị trong hình sau), các kiểu dữ liệu được chuẩn hóa để giúp đảm bảo khả năng tương thích — ví dụ: chuyển đổi Hive sang Iceberg . Mọi trường hoặc thay đổi loại mới đều được xác thực và áp dụng cho lược đồ Iceberg bằng cách sử dụng tác vụ Spark chạy trên Amazon EMR. Amazon DynamoDB lưu trữ các điểm kiểm tra đồng bộ hóa lược đồ cho phép theo dõi các thay đổi theo thời gian và duy trì tính nhất quán giữa lược đồ Hive và Iceberg.VARCHARSTRING\nTrong ví dụ này, thêm cột, giảm thiểu công việc thủ công và bảo trì kép trong thời gian di chuyển kéo dài.Channel\nThực hiện:\nĐể xử lý các thay đổi lược đồ giữa Hive và Iceberg, một quy trình đã được thực hiện để phát hiện và đối chiếu sự khác biệt một cách tự động. Khi thay đổi lược đồ xảy ra trong bảng Hive, Danh mục dữ liệu sẽ phát ra một sự kiện. Sự kiện này kích hoạt hàm Lambda (được định tuyến qua EventBridge), hàm này truy xuất lược đồ cập nhật từ Danh mục dữ liệu cho bảng Hive và so sánh với lược đồ Iceberg. Điều quan trọng cần lưu ý là trong thiết lập của NI, các thay đổi lược đồ bắt nguồn từ Hive vì bảng Iceberg được ẩn đằng sau các bí danh trên toàn hệ thống. Bởi vì Iceberg chủ yếu được sử dụng cho Snowflake, đồng bộ hóa một chiều từ Hive đến Iceberg là đủ. Do đó, không có cơ chế để phát hiện hoặc xử lý các thay đổi lược đồ được thực hiện trực tiếp trong Iceberg, vì chúng không cần thiết trong quy trình làm việc hiện tại.UpdateTable\nTrong quá trình đối chiếu lược đồ (hiển thị trong hình sau), các kiểu dữ liệu được chuẩn hóa để giúp đảm bảo khả năng tương thích — ví dụ: chuyển đổi Hive sang Iceberg . Mọi trường hoặc thay đổi loại mới đều được xác thực và áp dụng cho lược đồ Iceberg bằng cách sử dụng tác vụ Spark chạy trên Amazon EMR. Amazon DynamoDB lưu trữ các điểm kiểm tra đồng bộ hóa lược đồ cho phép theo dõi các thay đổi theo thời gian và duy trì tính nhất quán giữa lược đồ Hive và Iceberg.VARCHARSTRING\nBằng cách tự động hóa đồng bộ hóa lược đồ này, chi phí bảo trì đã giảm đáng kể và giải phóng các nhà phát triển khỏi việc đồng bộ hóa lược đồ theo cách thủ công, làm cho thời gian di chuyển dài dễ quản lý hơn đáng kể.\nHình trước mô tả quy trình làm việc tự động để duy trì tính nhất quán lược đồ giữa bảng Hive và Iceberg. AWS Glue ghi lại các sự kiện thay đổi trạng thái bảng từ Hive, kích hoạt sự kiện EventBridge. Sự kiện này gọi một hàm Lambda tìm nạp siêu dữ liệu từ DynamoDB và so sánh các lược đồ được tìm nạp từ AWS Glue cho cả bảng Hive và Iceberg. Nếu phát hiện sự không khớp, lược đồ trong Iceberg sẽ được cập nhật để giúp đảm bảo căn chỉnh, giảm thiểu sự can thiệp thủ công và hỗ trợ hoạt động trơn tru trong quá trình di chuyển.\n4. Quản lý bí danh trong Snowflake Mục tiêu: Cho phép người tiêu dùng Snowflake áp dụng Iceberg mà không cần thay đổi tham chiếu truy vấn.\nHình trước cho thấy bí danh Snowflake cho phép di chuyển liền mạch bằng cách ánh xạ các truy vấn như bảng Iceberg trong Glue Catalog. Ngay cả khi có thêm hậu tố trong quá trình di chuyển Iceberg, các truy vấn và quy trình làm việc hiện có vẫn không thay đổi, giảm thiểu sự gián đoạn cho các công cụ BI và nhà phân tích.SELECT platform, COUNT(clickouts) FROM funnel.clickouts\nThực hiện:\nĐể giúp đảm bảo trải nghiệm liền mạch cho các công cụ BI và nhà phân tích trong quá trình di chuyển, bí danh Snowflake đã được sử dụng để ánh xạ các bảng bên ngoài với siêu dữ liệu Iceberg được lưu trữ trong Danh mục dữ liệu. Bằng cách gán các bí danh khớp với tên bảng Hive ban đầu, các truy vấn và báo cáo hiện có đã được giữ nguyên mà không bị gián đoạn. Ví dụ: một bảng bên ngoài đã được tạo trong Snowflake và đặt bí danh thành tên bảng ban đầu, như được hiển thị trong truy vấn sau:\nCREATE OR REPLACE ICEBERG TABLE dlk\\_visitor\\_funnel\\_dwh\\_production.aggregated\\_cost EXTERNAL\\_VOLUME \\= 's3\\_dlk\\_visitor\\_funnel\\_dwh\\_production\\_iceberg\\_migration' CATALOG \\= 'glue\\_dlk\\_visitor\\_funnel\\_dwh\\_production\\_iceberg\\_migration' CATALOG\\_TABLE\\_NAME \\= 'aggregated\\_cost'; ALTER ICEBERG TABLE dlk\\_visitor\\_funnel\\_dwh\\_production.aggregated\\_cost REFRESH; Khi quá trình di chuyển hoàn tất, một thay đổi đơn giản trở lại bí danh đã được thực hiện để trỏ đến vị trí hoặc lược đồ mới, giúp quá trình chuyển đổi trở nên liền mạch và giảm thiểu bất kỳ sự gián đoạn nào đối với quy trình làm việc của người dùng.\n5. Thay thế bàn Mục tiêu: Khi tất cả các ETL và quy trình dữ liệu liên quan được chuyển đổi thành công để sử dụng các khả năng của Apache Iceberg và mọi thứ hoạt động chính xác với luồng đồng bộ hóa, đã đến lúc chuyển sang giai đoạn cuối cùng của quá trình di chuyển. Mục tiêu chính là duy trì tên bảng ban đầu, tránh sử dụng bất kỳ tiền tố nào như những tiền tố được sử dụng trong các bước di chuyển trung gian trước đó. Điều này giúp đảm bảo rằng cấu hình vẫn gọn gàng và không có phức tạp đặt tên không cần thiết.\nHình trước cho thấy việc thay thế bảng để hoàn tất quá trình di chuyển, trong đó Hive trên Amazon EMR được sử dụng để đăng ký các tệp Parquet dưới dạng bảng Iceberg trong khi vẫn giữ nguyên tên bảng ban đầu và tránh trùng lặp dữ liệu, giúp đảm bảo di chuyển liền mạch và gọn gàng.\nThực hiện:\nMột trong những thách thức là không thể đổi tên bảng trong AWS Glue, điều này ngăn cản việc sử dụng phương pháp đổi tên đơn giản cho các bảng luồng đồng bộ hóa hiện có. Ngoài ra, AWS Glue không hỗ trợ quy trình này tạo siêu dữ liệu Iceberg trên tệp dữ liệu hiện có trong khi vẫn giữ nguyên tên bảng ban đầu. Chiến lược để khắc phục hạn chế này là sử dụng metastore Hive trên cụm Amazon EMR. Bằng cách sử dụng Hive trên Amazon EMR, NI có thể tạo các bảng cuối cùng với tên ban đầu của chúng vì nó hoạt động trong một môi trường metastore riêng biệt, mang lại sự linh hoạt để xác định mọi lược đồ và tên bảng bắt buộc mà không bị can thiệp.Migrate\nQuy trình này được sử dụng để đăng ký một cách có phương pháp tất cả các tệp Parquet hiện có, do đó xây dựng tất cả siêu dữ liệu cần thiết trong Hive. Đây là một bước quan trọng, vì nó giúp đảm bảo rằng tất cả các tệp dữ liệu được lập danh mục và liên kết một cách thích hợp trong metastore.add_files\nHình trước cho thấy sự chuyển đổi của bảng sản xuất sang Iceberg bằng cách sử dụng thủ tục để đăng ký các tệp Parquet hiện có và tạo siêu dữ liệu Iceberg. Điều này giúp đảm bảo quá trình di chuyển suôn sẻ trong khi vẫn giữ nguyên dữ liệu gốc và tránh trùng lặp.add_files\nThiết lập này cho phép sử dụng các tệp Parquet hiện có mà không sao chép dữ liệu, do đó tiết kiệm tài nguyên. Mặc dù luồng đồng bộ hóa sử dụng các vùng lưu trữ riêng biệt cho kiến trúc cuối cùng, NI đã chọn duy trì các vùng lưu trữ ban đầu và dọn dẹp các tệp trung gian. Điều này dẫn đến cấu trúc thư mục khác trên Amazon S3. Dữ liệu lịch sử có các thư mục con cho mỗi phân vùng trong thư mục bảng gốc, trong khi dữ liệu Iceberg mới tổ chức các thư mục con trong thư mục dữ liệu. Sự khác biệt này có thể chấp nhận được để tránh trùng lặp dữ liệu và giữ nguyên các vùng lưu trữ Amazon S3 ban đầu.\nTóm tắt kỹ thuật Danh mục dữ liệu AWS Glue đóng vai trò là nguồn tin cậy chính cho các bản cập nhật lược đồ và bảng, với Amazon EventBridge ghi lại các sự kiện Danh mục dữ liệu để kích hoạt quy trình đồng bộ hóa. AWS Lambda phân tích siêu dữ liệu sự kiện và đồng bộ hóa lược đồ được quản lý, trong khi Apache Kafka đệm các sự kiện để xử lý theo thời gian thực. Apache Spark trên Amazon EMR xử lý chuyển đổi dữ liệu và cập nhật gia tăng, đồng thời Amazon DynamoDB duy trì trạng thái, bao gồm các điểm kiểm tra đồng bộ hóa và ánh xạ bảng. Cuối cùng, Snowflake sử dụng liền mạch các bảng Iceberg thông qua bí danh mà không làm gián đoạn quy trình làm việc hiện có.\nKết quả di chuyển Quá trình di chuyển đã được hoàn thành mà không có thời gian chết; Các hoạt động liên tục được duy trì trong suốt quá trình di chuyển, hỗ trợ hàng trăm đường ống và bảng điều khiển mà không bị gián đoạn. Quá trình di chuyển được thực hiện với tư duy tối ưu hóa chi phí với các bản cập nhật gia tăng và đồng bộ hóa cấp phân vùng giúp giảm thiểu việc sử dụng tài nguyên điện toán và lưu trữ. Cuối cùng, NI đã thiết lập một nền tảng hiện đại, trung lập với nhà cung cấp cho phép mở rộng nhu cầu phân tích và học máy đang phát triển của họ. Nó cho phép tích hợp liền mạch với nhiều công cụ điện toán và truy vấn, hỗ trợ tính linh hoạt và đổi mới hơn nữa.\nKết thúc Chuyển đổi trí tuệ tự nhiên sang Apache Iceberg là một bước quan trọng trong việc hiện đại hóa cơ sở hạ tầng dữ liệu của công ty. Bằng cách áp dụng chiến lược kết hợp và sử dụng sức mạnh của kiến trúc theo hướng sự kiện, NI đã giúp đảm bảo quá trình chuyển đổi liền mạch cân bằng giữa đổi mới với sự ổn định trong hoạt động. Hành trình nhấn mạnh tầm quan trọng của việc lập kế hoạch cẩn thận, hiểu hệ sinh thái dữ liệu và tập trung vào cách tiếp cận ưu tiên tổ chức.\nTrên hết, hoạt động kinh doanh được tập trung và tính liên tục ưu tiên trải nghiệm người dùng. Bằng cách đó, NI đã mở khóa tính linh hoạt và khả năng mở rộng của hồ dữ liệu của họ đồng thời giảm thiểu sự gián đoạn, cho phép các nhóm sử dụng khả năng phân tích tiên tiến, định vị công ty ở vị trí hàng đầu về quản lý dữ liệu hiện đại và sẵn sàng cho tương lai.\nNếu bạn đang cân nhắc di chuyển Apache Iceberg hoặc gặp phải những thách thức tương tự về cơ sở hạ tầng dữ liệu, chúng tôi khuyến khích bạn khám phá các khả năng. Nắm bắt các định dạng mở, sử dụng tự động hóa và thiết kế với nhu cầu riêng của tổ chức bạn. Hành trình có thể phức tạp, nhưng phần thưởng về khả năng mở rộng, tính linh hoạt và đổi mới rất xứng đáng với nỗ lực. Bạn có thể sử dụng hướng dẫn theo quy định của AWS để giúp tìm hiểu thêm về cách sử dụng Apache Iceberg tốt nhất cho tổ chức của bạn\nGiới thiệu về các tác giả\nẢnh đại diện Giới thiệu về các tác giả Yonatan Dolan là Chuyên gia phân tích chính tại Amazon Web Services. Yonatan là một nhà truyền giáo Apache Iceberg. Haya Stern là Giám đốc Cấp cao về Dữ liệu tại Natural Intelligence. Cô lãnh đạo việc phát triển nền tảng dữ liệu quy mô lớn của NI, tập trung vào việc cho phép phân tích, hợp lý hóa quy trình làm việc dữ liệu và cải thiện hiệu quả phát triển. Trong năm qua, cô đã dẫn dắt việc di chuyển thành công từ kiến trúc dữ liệu trước đó sang một ngôi nhà hồ hiện đại dựa trên Apache Iceberg và Snowflake. Zion Rubin là Kiến trúc sư dữ liệu tại Natural Intelligence với mười năm kinh nghiệm kiến trúc các nền tảng dữ liệu lớn quy mô lớn, hiện tập trung vào việc phát triển các hệ thống tác nhân thông minh biến dữ liệu phức tạp thành thông tin chi tiết về kinh doanh theo thời gian thực. Michał Urbanowicz là Kỹ sư dữ liệu đám mây tại Natural Intelligence với chuyên môn trong việc di chuyển kho dữ liệu và triển khai các quy trình lưu giữ, dọn dẹp và giám sát mạnh mẽ để đảm bảo khả năng mở rộng và độ tin cậy. Ông cũng phát triển các tính năng tự động hóa giúp hợp lý hóa và hỗ trợ các hoạt động quản lý chiến dịch trong môi trường dựa trên đám mây. "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/vi/5-workshop/5.5-fullstack/5.5.1-backend-deploy/",
	"title": "Deploy Backend Serverless",
	"tags": [],
	"description": "",
	"content": "Trong bước này, chúng ta sẽ cấu hình và triển khai toàn bộ Backend (bao gồm Lambda, API Gateway, Cognito\u0026hellip;) tự động thông qua Serverless Framework.\n1. Cấu hình file Serverless Mở thư mục source code dự án trên VS Code.\nĐi vào thư mục backend.\nMở file serverless.yml.\nTìm và chỉnh sửa các dòng sau (thay thế \u0026lt;your-bucket-name\u0026gt; bằng tên Bucket S3 bạn đã tạo ở bài 5.3):\nTại dòng biến môi trường S3_BUCKET_RAW: S3_BUCKET_RAW: contract-app-demo-tenban Tại phần iam -\u0026gt; statements -\u0026gt; Resource (Phân quyền truy cập S3): Resource: - \u0026#34;arn:aws:s3:::contract-app-demo-tenban\u0026#34; - \u0026#34;arn:aws:s3:::contract-app-demo-tenban/*\u0026#34; 2. Cấu hình API Endpoint cho Frontend Trước khi deploy, chúng ta cập nhật luôn địa chỉ API cho code frontend (mặc dù file này nằm trong thư mục backend nhưng nó phục vụ cho client).\nTruy cập AWS Console -\u0026gt; Lambda -\u0026gt; Chọn hàm ragsearch.\nTrong phần Function overview -\u0026gt; Triggers, copy đường dẫn API Endpoint.\nQuay lại VS Code, mở file theo đường dẫn: backend/src/services/ragService.ts (hoặc đường dẫn tương tự trong source code của bạn).\nTìm dòng khai báo RAG_API_URL và dán link vừa copy vào:\nconst RAG_API_URL = \u0026#34;https://example.execute-api.ap-southeast-1.amazonaws.com\u0026#34;; Lưu file lại (Ctrl + S). 3. Cấu hình AWS CLI Nếu bạn chưa cấu hình hoặc muốn đảm bảo chắc chắn, hãy chạy lại các lệnh sau trong Terminal (CMD/PowerShell) tại thư mục gốc của dự án:\nChạy lệnh: aws configure Nhập thông tin từ file .csv Access Key bạn đã tải về: AWS Access Key ID: \u0026lt;Nhập Access Key ID\u0026gt; AWS Secret Access Key: \u0026lt;Nhập Secret Access Key\u0026gt; Default region name: ap-southeast-1 Default output format: (Nhấn Enter để bỏ qua) 4. Deploy Backend Vẫn trong Terminal, di chuyển vào thư mục backend: cd backend Chạy lệnh deploy: npx serverless deploy ⏳ Chờ đợi: Quá trình này sẽ mất khoảng 3-5 phút. Serverless Framework sẽ đóng gói code, tạo CloudFormation stack và đẩy lên AWS.\n5. Kiểm tra kết quả Nếu deploy thành công, Terminal sẽ hiện thông báo màu xanh với các thông tin về Service Information, endpoints, và functions.\nLúc này, quay lại AWS Console, bạn sẽ thấy các tài nguyên mới như Amazon Cognito User Pool và các Lambda functions mới đã được tạo ra.\n"
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/vi/4-eventparticipated/4.1-event1/",
	"title": "Event 1",
	"tags": [],
	"description": "",
	"content": "Vietnam Cloud Day 2025: Ho Chi Minh City Connect Edition for Builders Thời gian: Thứ Năm, ngày 18/09/2025, 9:00 – 17:30 Địa điểm: Amazon Web Services Vietnam, Tầng 36, số 2 Hải Triều, Phường Bến Nghé, Quận 1, Thành phố Hồ Chí Minh Vai trò: Người tham dự\nMục Đích Của Sự Kiện Cập nhật xu hướng công nghệ chiến lược hàng đầu: Agentic AI. Tìm hiểu giải pháp xây dựng nền tảng dữ liệu (Data Foundation) để giải quyết vấn đề \u0026ldquo;Data Silos\u0026rdquo; mà 52% CDO đang gặp phải. Tiếp cận quy trình phát triển phần mềm mới AI-Driven Development Lifecycle (AI-DLC). Nắm bắt các tiêu chuẩn bảo mật cho GenAI (MITRE ATLAS, OWASP, NIST) và các tầng rủi ro. Danh Sách Diễn Giả Eric Yeo - Country General Manager, AWS Vietnam Dr. Jens Lottner - CEO, Techcombank Ms. Trang Phung - CEO \u0026amp; Co-Founder, U2U Network Jaime Valles - VP, GM Asia Pacific and Japan, AWS Jeff Johnson - Managing Director, ASEAN, AWS Vu Van - Co-founder \u0026amp; CEO, ELSA Corp Nguyen Hoa Binh - Chairman, Nexttech Group Dieter Botha - CEO, TymeX Jun Kai Loke - AI/ML Specialist SA, AWS Kien Nguyen - Solutions Architect, AWS Tamelly Lim - Storage Specialist SA, AWS Binh Tran - Senior Solutions Architect, AWS Taiki Dang - Solutions Architect, AWS Michael Armentano - Principal WW GTM Specialist, AWS Hung Nguyen Gia - Head of Solutions Architect, AWS Son Do - Technical Account Manager, AWS Nguyen Van Hai - Director of Software Engineering, Techcombank Phuc Nguyen - Solutions Architect, AWS Alex Tran - AI Director, OCB Nguyen Minh Ngan - AI Specialist, OCB Nguyen Manh Tuyen - Head of Data Application, LPBank Securities Vinh Nguyen - Co-Founder \u0026amp; CTO, Ninety Eight Hung Hoang - Customer Solutions Manager, AWS Christal Poon - Specialist Solutions Architect, AWS Nội Dung Nổi Bật 1. Xu hướng Agentic AI \u0026amp; Data Strategy Thực trạng: 88% CDO đang tiến tới với GenAI, nhưng 52% cho rằng nền tảng dữ liệu chưa sẵn sàng. Thách thức: Doanh nghiệp bị kìm hãm bởi 3 loại \u0026ldquo;Silos\u0026rdquo;: Data Silos, People Silos, và Business Silos. Chiến lược dữ liệu: Mô hình End-to-end từ Producers → Foundations → Consumers. Hạ tầng: Amazon S3 (Data Lakes), Amazon Redshift (Data Warehouses), hỗ trợ chuẩn mở Apache Iceberg. Quản trị: Amazon DataZone giúp quản trị dữ liệu và AI (Data \u0026amp; AI Governance). Công cụ mới: Giới thiệu Unified Studio tích hợp các công cụ analytics và AI. 2. AI-Driven Development Lifecycle (AI-DLC) Speaker Binh Tran đã giới thiệu sự chuyển dịch từ AI-Assisted sang AI-Driven Development. Quy trình AI-DLC gồm 3 giai đoạn chính:\nInception: Build Context trên code hiện có, làm rõ ý định (User Stories), lên kế hoạch (Units of Work), mô hình hóa Domain. Construction: AI tự sinh code \u0026amp; Test, bổ sung các thành phần kiến trúc. Operation: Triển khai với IaC \u0026amp; tests, quản lý sự cố. 3. Bảo mật GenAI (Security for GenAI) Speaker Taiki Dang nhấn mạnh bảo mật phải chạy song hành với Generative AI.\nPhân lớp rủi ro (Risks Layers): Top layer (Consumer): Rủi ro về IP, pháp lý, ảo giác (Hallucination), an toàn. Middle layer (Tuner): Rủi ro từ Managed services, chính sách lưu trữ dữ liệu. Bottom layer (Provider): Rủi ro từ dữ liệu training. Framework \u0026amp; Tiêu chuẩn: Áp dụng MITRE ATLAS, OWASP Top 10 for LLM, NIST AI RMF, ISO 42001. Giải pháp: Sử dụng Amazon Bedrock Guardrails để ngăn chặn và giảm thiểu rủi ro (như toxicity, PII leak). 4. Analytics \u0026amp; Business Intelligence Speaker Christal Poon giới thiệu về sự chuyển đổi từ Amazon QuickSight sang Amazon Q.\nTính năng tạo Dashboard, báo cáo và Data QA bằng ngôn ngữ tự nhiên. Sắp ra mắt tại Việt Nam: Amazon Agentic AI Workbench (Quick Suite) với khả năng Quick Researcher và Quick Automate, đưa con người vào vòng lặp (Human in the loop) để kiểm soát. Những Gì Học Được Tư duy về Agent: Hiểu rõ cấu trúc của một AI Agent gồm: Goals → Observation → Tools → Context → Action. Kiến trúc Agent Core: Cần đảm bảo đủ các thành phần: Runtime, Gateway, Memory, Observability và Identity để đưa Agent vào production một cách an toàn và mở rộng được. Bảo mật đa lớp: Không chỉ bảo mật ứng dụng mà phải kiểm soát rủi ro từ dữ liệu training, quá trình fine-tuning cho đến người dùng cuối (Consumer risks). Ứng Dụng Vào Công Việc Triển khai AI-DLC: Thử nghiệm áp dụng quy trình 7 bước của AI-DLC vào dự án mới, bắt đầu từ việc dùng AI để \u0026ldquo;Build Context\u0026rdquo; và \u0026ldquo;Domain Modeling\u0026rdquo;. Tăng cường bảo mật: Rà soát lại ứng dụng GenAI hiện tại theo checklist của OWASP Top 10 for LLM và tích hợp Bedrock Guardrails để lọc nội dung độc hại. Modernize Data Stack: Đánh giá khả năng chuyển đổi Data Warehouse hiện tại sang kiến trúc Lakehouse với Apache Iceberg trên AWS để phá vỡ các Data Silos. Trải Nghiệm Cá Nhân Sự kiện lần này thực sự đi sâu vào kỹ thuật (deep-dive) hơn em mong đợi, mang lại nhiều giá trị thực tiễn:\nEm rất tâm đắc với phần chia sẻ về AI-DLC của anh Bình Trần. Nó thay đổi hoàn toàn cách em nhìn nhận về việc coding: developer không còn chỉ viết code mà trở thành người \u0026ldquo;kiến trúc\u0026rdquo; và \u0026ldquo;review\u0026rdquo; cho AI thực thi. Slide về Scoping Matrix và các lớp rủi ro trong GenAI giúp em có cái nhìn hệ thống hơn để giải trình với bộ phận Security của công ty khi muốn triển khai dự án AI mới. Rất hào hứng với thông tin Amazon Agentic AI Workbench sắp về Việt Nam, hứa hẹn sẽ giải quyết được bài toán tự động hóa quy trình nghiên cứu thị trường (Quick Researcher) mà team business đang cần. Một số hình ảnh khi tham gia sự kiện Tổng kết: Một sự kiện \u0026ldquo;must-attend\u0026rdquo; cho Builders. Kiến thức về Agentic AI và AI-DLC sẽ là kim chỉ nam cho lộ trình phát triển kỹ thuật của em trong năm tới.\n"
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/vi/1-worklog/",
	"title": "Nhật ký công việc",
	"tags": [],
	"description": "",
	"content": "Tuần 1: Làm quen với AWS và các dịch vụ cơ bản trong AWS\nTuần 2: Xây dựng website tĩnh với S3 và kết nối database RDS\nTuần 3: Tối ưu hiệu suất với CloudFront, DynamoDB và ElastiCache\nTuần 4: Migration và Disaster Recovery với DMS và EDR\nTuần 5: Infrastructure as Code với CloudFormation, CDK và Systems Manager\nTuần 6: Làm công việc về Bảo mật và Quản lý chi phí với IAM Policy, KMS, Secrets Manager, Billing Dashboard và AWS Budgets\nTuần 7: Nâng cao độ khả năng mở rộng hệ thống với Auto Scaling, Load Balancer, SQS/SNS, và VPC Flow Logs\nTuần 8: Ôn tập AWS Well-Architected Framework và củng cố kiến thức các dịch vụ trọng tâm\nTuần 9: Thực hành các dịch vụ Data \u0026amp; Analytics: Data Lake với S3, Glue, Athena và QuickSight\nTuần 10: Thực hành các dịch vụ AI/ML: SageMaker, Rekognition, Comprehend và Kendra\nTuần 11: Viết proposal và hoàn thiện làm dự án cuối kỳ\nTuần 12: Dự án cuối kỳ - Tổng kết kiến thức và xây dựng dự án AWS hoàn chỉnh\n"
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/vi/5-workshop/5.3-infrastructure/5.3.1-amazon-s3-bucket/",
	"title": "Tạo Amazon S3 Bucket",
	"tags": [],
	"description": "",
	"content": "Trong bước này, chúng ta sẽ tạo một S3 Bucket để lưu trữ tài liệu hợp đồng mẫu và các dữ liệu vector cần thiết cho AI.\n1. Tạo Bucket Truy cập dịch vụ S3 trên AWS Console. Chọn nút màu cam Create bucket. General configuration: Bucket type: Chọn General purpose. Bucket name: Đặt một tên duy nhất toàn cầu (ví dụ: contract-app-demo). Lưu ý: Tên bucket chỉ được dùng chữ thường, số và dấu gạch ngang, không dùng chữ hoa. Kéo xuống dưới cùng, các thiết lập khác giữ nguyên mặc định. Bấm Create bucket. 2. Tạo cấu trúc thư mục Sau khi tạo xong, chúng ta cần tạo cấu trúc thư mục để tổ chức dữ liệu.\nBấm vào tên Bucket bạn vừa tạo để truy cập vào bên trong. Bấm nút Create folder. Lần lượt tạo 4 thư mục với tên chính xác như sau (bạn cần lặp lại thao tác tạo folder 4 lần): contract-templates index legal-corpus user-data Sau khi tạo xong, cấu trúc bucket của bạn sẽ trông như sau:\n3. Upload dữ liệu mẫu Sử dụng các file trong thư mục source code contract-demo mà bạn đã tải về máy tính ở phần Chuẩn bị:\nTruy cập vào folder contract-templates trên S3 -\u0026gt; Bấm Upload -\u0026gt; Chọn các file mẫu hợp đồng (.docx/.pdf) từ máy tính -\u0026gt; Bấm Upload. Truy cập vào folder index trên S3 -\u0026gt; Bấm Upload -\u0026gt; Chọn file template_metadata.jsonl. Truy cập vào folder legal-corpus trên S3 -\u0026gt; Bấm Upload -\u0026gt; Chọn file dữ liệu luật (nếu có trong source code). Sử dụng các file trong thư mục source code contract-demo mà bạn đã tải về máy tính ở phần Chuẩn bị. Thực hiện upload theo cấu trúc sau:\nTruy cập vào folder index trên S3 -\u0026gt; Bấm Upload -\u0026gt; Chọn file template_metadata.jsonl -\u0026gt; Bấm Upload. Truy cập vào folder legal-corpus trên S3. Tại đây, hãy tạo thêm 2 thư mục con là index và raw-documents. Truy cập vào folder con index -\u0026gt; Bấm Upload -\u0026gt; Chọn file legal_chunks_with_emb.jsonl (hoặc legal_metadata.json). Truy cập vào folder con raw-documents -\u0026gt; Bấm Upload -\u0026gt; Chọn các file tài liệu luật gốc (nếu có). "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/vi/5-workshop/5.4-backed/5.4.1-rag-search/",
	"title": "Tạo Lambda RAG Search",
	"tags": [],
	"description": "",
	"content": "Hàm ragsearch là thành phần quan trọng nhất, chịu trách nhiệm tìm kiếm dữ liệu pháp lý và trả lời câu hỏi.\n1. Khởi tạo Function Truy cập dịch vụ Lambda -\u0026gt; Chọn Create function. Chọn Author from scratch. Điền các thông tin cơ bản: Function name: ragsearch Runtime: Python 3.12 Architecture: x86_64 Bấm Create function. 2. Cập nhật Code Tại tab Code, mở file lambda_function.py. Xóa hết nội dung mặc định. Mở file ragsearch.py trong thư mục source code, copy toàn bộ nội dung và dán vào cửa sổ code trên AWS Console. import json import os import math import logging from typing import List, Dict, Any, Tuple import boto3 from botocore.exceptions import ClientError logger = logging.getLogger() logger.setLevel(logging.INFO) # ----------------------------------------------------------------------------- # Config # ----------------------------------------------------------------------------- AWS_REGION = os.getenv(\u0026#34;AWS_REGION\u0026#34;, \u0026#34;ap-southeast-1\u0026#34;) EMBED_MODEL_ID = os.getenv(\u0026#34;EMBED_MODEL_ID\u0026#34;, \u0026#34;cohere.embed-multilingual-v3\u0026#34;) LEGAL_INDEX_BUCKET = os.getenv(\u0026#34;LEGAL_INDEX_BUCKET\u0026#34;) # tên của bucket S3 LEGAL_INDEX_KEY = os.getenv(\u0026#34;LEGAL_INDEX_KEY\u0026#34;, \u0026#34;index/legal_chunks_with_emb.jsonl\u0026#34;) if not LEGAL_INDEX_BUCKET: raise RuntimeError(\u0026#34;LEGAL_INDEX_BUCKET env var is required\u0026#34;) s3 = boto3.client(\u0026#34;s3\u0026#34;, region_name=AWS_REGION) bedrock = boto3.client(\u0026#34;bedrock-runtime\u0026#34;, region_name=AWS_REGION) # ----------------------------------------------------------------------------- # Global cache # ----------------------------------------------------------------------------- INDEX_CACHE = { \u0026#34;loaded\u0026#34;: False, \u0026#34;chunks\u0026#34;: [], # list of dict (metadata + text, no embedding) \u0026#34;vectors\u0026#34;: [] # list of list[float] } # ----------------------------------------------------------------------------- # Helpers # ----------------------------------------------------------------------------- def cosine_similarity(a: List[float], b: List[float]) -\u0026gt; float: if not a or not b or len(a) != len(b): return 0.0 dot = 0.0 na = 0.0 nb = 0.0 for x, y in zip(a, b): dot += x * y na += x * x nb += y * y if na == 0.0 or nb == 0.0: return 0.0 return dot / math.sqrt(na * nb) def get_embedding(text: str) -\u0026gt; List[float]: if not text or not text.strip(): raise ValueError(\u0026#34;Query text is empty\u0026#34;) model_id = EMBED_MODEL_ID # Nếu là Cohere Embed v3 (english/multilingual) if model_id.startswith(\u0026#34;cohere.embed-\u0026#34;): # Query → dùng search_query body_dict = { \u0026#34;texts\u0026#34;: [text], \u0026#34;input_type\u0026#34;: \u0026#34;search_query\u0026#34; # Có thể thêm \u0026#34;truncate\u0026#34;: \u0026#34;END\u0026#34; nếu cần } else: # Mặc định: Titan embeddings body_dict = { \u0026#34;inputText\u0026#34;: text } body = json.dumps(body_dict) try: response = bedrock.invoke_model( modelId=model_id, body=body, contentType=\u0026#34;application/json\u0026#34;, accept=\u0026#34;application/json\u0026#34;, ) except ClientError as e: logger.error(\u0026#34;Bedrock invoke_model failed: %s\u0026#34;, e) raise response_body = json.loads(response[\u0026#34;body\u0026#34;].read()) # Parse output tùy model if model_id.startswith(\u0026#34;cohere.embed-\u0026#34;): # \u0026#34;embeddings\u0026#34;: [ [1024 floats] ] embeddings = response_body.get(\u0026#34;embeddings\u0026#34;) if not embeddings or not isinstance(embeddings, list): raise ValueError(\u0026#34;No embeddings found in Cohere response\u0026#34;) return embeddings[0] else: # Titan: {\u0026#34;embedding\u0026#34;: [..]} embedding = response_body.get(\u0026#34;embedding\u0026#34;) if not embedding: raise ValueError(\u0026#34;No embedding found in Titan response\u0026#34;) return embedding def load_index_if_needed(): if INDEX_CACHE[\u0026#34;loaded\u0026#34;]: return logger.info( \u0026#34;Loading legal index from s3://%s/%s ...\u0026#34;, LEGAL_INDEX_BUCKET, LEGAL_INDEX_KEY ) try: obj = s3.get_object(Bucket=LEGAL_INDEX_BUCKET, Key=LEGAL_INDEX_KEY) except ClientError as e: logger.error(\u0026#34;Failed to get index object from S3: %s\u0026#34;, e) raise chunks: List[Dict[str, Any]] = [] vectors: List[List[float]] = [] for line in obj[\u0026#34;Body\u0026#34;].iter_lines(): if not line: continue try: rec = json.loads(line.decode(\u0026#34;utf-8\u0026#34;)) except json.JSONDecodeError: logger.warning(\u0026#34;Invalid JSON line in index, skipped\u0026#34;) continue emb = rec.get(\u0026#34;embedding\u0026#34;) text = (rec.get(\u0026#34;text\u0026#34;) or \u0026#34;\u0026#34;).strip() if not emb or not text: # Bỏ qua record thiếu embedding / text continue # Tách embedding ra khỏi metadata rec_no_emb = dict(rec) rec_no_emb.pop(\u0026#34;embedding\u0026#34;, None) vectors.append(emb) chunks.append(rec_no_emb) INDEX_CACHE[\u0026#34;chunks\u0026#34;] = chunks INDEX_CACHE[\u0026#34;vectors\u0026#34;] = vectors INDEX_CACHE[\u0026#34;loaded\u0026#34;] = True logger.info( \u0026#34;Loaded %d chunks with embeddings into cache\u0026#34;, len(chunks) ) def parse_event_body(event: Dict[str, Any]) -\u0026gt; Dict[str, Any]: if \u0026#34;body\u0026#34; not in event: return event body = event[\u0026#34;body\u0026#34;] if event.get(\u0026#34;isBase64Encoded\u0026#34;): body = base64.b64decode(body).decode(\u0026#34;utf-8\u0026#34;) # cần import base64 nếu dùng try: data = json.loads(body) except json.JSONDecodeError: raise ValueError(\u0026#34;Request body must be valid JSON\u0026#34;) return data def apply_filters(rec: Dict[str, Any], filters: Dict[str, Any]) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34; filters dạng: { \u0026#34;source_type\u0026#34;: [\u0026#34;legal\u0026#34;, \u0026#34;template\u0026#34;], \u0026#34;doc_category\u0026#34;: [\u0026#34;luat\u0026#34;, \u0026#34;nghi_dinh\u0026#34;], \u0026#34;field\u0026#34;: [\u0026#34;Xây dựng - Đô thị\u0026#34;] } Trả về True nếu record PASS filter. \u0026#34;\u0026#34;\u0026#34; if not filters: return True # source_type st_list = filters.get(\u0026#34;source_type\u0026#34;) if st_list: st_val = rec.get(\u0026#34;source_type\u0026#34;) if st_val not in st_list: return False # doc_category cat_list = filters.get(\u0026#34;doc_category\u0026#34;) if cat_list: cat_val = rec.get(\u0026#34;doc_category\u0026#34;) if cat_val not in cat_list: return False # field field_list = filters.get(\u0026#34;field\u0026#34;) if field_list: field_val = rec.get(\u0026#34;field\u0026#34;) if field_val not in field_list: return False return True def search_index(query: str, top_k: int, filters: Dict[str, Any]) -\u0026gt; List[Dict[str, Any]]: load_index_if_needed() q_emb = get_embedding(query) scores: List[Tuple[float, int]] = [] for i, vec in enumerate(INDEX_CACHE[\u0026#34;vectors\u0026#34;]): s = cosine_similarity(q_emb, vec) scores.append((s, i)) # sort từ cao xuống thấp scores.sort(key=lambda x: x[0], reverse=True) results: List[Dict[str, Any]] = [] for score, idx in scores: if score \u0026lt;= 0: break rec = INDEX_CACHE[\u0026#34;chunks\u0026#34;][idx] if not apply_filters(rec, filters): continue # copy metadata + thêm score res = dict(rec) res[\u0026#34;score\u0026#34;] = score results.append(res) if len(results) \u0026gt;= top_k: break return results def make_response(status_code: int, body: Dict[str, Any]) -\u0026gt; Dict[str, Any]: return { \u0026#34;statusCode\u0026#34;: status_code, \u0026#34;headers\u0026#34;: { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;Access-Control-Allow-Origin\u0026#34;: \u0026#34;*\u0026#34;, # cho demo }, \u0026#34;body\u0026#34;: json.dumps(body, ensure_ascii=False), } # ----------------------------------------------------------------------------- # Lambda handler # ----------------------------------------------------------------------------- def lambda_handler(event, context): logger.info(\u0026#34;Received event: %s\u0026#34;, json.dumps(event)[:1000]) try: body = parse_event_body(event) # ================================================================== # 🔥 ĐOẠN CODE MỚI: XỬ LÝ KEEP WARM # ================================================================== if body.get(\u0026#34;keep_warm\u0026#34;): logger.info(\u0026#34;Keep-warm ping received. Checking index cache...\u0026#34;) # Quan trọng: Gọi hàm này để tải file từ S3 vào RAM (nếu chưa có) # Giúp người dùng tiếp theo không bị chờ lâu (Cold Start) load_index_if_needed() return make_response(200, {\u0026#34;message\u0026#34;: \u0026#34;Pong! I am warm and index is loaded.\u0026#34;}) # ================================================================== query = (body.get(\u0026#34;query\u0026#34;) or \u0026#34;\u0026#34;).strip() if not query: return make_response(400, {\u0026#34;error\u0026#34;: \u0026#34;query is required\u0026#34;}) language = (body.get(\u0026#34;language\u0026#34;) or \u0026#34;vi\u0026#34;).lower() top_k = body.get(\u0026#34;top_k\u0026#34;) or 10 try: top_k = int(top_k) if top_k \u0026lt;= 0: top_k = 10 except Exception: top_k = 10 filters = body.get(\u0026#34;filters\u0026#34;) or {} # Thực hiện tìm kiếm (khi đã chắc chắn index được load) results = search_index(query=query, top_k=top_k, filters=filters) resp = { \u0026#34;query\u0026#34;: query, \u0026#34;language\u0026#34;: language, \u0026#34;top_k\u0026#34;: top_k, \u0026#34;results\u0026#34;: results, } return make_response(200, resp) except ValueError as ve: return make_response(400, {\u0026#34;error\u0026#34;: str(ve)}) except ClientError as ce: logger.error(\u0026#34;AWS client error: %s\u0026#34;, ce) return make_response( 502, {\u0026#34;error\u0026#34;: \u0026#34;Upstream AWS error\u0026#34;, \u0026#34;details\u0026#34;: str(ce)}, ) except Exception as e: logger.error(\u0026#34;Unexpected error: %s\u0026#34;, e) return make_response( 500, {\u0026#34;error\u0026#34;: \u0026#34;Internal server error\u0026#34;, \u0026#34;details\u0026#34;: str(e)}, ) Bấm nút Deploy (nút xám) để lưu thay đổi. 3. Cấu hình (Configuration) Chuyển sang tab Configuration để thiết lập thông số kỹ thuật.\nA. General configuration\nChọn mục General configuration ở menu trái -\u0026gt; Bấm Edit. Memory: Tăng lên 3000 MB. Timeout: Tăng lên 1 min 0 sec. Bấm Save. B. Environment variables\nChọn mục Environment variables ở menu trái -\u0026gt; Bấm Edit. Bấm Add environment variable và thêm 2 dòng sau: Key: LEGAL_INDEX_BUCKET | Value: \u0026lt;Tên-Bucket-S3-Của-Bạn\u0026gt; Key: LEGAL_INDEX_KEY | Value: legal-corpus/index/legal_chunks_with_emb.jsonl Bấm Save. 4. Cấp quyền (Permissions) Chọn mục Permissions ở menu trái. Bấm vào Role name (đường link màu xanh) để mở trang IAM Role. Tại trang IAM vừa mở: Bấm Add permissions -\u0026gt; Attach policies. Tìm kiếm và tích chọn 2 policies sau: AmazonBedrockFullAccess AmazonS3FullAccess Bấm Add permissions. "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/vi/5-workshop/5.1-workshop-overview/",
	"title": "Tổng quan",
	"tags": [],
	"description": "",
	"content": "Xây dựng Trợ lý Hợp đồng Thông minh (Smart Contract Assistant) trên AWS Giới thiệu Nền tảng AI Contract Intelligence là một dịch vụ web dành cho cá nhân và các nhóm người dùng nhỏ (freelancer, chủ doanh nghiệp nhỏ, nhân sự hành chính/pháp lý) làm việc với hợp đồng hằng ngày nhưng không có chuyên môn pháp lý sâu. Giải pháp sử dụng Amazon Bedrock và kiến trúc AWS serverless hoàn toàn để phân tích hợp đồng, làm nổi bật rủi ro, gợi ý chỉnh sửa điều khoản, và tạo tóm tắt cũng như mẫu hợp đồng mới.\nĐược xây dựng trên AWS Amplify, Lambda, API Gateway, DynamoDB, S3, Cognito, EventBridge và CloudWatch, nền tảng cung cấp khả năng rà soát hợp đồng bằng AI với độ trễ thấp, chi phí thấp và bảo mật cao, được tối ưu cho người dùng đơn lẻ hoặc các nhóm nhỏ mà không cần tính năng phức tạp như hệ thống doanh nghiệp.\nMục tiêu chính của ứng dụng là hỗ trợ người dùng (như luật sư, nhân viên pháp chế hoặc chủ doanh nghiệp) thực hiện các tác vụ phức tạp như:\nTra cứu thông tin: Hỏi đáp về các điều khoản luật dựa trên kho dữ liệu văn bản pháp lý có sẵn. Soạn thảo tự động: Yêu cầu AI tạo ra các bản nháp hợp đồng dựa trên các template mẫu và thông tin cung cấp. Phân tích: Tóm tắt và kiểm tra nội dung hợp đồng. Kiến trúc giải pháp Hệ thống được thiết kế theo kiến trúc Event-driven Serverless, giúp tối ưu hóa chi phí (chỉ trả tiền khi sử dụng) và khả năng mở rộng tự động. Chúng ta sẽ áp dụng kỹ thuật RAG (Retrieval-Augmented Generation) để cung cấp ngữ cảnh dữ liệu riêng (vector data) cho mô hình AI, giúp câu trả lời chính xác và thực tế hơn. Các thành phần dịch vụ AWS cốt lõi bao gồm:\nAmazon Bedrock (Generative AI):\nĐóng vai trò là bộ não xử lý ngôn ngữ tự nhiên. Chúng ta sẽ sử dụng các mô hình nền tảng (Foundation Models) như Claude 3 (Haiku/Sonnet) thông qua API để thực hiện việc hiểu câu hỏi, sinh văn bản và xử lý logic pháp lý. AWS Lambda \u0026amp; Amazon API Gateway (Backend):\nAWS Lambda: Chạy các function Python để xử lý logic nghiệp vụ, như: tìm kiếm vector (search vectors), gọi Bedrock API, và ghép nối dữ liệu. Amazon API Gateway: Tạo các điểm cuối HTTP (RESTful API) an toàn để Frontend có thể giao tiếp với Backend. Amazon S3 (Lưu trữ):\nLưu trữ các file tĩnh như tài liệu hợp đồng mẫu (.docx, .pdf). Lưu trữ cơ sở dữ liệu vector (embeddings) của các văn bản luật để phục vụ cho tính năng tìm kiếm ngữ nghĩa (Semantic Search). Amazon DynamoDB (Cơ sở dữ liệu):\nLưu trữ thông tin người dùng (Users). Quản lý các phiên làm việc (Chat Sessions) và lưu lại toàn bộ lịch sử đoạn chat (Chat Messages) với độ trễ thấp nhất. AWS Amplify \u0026amp; Amazon Cognito (Frontend \u0026amp; Auth):\nAWS Amplify: Tự động hóa việc triển khai (CI/CD) và hosting cho ứng dụng web (React/Vue). Amazon Cognito: Quản lý định danh người dùng, cho phép đăng ký/đăng nhập an toàn và cấp quyền truy cập vào API. Mục tiêu học tập Sau khi hoàn thành workshop, bạn sẽ đạt được:\nHiểu rõ quy trình xây dựng ứng dụng GenAI từ Backend đến Frontend. Kinh nghiệm thực tế khi làm việc với Amazon Bedrock và kỹ thuật RAG. Khả năng thiết lập và cấu hình các dịch vụ Serverless: Lambda, DynamoDB, API Gateway. Kỹ năng triển khai ứng dụng web hiện đại với AWS Amplify. "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/vi/1-worklog/1.1-week1/",
	"title": "Worklog Tuần 1",
	"tags": [],
	"description": "",
	"content": "📘 Báo cáo công việc Tuần 1 – AWS Journey 1. Mục tiêu hàng tuần Mục tiêu chính của Tuần 1 là thiết lập môi trường nền tảng cho hành trình AWS và hiểu các nguyên tắc vận hành cốt lõi. Các mục tiêu cụ thể bao gồm:\nOnboarding (Nhập môn): Làm quen với quy trình thực tập FCJ, các kênh liên lạc và nội quy. Thiết lập tài khoản: Hoàn tất đăng ký AWS Free Tier, cấu hình AWS CLI, và kích hoạt các chuẩn bảo mật cơ bản (MFA, IAM). Dịch vụ cốt lõi: Có cái nhìn tổng quan về hệ sinh thái AWS (Tính toán, Lưu trữ, Mạng, Cơ sở dữ liệu, Bảo mật). Thực hành: Sử dụng thành thạo AWS Management Console \u0026amp; AWS CLI v2. Cơ sở hạ tầng: Triển khai và vận hành một instance EC2 t2.micro và thực hiện các thao tác EBS cơ bản. Kiểm soát chi phí: Thiết lập AWS Budgets để giám sát chi tiêu. 2. Tóm tắt công việc chi tiết 🗂 Kế hoạch thực hiện so với Thực tế Hạng mục Kế hoạch Thực tế Trạng thái Onboarding \u0026amp; Nội quy Giới thiệu, nắm bắt kênh liên lạc Đã được giới thiệu, ghi chú chuẩn báo cáo ✅ Hoàn thành Tổng quan AWS Hệ thống hóa nhóm dịch vụ + Mindmap Hoàn tất, đã ghi chú theo phân loại ✅ Hoàn thành Free Tier \u0026amp; Bảo mật Tạo tài khoản, bật MFA, tạo IAM user Đã bật MFA; tạo user + nhóm Viewer ✅ Hoàn thành AWS CLI Cài đặt CLI, cấu hình profile Đã set profile acj-student, test sts OK ✅ Hoàn thành EC2/EBS/SSH Tạo EC2, SSH, gắn EBS EC2 t2.micro + EBS 8GB gp3, SSH thành công ✅ Hoàn thành Quản lý chi phí Đặt ngân sách $5/tháng Đã nhận email cảnh báo thử nghiệm ✅ Hoàn thành 📅 Nhật ký hoạt động theo ngày Thứ Nhiệm vụ Ngày bắt đầu Ngày hoàn thành Tài liệu tham khảo Thứ Hai Onboarding: Định hướng FCJ, đọc nội quy, học chuẩn báo cáo 08/09 08/09 AWS Journey Thứ Ba Nghiên cứu: Khám phá hệ sinh thái AWS (Compute/Storage/Networking/DB/Security), tạo mindmap 09/09 09/09 AWS Journey Thứ Tư Thiết lập tài khoản: Tạo AWS Free Tier, bật MFA cho root, tạo IAM user + nhóm Viewer 10/09 10/09 AWS Journey Thứ Năm Cài đặt CLI: Cài AWS CLI v2 (Windows), chạy aws configure (profile acj-student), kiểm tra danh tính sts 11/09 11/09 AWS Journey Thứ Sáu Lý thuyết: Học về EC2 (loại instance, AMI, EBS, SG, Elastic IP) + checklist Free Tier 12/09 12/09 AWS Journey Thứ Bảy Thực hành: Tạo EC2 t2.micro (AL2023), tạo/dùng key pair (.pem), SSH; gắn EBS 8GB, định dạng \u0026amp; mount 13/09 13/09 AWS Journey 3. Kết quả \u0026amp; Minh chứng 3.1 Tài nguyên đã tạo IAM: 01 User làm việc hàng ngày (Nhóm: Viewer), đã bật MFA cho tài khoản root. EC2: t2.micro (Free Tier), AMI: Amazon Linux 2023. Security Group: Quy tắc Inbound mở cổng 22/tcp chỉ giới hạn cho My IP. EBS: Volume 8GB gp3, đã định dạng (xfs) và mount vào thư mục /data. Budgets: Ngân sách hàng tháng đặt mức $5 USD với cảnh báo qua email. CLI Region: Mặc định là ap-southeast-1 (Singapore). 3.2 Các lệnh CLI đã thực thi aws sts get-caller-identity --profile acj-student aws ec2 describe-regions --profile acj-student --output table aws ec2 describe-instances --profile acj-student --region ap-southeast-1 aws ec2 create-key-pair --key-name fcj-key --query \u0026#34;KeyMaterial\u0026#34; --output text \u0026gt; fcj-key.pem "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/vi/1-worklog/1.2-week2/",
	"title": "Worklog Tuần 2",
	"tags": [],
	"description": "",
	"content": "📘 Báo cáo công việc Tuần 2 – AWS Journey 1. Mục tiêu hàng tuần Trong Tuần 2, mục tiêu chính là đạt được kinh nghiệm thực tế nền tảng với các dịch vụ cơ sở hạ tầng cốt lõi của AWS, bao gồm:\nAmazon S3 – Lưu trữ trang web tĩnh và quản lý quyền truy cập bucket. Amazon RDS (MySQL) – Cấp phát cơ sở dữ liệu quan hệ được quản lý và cấu hình kết nối. Amazon EC2 – Sử dụng EC2 instance như một máy chủ trung gian (bastion host) bảo mật để truy cập RDS. Amazon Route53 – Quản lý tên miền và ánh xạ bản ghi DNS tới các dịch vụ AWS. Tuần này tập trung vào việc xây dựng các thành phần kiến trúc đám mây cơ bản, đóng vai trò là tiền đề cho các nhiệm vụ của Tuần 3 liên quan đến CloudFront, DynamoDB và ElastiCache.\n2. Tóm tắt công việc chi tiết 🗂 Bảng hoạt động Ngày Nhiệm vụ Ngày bắt đầu Ngày hoàn thành Tham khảo Thứ Hai - Tạo S3 bucket cho nội dung web tĩnh\n- Tải lên các tệp demo HTML/CSS ban đầu 15/09/2025 15/09/2025 AWS Journey Thứ Ba - Bật tính năng Static Website Hosting trên S3\n- Cấu hình Bucket Policy cho phép quyền đọc công khai (public read)\n- Kiểm tra truy cập website qua endpoint S3 16/09/2025 16/09/2025 AWS Journey Thứ Tư - Tạo RDS MySQL instance (Gói Free Tier)\n- Cấu hình VPC Security Groups cho lưu lượng truy cập vào\n- Ghi lại endpoint DB \u0026amp; thông tin đăng nhập 17/09/2025 17/09/2025 AWS Journey Thứ Năm - Khởi chạy EC2 instance và cài đặt MySQL client\n- Kết nối từ EC2 → RDS bằng dòng lệnh\n- Thực thi các truy vấn thử nghiệm và tạo bảng mẫu 18/09/2025 18/09/2025 AWS Journey Thứ Sáu - Tìm hiểu chức năng Route53\n- Tạo Hosted Zone và các bản ghi DNS (A/CNAME)\n- Cấu hình định tuyến từ tên miền tùy chỉnh → trang web tĩnh S3\n- Xác thực truy cập website bằng tên miền 19/09/2025 19/09/2025 AWS Journey 3. Chi tiết triển khai kỹ thuật 3.1 AWS S3 – Thiết lập Website tĩnh Tạo S3 bucket mới tuân theo quy ước đặt tên và vị trí vùng (region). Tải lên tài nguyên tĩnh (HTML/CSS/Hình ảnh). Bật tính năng Static Website Hosting. Cấu hình index.html và error.html. Thêm Bucket Policy (quyền public-read) để phục vụ nội dung toàn cầu. Xác minh khả năng truy cập qua endpoint của website: http://\u0026lt;bucket-name\u0026gt;.s3-website-\u0026lt;region\u0026gt;.amazonaws.com 3.2 Amazon RDS – Cấp phát Cơ sở dữ liệu Khởi chạy instance RDS MySQL 8.0 thuộc gói Free Tier. Áp dụng các quy tắc Security Group bảo mật (EC2 → RDS, cổng 3306). Lưu trữ endpoint được tạo để kết nối sau này. Đảm bảo DB subnet group và cấu hình VPC hợp lệ cho truy cập riêng tư. 3.3 Amazon EC2 – Kết nối DB bảo mật Tạo một instance EC2 t2.micro trong cùng VPC với RDS instance. Cài đặt MySQL Client: sudo yum install mysql -y Kết nối thành công đến RDS: mysql -h \u0026lt;rds-endpoint\u0026gt; -u admin -p Tạo cơ sở dữ liệu mẫu và bảng để kiểm tra. 3.4 Amazon Route53 – Cấu hình DNS Thiết lập một Hosted Zone mới. Thêm các bản ghi DNS: A Record → Trang web tĩnh S3. CNAME Record cho các bí danh thử nghiệm. Chờ DNS lan truyền (thường từ 1–5 phút). Truy cập thành công trang web tĩnh bằng tên miền tùy chỉnh. 4. Thành tựu Đến cuối Tuần 2, các kết quả sau đã đạt được:\n✔ Thành công về mặt chức năng Hoàn thành một trang web tĩnh được lưu trữ trên S3 hoạt động hoàn chỉnh. Cho phép truy cập qua cả endpoint S3 và tên miền Route53. Triển khai và kết nối thành công cơ sở dữ liệu RDS MySQL. Xác minh giao tiếp bảo mật giữa EC2 ↔ RDS. ✔ Phát triển kỹ năng Thể hiện sự hiểu biết về: IAM roles \u0026amp; quyền hạn. Kiểm soát truy cập S3. Mạng VPC \u0026amp; Security Groups. Các khái niệm định tuyến DNS. Có được kinh nghiệm nền tảng với các dịch vụ cốt lõi của AWS. Củng cố hiểu biết về luồng ứng dụng đám mây đầu cuối (end-to-end). Xây dựng sự tự tin khi làm việc với các thao tác dòng lệnh (CLI). Cải thiện kỹ năng khắc phục sự cố (lan truyền DNS, cấu hình SG và chính sách truy cập công khai). 5. Thách thức gặp phải \u0026amp; Giải pháp Thách thức 1: S3 Public Access Block (Chặn truy cập công khai)\nVấn đề: Website không thể truy cập do cài đặt chặn truy cập công khai mặc định của S3. Giải pháp: Tắt cài đặt “Block Public Access” và thêm bucket policy chính xác. Thách thức 2: Lỗi kết nối EC2 -\u0026gt; RDS (Timeout)\nVấn đề: Security Group không cho phép lưu lượng MySQL đi vào. Giải pháp: Sửa đổi Security Group của RDS để chấp nhận lưu lượng cụ thể từ Security Group của EC2 trên cổng 3306. Thách thức 3: DNS không phân giải ngay lập tức\nVấn đề: Tên miền mất thời gian để cập nhật trong Route53. Giải pháp: Chờ hết thời gian TTL và kiểm tra lại bằng lệnh dig / nslookup. "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/vi/1-worklog/1.3-week3/",
	"title": "Worklog Tuần 3",
	"tags": [],
	"description": "",
	"content": "📘 Báo cáo công việc Tuần 3 – AWS Journey 1. Mục tiêu hàng tuần Trong Tuần 3, mục tiêu chính là tối ưu hóa hiệu suất ứng dụng và mở rộng kỹ năng quản lý dữ liệu ngoài cơ sở dữ liệu quan hệ. Các mục tiêu cụ thể bao gồm:\nAmazon CloudFront – Hiểu về Mạng phân phối nội dung (CDN) và tối ưu hóa việc phân phối trang web tĩnh. Amazon DynamoDB – Có kinh nghiệm thực tế với mô hình hóa và vận hành cơ sở dữ liệu NoSQL. Amazon ElastiCache (Redis) – Triển khai bộ nhớ đệm (caching) để cải thiện tốc độ đọc dữ liệu. Tích hợp AWS CLI – Tương tác nâng cao với các dịch vụ AWS bằng các tập lệnh dòng lệnh. Tuần này tập trung vào việc chuyển đổi từ kiến trúc cơ bản sang mô hình hiệu suất cao, có khả năng mở rộng bằng cách sử dụng các lớp caching và dịch vụ NoSQL được quản lý.\n2. Tóm tắt công việc chi tiết 🗂 Bảng hoạt động Ngày Nhiệm vụ Ngày bắt đầu Ngày hoàn thành Tham khảo Thứ Hai - Giới thiệu khái niệm CDN và lợi ích của CloudFront\n- Tạo CloudFront Distribution để phân phối nội dung web từ S3 22/09/2025 22/09/2025 AWS Journey Thứ Ba - Cấu hình hành vi (behaviors) và chính sách cache cho CloudFront\n- Kiểm tra truy cập web qua URL CloudFront\n- Thực hiện Invalidation (làm mới cache) để cập nhật nội dung mới 23/09/2025 23/09/2025 AWS Journey Thứ Tư - Giới thiệu DynamoDB (Kiến trúc NoSQL)\n- Tạo các bảng DynamoDB (Users, Products)\n- Thực hành các thao tác CRUD trên Console 24/09/2025 24/09/2025 AWS Journey Thứ Năm - Kết nối và truy vấn DynamoDB bằng AWS CLI\n- Viết các script nhỏ để thêm (put-item) và đọc dữ liệu 25/09/2025 25/09/2025 AWS Journey Thứ Sáu - Tìm hiểu về ElastiCache (Redis \u0026amp; Memcached)\n- Khởi tạo cụm Redis cơ bản\n- Kiểm tra kết nối từ EC2 để lưu/đọc dữ liệu cache 26/09/2025 26/09/2025 AWS Journey 3. Chi tiết triển khai kỹ thuật 3.1 Amazon CloudFront – Tích hợp CDN Tạo distribution trỏ đến S3 bucket đã tạo trong Tuần 2. Cấu hình Origin Access Control (OAC) để chỉ cho phép truy cập S3 thông qua CloudFront. Bật HTTPS sử dụng chứng chỉ mặc định của CloudFront. Kiểm tra sự cải thiện hiệu suất (giảm độ trễ) so với truy cập trực tiếp S3. Thực hiện invalidation thủ công cho tệp index.html: aws cloudfront create-invalidation --distribution-id \u0026lt;ID\u0026gt; --paths \u0026#34;/*\u0026#34; 3.2 Amazon DynamoDB – Triển khai NoSQL Tạo bảng Users với UserId làm Khóa phân vùng (Partition Key). Thực hiện các thao tác CRUD (Tạo, Đọc, Cập nhật, Xóa) thông qua Giao diện quản lý (Console). Tương tác qua AWS CLI để chèn dữ liệu: aws dynamodb put-item \\ --table-name Users \\ --item \u0026#39;{\u0026#34;UserId\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;u-101\u0026#34;}, \u0026#34;Name\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;Alice\u0026#34;}, \u0026#34;Role\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;Admin\u0026#34;}}\u0026#39; Xác thực dữ liệu đã chèn bằng lệnh scan. 3.3 Amazon ElastiCache – Thiết lập Redis Khởi chạy cụm ElastiCache for Redis (cache.t2.micro hoặc t3.micro). Cấu hình Security Groups để cho phép lưu lượng vào cổng 6379 từ EC2 instance. Kết nối từ EC2 bằng redis-cli (cài đặt qua amazon-linux-extras hoặc yum). Kiểm tra logic lưu bộ nhớ đệm: set mykey \u0026#34;Hello AWS\u0026#34; get mykey # Kết quả: \u0026#34;Hello AWS\u0026#34; 4. Thành tựu Đến cuối Tuần 3, các kết quả sau đã đạt được:\n✔ Thành công về mặt chức năng Tăng tốc thành công trang web tĩnh S3 trên phạm vi toàn cầu bằng CloudFront. Thể hiện kiến thức làm việc với cấu trúc dữ liệu NoSQL. Thiết lập cụm Redis cache hoạt động tốt, có thể truy cập từ tài nguyên VPC riêng tư. Tích hợp AWS CLI để quản lý cơ sở dữ liệu, vượt ra khỏi các thao tác chỉ dùng Console. ✔ Phát triển kỹ năng Hiểu rõ vai trò của các vị trí biên (edge locations) và chiến lược caching. Nắm vững sự khác biệt giữa mô hình Quan hệ (RDS) và NoSQL (DynamoDB). Học cách thực hiện Cache Invalidation khi cập nhật nội dung tĩnh. Có kinh nghiệm trong việc bảo mật các lớp cache nội bộ (ElastiCache) thông qua Security Groups. 5. Thách thức gặp phải \u0026amp; Giải pháp Thách thức 1: Nội dung CloudFront không cập nhật\nVấn đề: Các cập nhật cho index.html trên S3 không phản ánh ngay lập tức trên trang web. Giải pháp: Tìm hiểu về TTL (Time To Live) và thực hiện CloudFront Invalidation để buộc làm mới dữ liệu. Thách thức 2: Cú pháp phức tạp của DynamoDB CLI\nVấn đề: Gặp khó khăn khi định dạng JSON chính xác cho các lệnh CLI. Giải pháp: Sử dụng công cụ tạo JSON và tham khảo tài liệu AWS CLI để biết cú pháp AttributeValue chính xác (S, N, v.v.). Thách thức 3: Kết nối Redis từ máy cá nhân\nVấn đề: Cố gắng kết nối với ElastiCache từ bên ngoài VPC (thất bại). Giải pháp: Hiểu rằng ElastiCache chỉ dành cho mạng nội bộ VPC; sử dụng EC2 bastion host đã thiết lập ở Tuần 2 làm máy trung gian (jump box). "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/vi/1-worklog/1.4-week4/",
	"title": "Worklog Tuần 4",
	"tags": [],
	"description": "",
	"content": "📘 Báo cáo công việc Tuần 4 – AWS Journey 1. Mục tiêu hàng tuần Trong Tuần 4, trọng tâm chính chuyển sang các Chiến lược di chuyển (Migration) và Đảm bảo tính liên tục trong kinh doanh (Business Continuity). Mục tiêu là hiểu cách chuyển khối lượng công việc từ on-premise (tại chỗ) lên đám mây và đảm bảo hệ thống có khả năng phục hồi trước các sự cố. Các mục tiêu chính bao gồm:\nQuy trình Migration – Hiểu về \u0026ldquo;6 Rs\u0026rdquo; trong di chuyển (Rehost, Replatform, Refactor, v.v.). AWS Database Migration Service (DMS) – Di chuyển dữ liệu từ cơ sở dữ liệu nguồn sang Amazon RDS với thời gian ngừng hoạt động tối thiểu. Elastic Disaster Recovery (EDR) – Triển khai sao chép và chiến lược phục hồi để giảm thiểu mất mát dữ liệu. Lập kế hoạch Khôi phục thảm họa (DR) – Xác định RTO (Thời gian khôi phục mục tiêu) và RPO (Điểm khôi phục mục tiêu). Tuần này thiết lập các kỹ năng quan trọng cần thiết cho độ tin cậy và hiện đại hóa cơ sở hạ tầng cấp doanh nghiệp.\n2. Tóm tắt công việc chi tiết 🗂 Bảng hoạt động Ngày Nhiệm vụ Ngày bắt đầu Ngày hoàn thành Tham khảo Thứ Hai - Tìm hiểu các khái niệm Migration (Lift \u0026amp; Shift, Replatform, Refactor)\n- Giới thiệu về AWS Database Migration Service (DMS) 29/09/2025 29/09/2025 AWS Journey Thứ Ba - Thực hành tạo Replication Instance trong DMS\n- Cấu hình nguồn dữ liệu (giả lập on-premise) và đích (RDS)\n- Thực hiện di chuyển dữ liệu thử nghiệm 30/09/2025 30/09/2025 AWS Journey Thứ Tư - Giới thiệu về Elastic Disaster Recovery (EDR)\n- Tìm hiểu cách thiết lập máy chủ sao chép và instance khôi phục 01/10/2025 01/10/2025 AWS Journey Thứ Năm - Thực hành mô phỏng sự cố: tắt EC2 chính và khởi chạy instance khôi phục từ EDR\n- Đánh giá thời gian khôi phục (RTO/RPO) 02/10/2025 02/10/2025 AWS Journey Thứ Sáu - Tạo kế hoạch DR cơ bản (sao lưu, khôi phục, chuyển đổi dự phòng)\n- Viết tài liệu tổng hợp quy trình Migration + DR 03/10/2025 03/10/2025 AWS Journey 3. Chi tiết triển khai kỹ thuật 3.1 AWS Database Migration Service (DMS) Replication Instance: Cấp phát một instance dms.t2.micro trong VPC để xử lý tác vụ di chuyển. Cấu hình Endpoints: Source (Nguồn): Cấu hình cơ sở dữ liệu MySQL trên EC2 (mô phỏng máy chủ tại chỗ) với quyền truy cập phù hợp. Target (Đích): Kết nối với RDS MySQL instance đã tạo ở Tuần 2. Tác vụ Migration: Tạo tác vụ \u0026ldquo;Full Load\u0026rdquo; (Tải toàn bộ) để di chuyển các bảng hiện có. Quy tắc ánh xạ: Cấu hình quy tắc chọn lược đồ (schema) để bao gồm các bảng cụ thể (ví dụ: Users, Products). 3.2 Thiết lập Elastic Disaster Recovery (EDR) Khởi tạo dịch vụ EDR trong Region AWS cụ thể. Cài đặt Agent: Tải xuống và cài đặt AWS Replication Agent trên EC2 instance nguồn (Linux). Staging Area: Xác minh rằng máy chủ sao chép đã tự động khởi chạy trong subnet staging. Sao chép dữ liệu: Theo dõi tiến trình đồng bộ hóa ban đầu cho đến khi trạng thái đạt \u0026ldquo;Healthy\u0026rdquo; và \u0026ldquo;Data replicated\u0026rdquo;. 3.3 Mô phỏng chuyển đổi dự phòng (Failover Drill) Kịch bản: Mô phỏng sự cố nghiêm trọng bằng cách dừng (Stop) instance EC2 nguồn. Hành động khôi phục: Khởi tạo \u0026ldquo;Recovery Drill\u0026rdquo; trong giao diện điều khiển EDR. Cài đặt khởi chạy: Cấu hình Launch Template (loại instance, security groups) cho instance khôi phục. Xác thực: SSH thành công vào Recovery Instance đã khởi chạy và xác minh tính toàn vẹn của dữ liệu ứng dụng. 3.4 Lập kế hoạch \u0026amp; Tài liệu DR Soạn thảo kế hoạch DR cơ bản phác thảo: Chiến lược sao lưu: Snapshots tự động so với Sao chép liên tục. Các bước Failover: Trình tự các hành động để chuyển sang trang web khôi phục. Phân tích RTO/RPO: Đo lường thời gian khôi phục (RTO) và lượng dữ liệu có thể bị trễ (RPO). 4. Thành tựu Đến cuối Tuần 4, các kết quả sau đã đạt được:\n✔ Thành công về mặt chức năng Di chuyển dữ liệu thành công giữa hai điểm cuối cơ sở dữ liệu bằng AWS DMS. Cấu hình sao chép liên tục ở cấp độ khối (block-level) bằng Elastic Disaster Recovery. Thực hiện thành công cuộc diễn tập failover, đưa máy chủ khôi phục hoạt động trong vòng vài phút. Xác minh tính nhất quán dữ liệu giữa hệ thống Nguồn và Đích. ✔ Phát triển kỹ năng Hiểu rõ trọn vẹn Vòng đời di chuyển (Migration lifecycle) (Đánh giá → Huy động → Di chuyển \u0026amp; Hiện đại hóa). Có kinh nghiệm thực tế với các khái niệm mạng Hybrid Cloud (Nguồn → AWS). Hiểu sâu hơn về Lập kế hoạch kinh doanh liên tục (BCP). Học cách phân biệt giữa các chiến lược Sao lưu (Backup) và giải pháp Khôi phục thảm họa (Disaster Recovery). 5. Thách thức gặp phải \u0026amp; Giải pháp Thách thức 1: Lỗi kết nối DMS\nVấn đề: Replication Instance không thể kết nối với cơ sở dữ liệu EC2 nguồn. Giải pháp: Cập nhật Security Group của nguồn để cho phép lưu lượng vào cổng 3306 cụ thể từ Private IP của DMS Replication Instance. Thách thức 2: Lỗi cài đặt EDR Agent\nVấn đề: Agent sao chép không cài đặt được do thiếu quyền IAM. Giải pháp: Tạo một IAM user với các access keys lập trình cụ thể cần thiết cho AWS Replication Agent và chạy lại trình cài đặt. Thách thức 3: RTO cao trong quá trình diễn tập\nVấn đề: Instance khôi phục mất nhiều thời gian hơn dự kiến để sẵn sàng. Giải pháp: Tối ưu hóa Launch Template để sử dụng AMI phù hợp hoặc loại instance tốt hơn để tăng tốc quá trình khởi động. "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/vi/1-worklog/1.5-week5/",
	"title": "Worklog Tuần 5",
	"tags": [],
	"description": "",
	"content": "📘 Báo cáo công việc Tuần 5 – AWS Journey 1. Mục tiêu hàng tuần Trong Tuần 5, trọng tâm chuyển từ các thao tác thủ công (\u0026ldquo;ClickOps\u0026rdquo;) sang Cơ sở hạ tầng dưới dạng mã (IaC) và Vận hành hệ thống. Mục tiêu là tự động hóa việc cấp phát và quản lý tài nguyên để đảm bảo tính nhất quán và tốc độ. Các mục tiêu chính bao gồm:\nCơ sở hạ tầng dưới dạng mã (IaC) – Tìm hiểu AWS CloudFormation và AWS Cloud Development Kit (CDK). Tự động hóa – Viết các mẫu (templates) và mã để triển khai S3 buckets và EC2 instances bằng lập trình. AWS Systems Manager (SSM) – Tập trung hóa dữ liệu vận hành và quản lý máy chủ mà không cần khóa SSH. Tối ưu vận hành – Triển khai các quy trình tự động khởi động/dừng máy chủ để tiết kiệm chi phí. Tuần này đánh dấu sự chuyển đổi sang các thực hành DevOps, chuẩn bị cho việc quản lý cơ sở hạ tầng có khả năng mở rộng.\n2. Tóm tắt công việc chi tiết 🗂 Bảng hoạt động Ngày Nhiệm vụ Ngày bắt đầu Ngày hoàn thành Tham khảo Thứ Hai - Giới thiệu về khái niệm IaC và lợi ích so với triển khai thủ công\n- Làm quen với AWS CloudFormation: template, stack, parameter 06/10/2025 06/10/2025 AWS Journey Thứ Ba - Viết CloudFormation template để triển khai S3 bucket và EC2 instance\n- Tạo, cập nhật và xóa stack thông qua AWS Console 07/10/2025 07/10/2025 AWS Journey Thứ Tư - Giới thiệu về AWS CDK (Cloud Development Kit)\n- Cài đặt AWS CDK, tạo dự án CDK bằng Python hoặc TypeScript\n- Viết mã CDK để triển khai EC2 instance 08/10/2025 08/10/2025 AWS Journey Thứ Năm - Giới thiệu về AWS Systems Manager (SSM) và các tính năng chính\n- Tạo Parameter Store để lưu trữ các biến cấu hình 09/10/2025 09/10/2025 AWS Journey Thứ Sáu - Thực hành tạo Automation Document trong SSM để tự động Start/Stop EC2\n- Test Session Manager (truy cập EC2 không cần khóa SSH)\n- Tổng kết tuần: Demo IaC + SSM 10/10/2025 10/10/2025 AWS Journey 3. Chi tiết triển khai kỹ thuật 3.1 AWS CloudFormation Thiết kế Template: Tạo tệp YAML định nghĩa AWS::S3::Bucket và AWS::EC2::Instance. Parameters (Tham số): Sử dụng Parameters để cho phép nhập InstanceType (ví dụ: t2.micro) tại thời điểm triển khai. Thao tác với Stack: Create Stack: Tải template lên CloudFormation Designer. Update Stack: Sửa đổi template (thêm tags) và áp dụng changeset. Drift Detection: Kiểm tra xem tài nguyên có bị thay đổi thủ công bên ngoài stack hay không. 3.2 AWS CDK (Cloud Development Kit) Cài đặt: Cài đặt Node.js và CDK CLI (npm install -g aws-cdk). Khởi tạo: Tạo dự án mới: cdk init app --language python. Viết mã: Định nghĩa tài nguyên sử dụng các cấu trúc cấp cao (L2 constructs) bằng Python. Triển khai: cdk synth: Tạo ra template CloudFormation từ mã nguồn. cdk deploy: Cấp phát tài nguyên vào tài khoản AWS. 3.3 AWS Systems Manager (SSM) Parameter Store: Tạo các tham số phân cấp (ví dụ: /dev/db/password) dạng SecureString để lưu cấu hình nhạy cảm. Session Manager: Gắn IAM role AmazonSSMManagedInstanceCore vào EC2 instance. Kết nối thành công vào shell của instance thông qua AWS Console (trình duyệt) mà không cần mở cổng 22 (SSH). Automation: Thực thi một SSM Document (AWS-StopEC2Instance) để kiểm tra các tác vụ vận hành tự động. 4. Thành tựu Đến cuối Tuần 5, các kết quả sau đã đạt được:\n✔ Thành công về mặt chức năng Thay thế thành công việc tạo tài nguyên thủ công bằng các CloudFormation templates có thể tái sử dụng. Triển khai một stack cơ sở hạ tầng hoạt động tốt bằng mã lệnh (CDK). Loại bỏ nhu cầu quản lý khóa SSH nhờ sử dụng SSM Session Manager. Tập trung hóa việc quản lý cấu hình bằng SSM Parameter Store. ✔ Phát triển kỹ năng Hiểu sự khác biệt giữa các phương pháp IaC Declarative (Khai báo - CloudFormation) và Imperative (Mệnh lệnh - CDK). Học được tầm quan trọng của tính Idempotency (Tính bất biến/nhất quán) trong triển khai cơ sở hạ tầng. Có kinh nghiệm quản lý dựa trên Agent (SSM Agent). Cải thiện tư thế bảo mật bằng cách loại bỏ nhu cầu truy cập SSH công khai. 5. Thách thức gặp phải \u0026amp; Giải pháp Thách thức 1: Lỗi thụt lề (Indentation) trong CloudFormation YAML\nVấn đề: Việc tạo Stack thất bại do lỗi phân tích cú pháp trong tệp YAML. Giải pháp: Sử dụng YAML Linter và tiện ích mở rộng VS Code \u0026ldquo;CloudFormation Linter\u0026rdquo; để xác thực cú pháp trước khi tải lên. Thách thức 2: CDK Bootstrapping\nVấn đề: Lệnh cdk deploy thất bại với lỗi thiếu toolkit stack. Giải pháp: Hiểu rằng môi trường phải được bootstrap một lần cho mỗi region bằng lệnh cdk bootstrap aws://\u0026lt;account-id\u0026gt;/\u0026lt;region\u0026gt;. Thách thức 3: SSM Agent không kết nối\nVấn đề: EC2 instance không xuất hiện trong Systems Manager Fleet Manager. Giải pháp: Phát hiện ra EC2 instance thiếu IAM Role cần thiết (AmazonSSMManagedInstanceCore). Đã gắn role và khởi động lại instance. "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/vi/1-worklog/1.6-week6/",
	"title": "Worklog Tuần 6",
	"tags": [],
	"description": "",
	"content": "📘 Báo cáo công việc Tuần 6 – AWS Journey 1. Mục tiêu hàng tuần Trong Tuần 6, trọng tâm chuyển sang các trụ cột quan trọng của khung kiến trúc Well-Architected: Bảo mật (Security) và Tối ưu hóa chi phí (Cost Optimization). Các mục tiêu chính bao gồm:\nQuản lý danh tính \u0026amp; truy cập (IAM) – Nắm vững cấu trúc chính sách nâng cao để thực thi Nguyên tắc đặc quyền tối thiểu (Least Privilege). Bảo mật dữ liệu – Triển khai mã hóa bằng AWS KMS và quản lý thông tin xác thực với Secrets Manager. Quản lý chi phí – Phân tích mô hình chi tiêu thông qua Cost Explorer và thiết lập cảnh báo tự động với AWS Budgets. Tuần này đảm bảo rằng cơ sở hạ tầng được xây dựng trong các tuần trước không chỉ hoạt động tốt mà còn an toàn và hiệu quả về mặt tài chính.\n2. Tóm tắt công việc chi tiết 🗂 Bảng hoạt động Ngày Nhiệm vụ Ngày bắt đầu Ngày hoàn thành Tham khảo Thứ Hai - Ôn tập kiến thức IAM cơ bản\n- Học IAM Policy nâng cao (Cấu trúc JSON, Điều kiện)\n- Tạo chính sách tùy chỉnh và gắn cho users/groups 13/10/2025 13/10/2025 AWS Journey Thứ Ba - Giới thiệu về AWS Key Management Service (KMS)\n- Tạo Customer Managed Key (CMK)\n- Áp dụng KMS để mã hóa S3 bucket hoặc EBS volume 14/10/2025 14/10/2025 AWS Journey Thứ Tư - Làm quen với AWS Secrets Manager\n- Tạo secret để lưu thông tin kết nối Database\n- Viết script Lambda nhỏ để đọc secret từ Secrets Manager 15/10/2025 15/10/2025 AWS Journey Thứ Năm - Khám phá AWS Billing Dashboard và Cost Explorer\n- Xem chi phí theo dịch vụ, khu vực và loại sử dụng\n- Thiết lập Cost Anomaly Detection (Phát hiện bất thường) 16/10/2025 16/10/2025 AWS Journey Thứ Sáu - Tạo AWS Budget và cấu hình cảnh báo qua email\n- Viết báo cáo tổng hợp chi phí tuần với đề xuất tối ưu hóa (tắt EC2, dọn dẹp EBS)\n- Tổng kết kiến thức Tuần 6 17/10/2025 17/10/2025 AWS Journey 3. Chi tiết triển khai kỹ thuật 3.1 Chính sách IAM Nâng cao Phân tích cấu trúc JSON: Version, Statement, Effect, Action, Resource. Tạo Chính sách dựa trên điều kiện (Condition) để hạn chế truy cập theo IP nguồn hoặc tags: { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:*\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::my-secure-bucket/*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;IpAddress\u0026#34;: {\u0026#34;aws:SourceIp\u0026#34;: \u0026#34;203.0.113.0/24\u0026#34;} } } Gắn các chính sách trực tiếp (inline) vào các IAM Group cụ thể để thực thi phân chia nhiệm vụ. 3.2 Mã hóa dữ liệu với AWS KMS Tạo khóa Customer Managed Key (CMK) (Đối xứng). Cấu hình Key Policy để xác định Quản trị viên khóa và Người dùng khóa. Bật mã hóa mặc định trên một S3 bucket sử dụng CMK mới tạo. Thử nghiệm mã hóa thủ công qua CLI: aws kms encrypt --key-id \u0026lt;key-id\u0026gt; --plaintext fileb://data.txt --output text --query CiphertextBlob 3.3 Tích hợp AWS Secrets Manager Lưu trữ thông tin đăng nhập RDS (username/password) an toàn trong Secrets Manager. Cấu hình cài đặt tự động xoay vòng (automatic rotation) (tìm hiểu khái niệm). Phát triển script Python (Boto3) cho Lambda để lấy secret bằng lập trình, tránh việc hardcode thông tin xác thực trong mã nguồn. 3.4 Quản lý \u0026amp; Tối ưu hóa chi phí Cost Explorer: Kích hoạt các thẻ (tags) (ví dụ: Project: WebApp) để lọc chi phí theo từng khối lượng công việc cụ thể. AWS Budgets: Thiết lập ngân sách hàng tháng là $10.00 với cảnh báo kích hoạt khi sử dụng đạt 80% ($8.00). Anomaly Detection: Bật phát hiện bất thường chi phí AWS để nhận diện các đợt tăng đột biến trong việc sử dụng dịch vụ (ví dụ: vòng lặp Lambda không mong muốn). 4. Thành tựu Đến cuối Tuần 6, các kết quả sau đã đạt được:\n✔ Thành công về mặt chức năng Thành thạo việc tạo và áp dụng các IAM Policy chi tiết để kiểm soát chặt chẽ quyền truy cập tài nguyên. Triển khai thành công mã hóa dữ liệu khi nghỉ (encryption at rest) cho S3 và EBS bằng KMS. Thay thế thông tin đăng nhập database hardcode bằng việc truy xuất động từ Secrets Manager. Thiết lập khung quản trị chi phí sử dụng Budgets và Alerts. ✔ Phát triển kỹ năng Hiểu sâu hơn về Mô hình trách nhiệm chia sẻ liên quan đến bảo mật. Học cách cân bằng giữa sự nghiêm ngặt về bảo mật và khả năng vận hành dễ dàng. Có nhận thức về \u0026ldquo;FinOps\u0026rdquo;: cách phân tích chi phí, đề xuất các biện pháp tối ưu hóa (ví dụ: dừng EC2 nhàn rỗi, xóa EBS không gắn kết) và duy trì hiệu quả. 5. Thách thức gặp phải \u0026amp; Giải pháp Thách thức 1: Bị từ chối truy cập do KMS Key Policy\nVấn đề: Một IAM user có quyền Admin nhưng không thể giải mã file. Giải pháp: Hiểu rằng KMS Key Policies tách biệt với IAM policies. Đã thêm ARN của user vào phần \u0026ldquo;Key Users\u0026rdquo; trong chính sách của KMS. Thách thức 2: Chi phí/Độ trễ của Secrets Manager\nVấn đề: Việc gọi Secrets Manager quá thường xuyên làm tăng chi phí và độ trễ. Giải pháp: Triển khai cơ chế caching trong mã nguồn Lambda để lưu secret tạm thời trong ngữ cảnh thực thi (execution context). Thách thức 3: Đọc hiểu dữ liệu Cost Explorer\nVấn đề: Chi phí cho mục \u0026ldquo;EC2-Other\u0026rdquo; cao và không rõ ràng. Giải pháp: Phân tích sâu hơn theo \u0026ldquo;Usage Type\u0026rdquo; (Loại sử dụng) để xác định chi phí đến từ truyền tải dữ liệu qua NAT Gateway, dẫn đến việc xem xét lại kiến trúc. "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/vi/1-worklog/1.7-week7/",
	"title": "Worklog Tuần 7",
	"tags": [],
	"description": "",
	"content": "📘 Báo cáo công việc Tuần 7 – AWS Journey 1. Mục tiêu hàng tuần Trong Tuần 7, trọng tâm là xây dựng Tính sẵn sàng cao (High Availability - HA) và Khả năng mở rộng (Scalability) cho kiến trúc. Các mục tiêu chính bao gồm:\nTự động mở rộng \u0026amp; Cân bằng tải – Cấu hình hệ thống để xử lý tải lưu lượng thay đổi tự động bằng ASG và ALB. Kiến trúc Decoupling (Tách rời) – Sử dụng SQS và SNS để cho phép giao tiếp không đồng bộ giữa các vi dịch vụ. Giám sát mạng – Tăng cường khả năng quan sát bằng cách ghi lại và phân tích lưu lượng mạng với VPC Flow Logs. Tuần này biến đổi cơ sở hạ tầng tĩnh thành một hệ thống động, bền vững, có khả năng tự phục hồi và mở rộng theo nhu cầu.\n2. Tóm tắt công việc chi tiết 🗂 Bảng hoạt động Ngày Nhiệm vụ Ngày bắt đầu Ngày hoàn thành Tham khảo Thứ Hai - Tìm hiểu về các khái niệm High Availability, Fault Tolerance và Elasticity\n- Giới thiệu về Auto Scaling Group (ASG) và Elastic Load Balancer (ELB) 20/10/2025 20/10/2025 AWS Journey Thứ Ba - Thực hành tạo Auto Scaling Group cho EC2 instance\n- Thiết lập launch template, scaling policy và theo dõi mục tiêu (target tracking) 21/10/2025 21/10/2025 AWS Journey Thứ Tư - Tạo và cấu hình Application Load Balancer (ALB)\n- Kết nối ALB với ASG để phân phối tải\n- Kiểm tra truy cập website qua ALB DNS 22/10/2025 22/10/2025 AWS Journey Thứ Năm - Làm quen với dịch vụ Amazon SQS và SNS\n- Tạo SQS queue, SNS topic và subscription\n- Gửi và nhận thông báo giữa các thành phần 23/10/2025 23/10/2025 AWS Journey Thứ Sáu - Bật VPC Flow Logs để giám sát lưu lượng mạng\n- Phân tích nhật ký trong CloudWatch Logs\n- Tổng hợp kiến thức về độ tin cậy \u0026amp; mở rộng 24/10/2025 24/10/2025 AWS Journey 3. Chi tiết triển khai kỹ thuật 3.1 Auto Scaling Group (ASG) Launch Template: Tạo mẫu định nghĩa AMI, Loại instance (t2.micro) và Security Groups. Chính sách mở rộng (Scaling Policies): Triển khai Target Tracking Scaling Policy để duy trì mức sử dụng CPU trung bình ở 50%. Dung lượng: Cấu hình Min: 2, Max: 4, Desired: 2 để đảm bảo tính sẵn sàng cao trên nhiều Availability Zones. 3.2 Application Load Balancer (ALB) Target Group: Tạo nhóm đích cho lưu lượng HTTP (Cổng 80). Quy tắc Listener: Cấu hình ALB để lắng nghe HTTP và chuyển tiếp lưu lượng đến Target Group của ASG. Kiểm tra sức khỏe (Health Checks): Cấu hình đường dẫn /index.html để đảm bảo chỉ các instance khỏe mạnh mới nhận được lưu lượng. DNS: Xác thực quyền truy cập bằng tên DNS tự tạo của ALB (my-loadbalancer-123.region.elb.amazonaws.com). 3.3 Tách rời với SQS \u0026amp; SNS SNS (Simple Notification Service): Tạo Topic (OrderAlerts) và đăng ký địa chỉ email để nhận thông báo. SQS (Simple Queue Service): Tạo Hàng đợi tiêu chuẩn (OrderQueue). Mô hình Fan-out: Đăng ký SQS queue vào SNS topic. Xuất bản một tin nhắn lên SNS và xác minh nó xuất hiện trong cả hộp thư Email và hàng đợi SQS. 3.4 VPC Flow Logs \u0026amp; Giám sát Thiết lập: Bật Flow Logs cho VPC, gửi dữ liệu đến CloudWatch Logs. Phân tích: Sử dụng CloudWatch Log Insights để truy vấn lưu lượng: Xác định lưu lượng bị từ chối (REJECT) do chặn Security Group. Theo dõi các nỗ lực kết nối SSH. Phân tích luồng lưu lượng nội bộ giữa các subnets. 4. Thành tựu Đến cuối Tuần 7, các kết quả sau đã đạt được:\n✔ Thành công về mặt chức năng Hiểu rõ mô hình Tính sẵn sàng cao và cách duy trì thời gian hoạt động của hệ thống khi gặp sự cố. Triển khai thành công Auto Scaling Group + Load Balancer để tự động mở rộng dung lượng EC2. Cấu hình SQS/SNS để giao tiếp hàng đợi và thông báo tin cậy. Bật và đọc hiểu VPC Flow Logs, phân tích lưu lượng mạng trong CloudWatch. ✔ Phát triển kỹ năng Nắm vững mối quan hệ giữa Load Balancers và Auto Scaling Groups. Học cách giả lập tải (sử dụng công cụ stress) để kích hoạt sự kiện mở rộng (scaling events). Có kinh nghiệm trong thiết kế kiến trúc \u0026ldquo;Loose Coupling\u0026rdquo; (Kết nối lỏng lẻo). Cải thiện kỹ năng khắc phục sự cố mạng thông qua phân tích nhật ký (log). 5. Thách thức gặp phải \u0026amp; Giải pháp Thách thức 1: Unhealthy Targets trong ALB\nVấn đề: Các instance EC2 trong Target Group hiển thị trạng thái \u0026ldquo;Unhealthy\u0026rdquo;. Giải pháp: Security Group trên EC2 không cho phép lưu lượng từ Security Group của Load Balancer. Đã cập nhật quy tắc Inbound để cho phép HTTP từ SG của ALB. Thách thức 2: ASG không thu hẹp (Scale Down)\nVấn đề: Sau khi test tải, các instances vẫn chạy lâu hơn dự kiến. Giải pháp: Hiểu về khái niệm Cooldown Period (Thời gian hạ nhiệt). ASG đang chờ hết thời gian cooldown mặc định (300 giây) trước khi chấm dứt instances để tránh hiện tượng \u0026ldquo;dao động\u0026rdquo; (thrashing). Thách thức 3: Khả năng hiển thị tin nhắn SQS\nVấn đề: Tin nhắn đã được xử lý nhưng lại xuất hiện lại trong hàng đợi. Giải pháp: Điều chỉnh Visibility Timeout để khớp với thời gian xử lý của ứng dụng tiêu thụ (consumer). "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/vi/1-worklog/1.8-week8/",
	"title": "Worklog Tuần 8",
	"tags": [],
	"description": "",
	"content": "📘 Báo cáo công việc Tuần 8 – AWS Journey 1. Mục tiêu hàng tuần Tuần 8 đóng vai trò là giai đoạn tổng kết và củng cố toàn diện. Mục tiêu chính là hệ thống hóa tất cả kiến thức đã học trong 7 tuần qua dưới góc nhìn của AWS Well-Architected Framework (Khung kiến trúc tốt). Các mục tiêu chính bao gồm:\nWell-Architected Framework: Hiểu sâu về 5 trụ cột (Vận hành xuất sắc, Bảo mật, Tin cậy, Hiệu quả hiệu suất, Tối ưu hóa chi phí). Củng cố kiến trúc: Ôn tập các phương pháp hay nhất về Bảo mật (IAM, KMS), Khả năng phục hồi (Multi-AZ, DR) và Tối ưu hóa. Thiết kế tổng thể: Thiết kế một cơ sở hạ tầng hoàn chỉnh tích hợp các dịch vụ cốt lõi (EC2, S3, RDS, VPC, Lambda, CloudFront) và đánh giá theo tiêu chuẩn AWS. Tuần này chuyển tiếp từ việc học các dịch vụ riêng lẻ sang thiết kế các hệ thống mạnh mẽ, sẵn sàng cho môi trường thực tế (production).\n2. Tóm tắt công việc chi tiết 🗂 Bảng hoạt động Ngày Nhiệm vụ Ngày bắt đầu Ngày hoàn thành Tham khảo Thứ Hai - Tổng quan về AWS Well-Architected Framework \u0026amp; 5 trụ cột\n- Xác định vai trò và tầm quan trọng của từng trụ cột trong thiết kế hệ thống 27/10/2025 27/10/2025 AWS Journey Thứ Ba - Ôn tập Thiết kế Kiến trúc Bảo mật\n- Chuyên sâu: IAM, MFA, SCP, KMS, WAF, Shield, GuardDuty, Security Groups so với NACLs 28/10/2025 28/10/2025 AWS Journey Thứ Tư - Ôn tập Thiết kế Kiến trúc Bền vững (Resilient)\n- Chủ đề: Multi-AZ, Multi-Region, Chiến lược DR, Route 53, Sao lưu \u0026amp; Khôi phục 29/10/2025 29/10/2025 AWS Journey Thứ Năm - Ôn tập Tối ưu hóa Hiệu suất và Chi phí\n- Chủ đề: Auto Scaling, Global Accelerator, Phân cấp S3, Savings Plans 30/10/2025 30/10/2025 AWS Journey Thứ Sáu - Thực hành tổng hợp: Xây dựng kiến trúc mẫu full-stack\n- Đánh giá theo 5 tiêu chí của Well-Architected Framework\n- Viết báo cáo tổng kết tuần 31/10/2025 31/10/2025 AWS Journey 3. Chi tiết triển khai kỹ thuật 3.1 Đánh giá Kiến trúc Bảo mật (Trụ cột Security) Phòng thủ theo chiều sâu (Defense in Depth): Thiết kế mô hình bảo mật đa lớp: Biên (Edge): AWS WAF \u0026amp; Shield (chống DDoS). VPC: Subnet Public/Private, NACLs (Stateless), Security Groups (Stateful). Danh tính: IAM Users với MFA, Roles cho dịch vụ, Nguyên tắc đặc quyền tối thiểu. Dữ liệu: KMS để mã hóa khi nghỉ (EBS/S3/RDS), TLS/ACM để mã hóa đường truyền. 3.2 Độ tin cậy \u0026amp; Khả năng phục hồi (Trụ cột Reliability) Tính sẵn sàng cao (HA): Thiết kế triển khai Multi-AZ cho EC2 (qua ASG) và RDS (Primary/Standby). Khôi phục thảm họa (DR): Rà soát 4 chiến lược DR: Backup \u0026amp; Restore (Rẻ nhất, RTO cao nhất). Pilot Light. Warm Standby. Multi-Site Active/Active (Đắt nhất, RTO thấp nhất). 3.3 Hiệu suất \u0026amp; Chi phí (Trụ cột Efficiency \u0026amp; Optimization) Hiệu suất: Triển khai CloudFront để lưu đệm tại biên (edge caching) và Global Accelerator để tối ưu hóa định tuyến giảm độ trễ. Chi phí: Phân tích S3 Lifecycle Policies (Standard -\u0026gt; IA -\u0026gt; Glacier) để tự động tiết kiệm lưu trữ. Đánh giá Compute Savings Plans so với Reserved Instances cho khối lượng công việc dài hạn. Sử dụng AWS Cost Explorer để xác định tài nguyên \u0026ldquo;zombie\u0026rdquo; (EIP không gắn kết, ELB nhàn rỗi). 3.4 Thực hành Kiến trúc Capstone Thiết kế Ứng dụng Web 3 tầng (3-Tier Web App): Tầng trình bày: CloudFront + S3 (tài sản tĩnh) / ALB (yêu cầu động). Tầng logic: EC2 Auto Scaling Group trong Private Subnets. Tầng dữ liệu: RDS Multi-AZ + DynamoDB. Thực hiện tự đánh giá bằng công cụ AWS Well-Architected Tool để xác định các rủi ro (High/Medium Risk Issues). 4. Thành tựu Đến cuối Tuần 8, các kết quả sau đã đạt được:\n✔ Làm chủ khái niệm Hiểu sâu và hệ thống hóa kiến thức về AWS Well-Architected Framework. Có khả năng phân tích sự đánh đổi (trade-offs) giữa Chi phí, Hiệu suất và Độ tin cậy. ✔ Củng cố kỹ thuật Củng cố 4 nhóm kiến trúc cốt lõi: Bảo mật, Bền vững, Hiệu suất và Tối ưu hóa chi phí. Nắm vững sự tương tác giữa các dịch vụ cốt lõi (ví dụ: cách CloudWatch kích hoạt Lambda để khắc phục sự cố). ✔ Ứng dụng thực tế Thực hành thiết kế một cơ sở hạ tầng hoàn chỉnh theo tiêu chuẩn công nghiệp từ con số 0. Học cách thực hiện tự đánh giá và rà soát kiến trúc. 5. Thách thức gặp phải \u0026amp; Giải pháp Thách thức 1: Phân tích sự đánh đổi (Trade-off)\nVấn đề: Khó khăn khi chọn giữa \u0026ldquo;Hiệu suất tối đa\u0026rdquo; và \u0026ldquo;Chi phí thấp nhất\u0026rdquo; (ví dụ: DynamoDB Provisioned vs. On-Demand). Giải pháp: Sử dụng Well-Architected Framework để ưu tiên yêu cầu nghiệp vụ (ví dụ: nếu lưu lượng không thể dự đoán, On-Demand tốt hơn dù chi phí đơn vị có thể cao hơn). Thách thức 2: Sự phức tạp của chiến lược DR\nVấn đề: Nhầm lẫn sự khác biệt nhỏ giữa \u0026ldquo;Pilot Light\u0026rdquo; và \u0026ldquo;Warm Standby\u0026rdquo;. Giải pháp: Tạo bảng so sánh tập trung vào mục tiêu RTO/RPO và số lượng tài nguyên hoạt động để phân biệt rõ ràng. Thách thức 3: Xung đột Security Group vs. NACL\nVấn đề: Khắc phục sự cố kết nối khi NACL chặn lưu lượng trả về (cổng ephemeral). Giải pháp: Củng cố hiểu biết rằng NACL là phi trạng thái (stateless) và yêu cầu quy tắc cho phép rõ ràng cho cả chiều vào (inbound) và chiều ra (outbound). "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/vi/1-worklog/1.9-week9/",
	"title": "Worklog Tuần 9",
	"tags": [],
	"description": "",
	"content": "📘 Báo cáo công việc Tuần 9 – AWS Journey 1. Mục tiêu hàng tuần Trong Tuần 9, trọng tâm mở rộng sang lĩnh vực Dữ liệu \u0026amp; Phân tích (Data \u0026amp; Analytics). Mục tiêu chính là hiểu quy trình dữ liệu đầu cuối (end-to-end) trên AWS, từ thu thập đến trực quan hóa. Các mục tiêu chính bao gồm:\nKiến trúc Data Lake – Xây dựng kho lưu trữ có khả năng mở rộng bằng Amazon S3. ETL \u0026amp; Danh mục dữ liệu – Sử dụng AWS Glue để khám phá và lập danh mục siêu dữ liệu (metadata). Truy vấn không máy chủ (Serverless Querying) – Phân tích dữ liệu trực tiếp trong S3 bằng Amazon Athena (SQL). Trí tuệ doanh nghiệp (BI) – Trực quan hóa thông tin chi tiết bằng Amazon QuickSight. Tuần này trình bày cách biến dữ liệu thô thành thông tin kinh doanh hữu ích bằng các công cụ phân tích serverless.\n2. Tóm tắt công việc chi tiết 🗂 Bảng hoạt động Ngày Nhiệm vụ Ngày bắt đầu Ngày hoàn thành Tham khảo Thứ Hai - Giới thiệu hệ sinh thái Data \u0026amp; Analytics trên AWS\n- Hiểu khái niệm Data Lake, quy trình ETL và cách kết nối nguồn dữ liệu 03/11/2025 03/11/2025 AWS Journey Thứ Ba - Tạo Data Lake trên Amazon S3\n- Cấu trúc thư mục, quyền truy cập\n- Thiết lập AWS Glue Crawler để xác định schema dữ liệu 04/11/2025 04/11/2025 AWS Journey Thứ Tư - Thực hành AWS Athena để truy vấn dữ liệu trong Data Lake\n- Viết câu lệnh SQL cơ bản và xuất kết quả ra S3 05/11/2025 05/11/2025 AWS Journey Thứ Năm - Giới thiệu và thực hành với Amazon QuickSight\n- Kết nối QuickSight với Athena\n- Tạo dashboard đơn giản với biểu đồ và bảng tổng hợp 06/11/2025 06/11/2025 AWS Journey Thứ Sáu - Ôn tập \u0026amp; củng cố kiến thức tuần (Thu thập → xử lý → phân tích)\n- So sánh Glue/Athena với công cụ truyền thống\n- Viết báo cáo tổng kết thực hành 07/11/2025 07/11/2025 AWS Journey 3. Chi tiết triển khai kỹ thuật 3.1 Thiết lập Data Lake (Amazon S3) Chiến lược lưu trữ: Tạo S3 bucket với cấu trúc thư mục logic (ví dụ: raw-data/, processed-data/). Nhập dữ liệu: Tải lên các tập dữ liệu mẫu (file CSV/JSON nhật ký bán hàng) vào thư mục raw-data. Bảo mật: Áp dụng \u0026ldquo;Block Public Access\u0026rdquo; và đảm bảo IAM roles sẵn sàng cho Glue và Athena truy cập. 3.2 Khám phá Metadata (AWS Glue) Glue Crawler: Cấu hình Crawler để quét S3 bucket. IAM Role: Tạo service role cấp quyền cho Glue đọc S3 và ghi vào Glue Data Catalog. Data Catalog: Chạy thành công crawler, tự động phát hiện schema (cột, kiểu dữ liệu) và tạo định nghĩa bảng trong Glue Database. 3.3 Truy vấn không máy chủ (Amazon Athena) Cấu hình: Thiết lập vị trí lưu kết quả truy vấn trong S3 (s3://my-bucket/athena-results/). Thao tác SQL: Thực thi các truy vấn SQL chuẩn trên bảng Glue: SELECT product_category, SUM(amount) as total_sales FROM sales_data GROUP BY product_category; Xác minh: Xác nhận rằng Athena có thể truy vấn dữ liệu CSV trực tiếp mà không cần nạp vào máy chủ cơ sở dữ liệu. 3.4 Trực quan hóa dữ liệu (Amazon QuickSight) Tạo Dataset: Chọn Athena làm nguồn dữ liệu và nhập bảng đã tạo ở các bước trước. SPICE: Nhập dữ liệu vào bộ nhớ đệm SPICE để hiển thị nhanh hơn. Dashboarding: Xây dựng bảng điều khiển chứa: Biểu đồ cột: Doanh số theo khu vực. Biểu đồ tròn: Nhân khẩu học khách hàng. KPI: Tổng doanh thu. 4. Thành tựu Đến cuối Tuần 9, các kết quả sau đã đạt được:\n✔ Thành công về mặt chức năng Xây dựng thành công Data Lake Serverless sử dụng S3. Tự động hóa việc khám phá cấu trúc dữ liệu bằng AWS Glue Crawlers. Thực hiện phân tích SQL tùy ý bằng Athena mà không cần cấp phát máy chủ. Tạo BI Dashboard trực quan trong QuickSight để trình bày thông tin chi tiết. ✔ Phát triển kỹ năng Hiểu sự tách biệt giữa Tính toán (Athena) và Lưu trữ (S3). Có kinh nghiệm với khái niệm Schema-on-Read so với Schema-on-Write truyền thống. Học cách quản lý quyền IAM giữa các dịch vụ Phân tích (QuickSight \u0026lt;-\u0026gt; Athena \u0026lt;-\u0026gt; S3). 5. Thách thức gặp phải \u0026amp; Giải pháp Thách thức 1: Lỗi vị trí đầu ra Athena\nVấn đề: Truy vấn thất bại với lỗi \u0026ldquo;No output location provided\u0026rdquo;. Giải pháp: Cấu hình \u0026ldquo;Query Result Location\u0026rdquo; trong cài đặt Athena trỏ đến một thư mục S3 hợp lệ. Thách thức 2: Quyền truy cập QuickSight\nVấn đề: QuickSight không thể truy cập dữ liệu trong S3 bucket. Giải pháp: Truy cập \u0026ldquo;Manage QuickSight\u0026rdquo; \u0026gt; \u0026ldquo;Security \u0026amp; Permissions\u0026rdquo; và tích chọn thủ công vào S3 bucket chứa dữ liệu. Thách thức 3: Phân loại sai của Glue Crawler\nVấn đề: Crawler phân loại dữ liệu CSV không chính xác do vấn đề về tiêu đề (header). Giải pháp: Chỉnh sửa trình phân loại tùy chỉnh (custom classifier) trong Glue để nhận diện đúng hàng tiêu đề CSV. "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/vi/2-proposal/",
	"title": "Bản đề xuất",
	"tags": [],
	"description": "",
	"content": "Nền tảng Smart Contract Assistant - AGREEME Giải pháp AWS Serverless cho việc rà soát hợp đồng cá nhân TEEJ - AGREEME 1. Tóm tắt (Executive Summary) Nền tảng Smart Contract Assistant - AGREEME là một dịch vụ web dành cho cá nhân và các nhóm người dùng nhỏ (freelancer, chủ doanh nghiệp nhỏ, nhân viên văn phòng) làm việc với hợp đồng hằng ngày nhưng không có chuyên môn pháp lý sâu. Giải pháp sử dụng Amazon Bedrock và kiến trúc AWS serverless hoàn toàn để phân tích hợp đồng, làm nổi bật rủi ro, gợi ý chỉnh sửa điều khoản, và tạo tóm tắt cũng như mẫu hợp đồng mới.\nĐược xây dựng trên AWS Amplify, Lambda, API Gateway, DynamoDB, S3, Cognito, EventBridge và CloudWatch, nền tảng cung cấp khả năng rà soát hợp đồng bằng AI với độ trễ thấp, chi phí thấp và bảo mật cao, được tối ưu cho người dùng đơn lẻ hoặc các nhóm nhỏ mà không cần tính năng phức tạp như hệ thống doanh nghiệp.\n2. Vấn đề và giải pháp (Problem Statement) Vấn đề Hợp đồng thường dài, phức tạp và khó hiểu đối với người không có chuyên môn về luật pháp. Việc thuê tư vấn pháp lý cho mỗi hợp đồng tốn kém và không phù hợp để mở rộng cho các cá nhân. Hiện tại vẫn chưa có công cụ tự phục vụ, đơn giản, tập trung vào việc rà soát hợp đồng nhanh và chính xác cho mục đích cá nhân hoặc các doanh nghiệp nhỏ. Người dùng cá nhân hoặc các doanh nghiệp nhỏ không có nhu cầu sử dụng các hệ thống phức tạp hay nền tảng quản lý tài liệu nặng nề; họ chỉ cần kiểm tra rủi ro nhanh với các chỉ dẫn rõ ràng. Giải pháp Nền tảng AGREEME cung cấp một web app sử dụng mô hình AI, nơi người dùng có thể upload file hợp đồng (PDF/DOCX) và nhận được:\nGiải thích về các điều khoản phức tạp bằng ngôn ngữ đơn giản, dễ hiểu. Ngữ cảnh pháp lý với việc đánh dấu các điều khoản có lợi/không có lợi. Phát hiện rủi ro và cảnh báo (điều khoản mất cân bằng, nghĩa vụ ẩn, vấn đề pháp lý tiềm ẩn). Gợi ý chỉnh sửa và câu chữ thay thế ở cấp độ điều khoản để phục vụ đàm phán. Bản tóm tắt điều hành (executive summary) tự động cho người dùng bận rộn. Khả năng tạo hợp đồng đơn giản từ mẫu (thuê nhà, mua bán, dịch vụ, v.v.) với các điều chỉnh dựa trên tình huống thực tế được AI hỗ trợ. Tất cả được vận hành trên kiến trúc AWS Serverless:\nFrontend trên AWS Amplify với Hosting, CDN và WAF tích hợp. APIs \u0026amp; compute thông qua Amazon API Gateway và AWS Lambda. AI sử dụng Amazon Bedrock (GenAI/LLM + embeddings/RAG). Lưu trữ \u0026amp; metadata bằng Amazon S3 và DynamoDB, được mã hóa bởi KMS. Định danh \u0026amp; bảo mật với Amazon Cognito, AWS WAF, IAM và KMS. Giám sát \u0026amp; sự kiện thông qua Amazon CloudWatch và EventBridge. Lợi ích và hoàn vốn đầu tư (Benefits and Return on Investment) Tác động kinh doanh\nGiảm thời gian đọc/hiểu hợp đồng ít nhất ≥ 70%. Giảm chi phí tư vấn pháp lý ít nhất ≥ 50% bằng cách thay thế bước rà soát ban đầu bằng AI. Tăng sự tự tin của người dùng khi ký kết và đàm phán hợp đồng. Hiệu năng kỹ thuật\nĐộ chính xác phân tích hợp đồng ≥ 85% (theo kiểm thử nội bộ và phản hồi người dùng). Thời gian phản hồi ≤ 8 giây sau khi upload. Tỷ lệ uptime hệ thống ≥ 99.9% cho người dùng cá nhân. Hiệu quả chi phí\nChi phí hạ tầng AWS ước tính: $27.91/tháng → $334.92/12 tháng. Nỗ lực triển khai: tổng 592 giờ, ≈ $637.12 cho chi phí nhân sự (Solution Architect + Software Engineer + AI Engineer). 3. Kiến trúc giải pháp (Solution Architecture) Nền tảng được triển khai dưới dạng kiến trúc serverless hoàn toàn, bảo mật và có khả năng mở rộng, được tối ưu cho xử lý tài liệu bằng GenAI và phân tích hợp đồng dựa trên RAG. Kiến trúc tổng thể (High-Level Architecture) Lớp Entry \u0026amp; Web\nAmazon Route 53 cho DNS và tên miền thân thiện. AWS Amplify Hosting cho frontend React/Amplify với CDN và WAF tích hợp. Định danh \u0026amp; Truy cập (Identity \u0026amp; Access)\nAmazon Cognito cho user pool và xác thực (JWT). API Gateway xác thực token Cognito trước khi gọi Lambda. IAM roles đảm bảo nguyên tắc phân quyền tối thiểu cho S3, DynamoDB, Bedrock, KMS và EventBridge. Tầng Backend Compute (Lambda Microservices)\nCore API Lambda để điều phối các yêu cầu từ API Gateway. Các Lambda chuyên biệt cho: Tạo hợp đồng (ContractGen). Gọi LLM tổng quát (tóm tắt, phân loại, chuyển đổi). Tìm kiếm RAG (truy xuất dựa trên embedding và tra cứu tri thức). Cập nhật metadata (DynamoDB). Quản lý template. Tầng AI \u0026amp; LLM\nAmazon Bedrock dùng cho: Phân tích hợp đồng (tóm tắt, rủi ro, phân loại điều khoản). Embeddings và RAG trên kho văn bản pháp luật và các template. Tạo hợp đồng và gợi ý viết lại điều khoản. Dữ liệu \u0026amp; Lưu trữ (Data \u0026amp; Storage)\nAmazon S3 lưu hợp đồng người dùng upload, tài liệu sinh ra và các mẫu hợp đồng, văn bản pháp luật thô. Amazon DynamoDB lưu metadata, template, và chỉ mục RAG. AWS KMS mã hóa dữ liệu lưu trữ cho S3, DynamoDB và secrets. Sự kiện \u0026amp; Tự động hóa (Events \u0026amp; Automation)\nAmazon EventBridge cho các workflow bất đồng bộ (xử lý nền, cập nhật metadata, đồng bộ template). Giám sát \u0026amp; Vận hành (Monitoring \u0026amp; Operations)\nAmazon CloudWatch cho logs, metrics và cảnh báo trên Lambda, API Gateway, Amplify và tương tác với Bedrock. Dịch vụ AWS sử dụng (AWS Services Used) Application Stack\nAWS Amplify (hosting frontend, CI/CD, tích hợp WAF). React / Amplify Framework (UI). Amazon API Gateway (quản lý API). AWS Lambda (backend microservices). Amazon Bedrock (suy luận LLM, embeddings, RAG). Amazon DynamoDB (metadata và kho tri thức). Amazon S3 (lưu trữ tài liệu). AWS KMS (mã hóa). Amazon EventBridge (định tuyến sự kiện). Monitoring \u0026amp; DevOps\nAmazon CloudWatch (quan sát hệ thống, cảnh báo). GitLab + Amplify CI/CD (quản lý mã nguồn và triển khai tự động). Security\nAWS WAF (thông qua Amplify). AWS IAM (kiểm soát truy cập). Amazon Cognito (xác thực). Thiết kế thành phần (Component Design) Web Interface: Ứng dụng React host trên Amplify để upload, xem kết quả phân tích, lịch sử và tạo hợp đồng. API Layer: API Gateway + Lambda để xử lý upload, điều phối Bedrock và trả kết quả. AI Logic: Luồng xử lý sử dụng Bedrock để phân tích điều khoản, chấm điểm rủi ro, tóm tắt và tạo hợp đồng dựa trên template. Data Layer: S3 lưu hợp đồng thô/đã sinh; DynamoDB lưu metadata, template và chỉ mục RAG. Security \u0026amp; Compliance: Xác thực dựa trên Cognito, mã hóa KMS, bảo vệ WAF và chính sách IAM chặt chẽ. 4. Triển khai kỹ thuật (Technical Implementation) Các giai đoạn triển khai (Implementation Phases) Phase 1 – Assessment (Tuần 1–2)\nThu thập yêu cầu kinh doanh và yêu cầu người dùng. Xác định các use case AI (phân tích, phát hiện rủi ro, gợi ý). Thiết kế kiến trúc tổng thể và baseline bảo mật. Phase 2 – Setup Base Infrastructure (Tuần 3–4)\nCấu hình dự án Amplify, IAM roles, S3, DynamoDB, KMS. Bật logging và monitoring bằng CloudWatch. Phase 3 – Frontend \u0026amp; Authentication (Tuần 5–6)\nTriển khai frontend qua Amplify. Tích hợp Cognito và WAF do Amplify quản lý. Xây dựng chức năng upload hợp đồng và giao diện dashboard cơ bản. Phase 4 – Backend Core \u0026amp; AI Integration (Tuần 7–8)\nXây dựng Lambda APIs và tích hợp Bedrock. Parse hợp đồng, lưu kết quả vào DynamoDB và trả lại phân tích cho UI. Phase 5 – Advanced AI Logic \u0026amp; Optimization (Tuần 9–10)\nXây dựng logic phát hiện rủi ro, gợi ý đàm phán và tìm kiếm RAG. Cải thiện UX, tối ưu độ trễ và chi phí. Phase 6 – Testing, Go-Live \u0026amp; Handover (Tuần 11–12)\nViết unit/integration test, kiểm thử bảo mật và hiệu năng. Triển khai production và chuyển giao kiến thức. Yêu cầu kỹ thuật (Technical Requirements) Thành thạo AWS Amplify, Lambda, API Gateway, Cognito, S3, DynamoDB, EventBridge, CloudWatch. Có quyền truy cập Amazon Bedrock (Claude, Cohere, v.v.) để phân tích văn bản pháp lý. Thư viện xử lý file để trích xuất nội dung PDF/DOCX. Chính sách xử lý dữ liệu và quyền riêng tư rõ ràng cho tài liệu hợp đồng nhạy cảm. 5. Timeline \u0026amp; Milestones Tổng thời gian: 12 tuần (6 sprint, mỗi sprint 2 tuần).\nCác sprint:\nSprint 1–2: Assessment \u0026amp; hạ tầng nền tảng. Sprint 3–4: Frontend + authentication. Sprint 5–6: Backend core, tích hợp AI, logic AI nâng cao và tối ưu. Áp dụng Agile liên tục với planning, review và retrospective mỗi sprint.\nChuyển giao kiến thức ở các sprint cuối (kiến trúc, vận hành, CloudWatch, best practices về prompt).\n6. Ước tính ngân sách (Budget Estimation) Chi phí hạ tầng (theo tháng) – Infrastructure Costs (per month) Infrastructure Costs Service Monthly Cost (USD) 12-Month Cost (USD) Amazon S3 $1.80 $21.60 Amazon API Gateway $0.05 $0.60 Amazon DynamoDB $4.02 $48.24 AWS Secrets Manager $1.08 $12.96 Amazon Route 53 $2.04 $24.48 Amazon Cognito $1.00 $12.00 AWS Amplify $16.25 $195.00 Amazon CloudWatch $0.53 $6.36 Amazon Bedrock $1.13 $13.56 Amazon Lambda $0.01 $0.12 Total $27.91/month $334.92/12 months AWS Pricing Calculator\nImplementation Team Cost Role Hourly Rate (USD) Solution Architect $2.30/hour Software Engineer $0.70/hour AI Engineer $0.70/hour Total estimated project effort: 592 hours → ≈ $637.12 USD.\n7. Đánh giá rủi ro (Risk Assessment) Rủi ro chính (Key Risks) AI accuracy risk: Mô hình có thể diễn giải sai một số điều khoản pháp lý. File quality risk: File scan chất lượng thấp hoặc PDF phức tạp có thể làm hỏng OCR/parsing. Sensitive data risk: Người dùng có thể upload hợp đồng có mức độ bí mật rất cao. Cloud dependency risk: Sự cố trên Bedrock hoặc Amplify sẽ ảnh hưởng tới khả dụng hệ thống. Usage \u0026amp; cost risk: Khối lượng gọi AI lớn sẽ làm tăng chi phí vận hành. Chiến lược giảm thiểu (Mitigation Strategies) Kiểm thử nội bộ kỹ lưỡng và cung cấp cảnh báo rõ ràng về giới hạn của AI. Xây dựng pipeline xử lý file vững chắc với bước kiểm tra và hướng dẫn người dùng. Mã hóa mạnh (KMS), giới hạn thời gian lưu trữ và kiểm soát truy cập chặt chẽ. Giám sát và cảnh báo qua CloudWatch, kèm theo runbook xử lý sự cố. Tối ưu prompt, giới hạn request và thiết lập budget alert để kiểm soát chi phí AI. 8. Kết quả kỳ vọng (Expected Outcomes) Kết quả kỹ thuật (Technical Outcomes) Trợ lý AI dạng serverless, sẵn sàng production cho cá nhân/nhóm nhỏ. Hiệu năng ổn định với thời gian phân tích ≤ 8 giây và uptime ≥ 99.9%. Xử lý tài liệu nhạy cảm một cách an toàn với mã hóa và phân quyền tối thiểu. Kết quả kinh doanh (Business Outcomes) Tăng tốc độ hiểu hợp đồng và giảm rủi ro pháp lý cho người không chuyên. Giảm đáng kể chi phí tư vấn pháp lý và thời gian rà soát thủ công. Nền tảng SaaS có khả năng mở rộng, có thể phát triển thêm tính năng hoặc nhóm người dùng mới trong các giai đoạn Proposal chi tiet tai day [ "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/vi/5-workshop/5.2-prerequiste/",
	"title": "Các bước chuẩn bị",
	"tags": [],
	"description": "",
	"content": "1. Yêu cầu hệ thống và Công cụ Trước khi bắt đầu, hãy đảm bảo máy tính của bạn đã được cài đặt các công cụ sau:\nAWS Account: Một tài khoản AWS đang hoạt động. AWS CLI: Đã cài đặt và cấu hình (Hướng dẫn cài đặt). Node.js \u0026amp; NPM: Để chạy các lệnh deploy backend và frontend. Git: Để tải source code. Text Editor: VS Code (khuyên dùng). 2. Tải Source Code Mở terminal trên máy tính của bạn và clone repository chứa mã nguồn của dự án:\ngit clone https://gitlab.com/manh-25/contract-demo.git cd contract-demo 3. Thiết lập Region ⚠️ Quan trọng: Trong suốt quá trình workshop, hãy đảm bảo bạn luôn chọn Region là Singapore (ap-southeast-1).\nĐiều này rất quan trọng vì các dịch vụ như Amazon Bedrock (mô hình Claude 3) và các cấu hình trong code mẫu đang được thiết lập cho Region này.\n4. Tạo IAM User và Access Keys Để ứng dụng và AWS CLI có thể tương tác với AWS, chúng ta cần tạo một IAM User có quyền quản trị.\nBước 1: Tạo User\nTruy cập IAM Console -\u0026gt; Chọn Users -\u0026gt; Create user. User name: Nhập contract-app-demo. Bấm Next. Bước 2: Cấp quyền (Set permissions)\nChọn Add user to group. Bấm Create group. User group name: Đặt là ADMIN. Trong danh sách Permission policies, tìm kiếm và tích chọn AdministratorAccess. Bấm Create user group. Sau khi group được tạo, tích chọn vào group ADMIN và bấm Next. Bấm Create user. Bước 3: Tạo Access Key\nBấm vào User contract-app-demo vừa tạo. Chuyển sang tab Security credentials. Kéo xuống mục Access keys, bấm Create access key. Chọn Use case: Local code. Tích vào ô xác nhận \u0026ldquo;I understand\u0026hellip;\u0026rdquo;. Bấm Next. Bấm Create access key. Bấm Done ⚠️ Lưu ý: Bấm Download .csv file để lưu key về máy tính. Bạn sẽ không thể xem lại Secret access key sau khi rời khỏi trang này.\n5. Cấu hình AWS CLI Mở Terminal trên máy tính của bạn và chạy lệnh sau để kết nối với tài khoản AWS:\naws configure Nhập các thông tin dựa trên file .csv bạn vừa tải về:\nAWS Access Key ID: (Lấy từ file csv) AWS Secret Access Key: (Lấy từ file csv) Default region name: ap-southeast-1 Default output format: json (hoặc để trống) "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/vi/5-workshop/5.5-fullstack/5.5.2-frontend-amplify/",
	"title": "Deploy Frontend (Amplify)",
	"tags": [],
	"description": "",
	"content": "1. Chuẩn bị GitLab Repository Bạn cần đưa mã nguồn frontend lên GitLab cá nhân để Amplify có thể lấy code về build. Có 2 cách để thực hiện: Cách 1: Tạo Repo mới và Push code (Khuyên dùng)\nĐăng nhập vào GitLab cá nhân và tạo một New project/repository (trống). Mở Terminal trên máy tính, di chuyển vào thư mục frontend: cd frontend Xóa cấu hình git cũ (nếu có): Windows (PowerShell): Remove-Item -Recurse -Force .git Mac/Linux: rm -rf .git Khởi tạo git mới và đẩy code lên: git init git add . git commit -m \u0026#34;Initial deploy\u0026#34; git remote add origin \u0026lt;URL-Repo-GitLab-Cua-Ban\u0026gt; git branch -M main git push -uf origin main Cách 2: Fork từ Repo mẫu\nTruy cập: https://gitlab.com/manh-25/contract-demo Bấm nút Fork để sao chép về tài khoản của bạn. 2. Tạo App trên AWS Amplify Truy cập dịch vụ AWS Amplify trên Console. Kéo xuống dưới cùng trang, chọn Create new app. Tại màn hình \u0026ldquo;Start building with Amplify\u0026rdquo;, chọn GitLab -\u0026gt; Bấm Next. Kết nối tài khoản GitLab và chọn Repository bạn vừa push (nhánh main). 3. Cấu hình Build Settings Đặt tên cho App (App name).\nTrong phần Build settings, bấm nút Edit YML file. Xóa toàn bộ nội dung mặc định và Ghi đè (Paste) đoạn code sau vào (để sử dụng bun giúp tốc độ build nhanh hơn):\nversion: 1 frontend: phases: preBuild: commands: - curl -fsSL https://bun.sh/install | bash - export BUN_INSTALL=\u0026#34;$HOME/.bun\u0026#34; - export PATH=\u0026#34;$BUN_INSTALL/bin:$PATH\u0026#34; - bun install build: commands: - export BUN_INSTALL=\u0026#34;$HOME/.bun\u0026#34; - export PATH=\u0026#34;$BUN_INSTALL/bin:$PATH\u0026#34; - bun run build artifacts: baseDirectory: dist files: - \u0026#34;**/*\u0026#34; cache: paths: - node_modules/**/* 4. Cấu hình Biến môi trường Bấm vào Advanced settings -\u0026gt; Kéo xuống phần Environment variables.\nBấm Add new variable để thêm lần lượt 4 biến sau.\nLấy giá trị (Value) ở đâu? -\u0026gt; Hãy mở một tab trình duyệt khác để lấy thông tin:\nVào dịch vụ Amazon Cognito: Chọn User Pool có tên ai-contract-backend-user-pool-dev. VITE_COGNITO_REGION = ap-southeast-1 VITE_COGNITO_USER_POOL_ID = Copy User pool ID. VITE_COGNITO_CLIENT_ID = Vào tab App integration, kéo xuống dưới cùng copy Client ID. Vào dịch vụ Lambda: Chọn hàm ai-contract-backend-dev-api. VITE_API_URL = Copy API Endpoint (trong phần Configuration -\u0026gt; Triggers hoặc API Gateway). Sau khi điền đủ 4 biến, bấm Next.\n5. Deploy và Kiểm tra Tại màn hình Review, kiểm tra lại thông tin và bấm Save and deploy. Chờ khoảng 3-5 phút để Amplify thực hiện các bước: Provision -\u0026gt; Build -\u0026gt; Deploy. Khi cả 3 bước đều xanh, link truy cập ứng dụng sẽ hiện ra ở phần Domain (ví dụ: https://main.d123...amplifyapp.com). 👉 Bấm vào link để trải nghiệm ứng dụng Smart Contract Assistant của bạn!\n"
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/vi/4-eventparticipated/4.2-event2/",
	"title": "Event 2",
	"tags": [],
	"description": "",
	"content": "Workshop: Data Science on AWS Thời gian: 9:30 – 11:45, Ngày 16/10/2025 Địa điểm: Hall A - Đại học FPT TP.HCM (FPTU HCMC) Vai trò: Người tham dự\nMục tiêu tham dự Tìm hiểu về bức tranh toàn cảnh (Landscape) các dịch vụ AI/ML của AWS. Tiếp cận cách xây dựng, huấn luyện và triển khai mô hình Machine Learning thực tế. Giao lưu với các chuyên gia từ cộng đồng AWS User Group. Danh Sách Diễn Giả Van Hoang Kha - Cloud Solutions Architect, AWS User Group Leader Bach Doang Vuong - Cloud DevOps Engineer, AWS Community Builder Nội dung chính 1. Hệ sinh thái AWS AI/ML Stack Diễn giả đã giới thiệu cấu trúc 3 tầng của AWS AI/ML Stack rất rõ ràng:\nAI Services (Top layer): Các dịch vụ có sẵn, không cần kiến thức sâu về ML vẫn dùng được (Vision, Speech, Chatbots\u0026hellip;). ML Services (Middle layer): Nền tảng Amazon SageMaker giúp xây dựng, huấn luyện và triển khai mô hình. Frameworks \u0026amp; Infrastructure (Bottom layer): Dành cho chuyên gia muốn can thiệp sâu (PyTorch, TensorFlow, EC2 GPU\u0026hellip;). 2. Các AI Services nổi bật \u0026amp; Demo Phần này em thấy rất thú vị vì tính ứng dụng cao, có thể tích hợp ngay vào đồ án của sinh viên:\nXử lý hình ảnh (Vision): Amazon Rekognition giúp nhận diện gương mặt, phân tích video, phát hiện đồ bảo hộ (PPE Detection) và kiểm duyệt nội dung (Content Moderation). Xử lý âm thanh \u0026amp; giọng nói: Combo Amazon Polly (chuyển văn bản thành giọng nói giống người thật) và Amazon Transcribe (chuyển giọng nói thành văn bản, hỗ trợ cả cuộc gọi thời gian thực). Xử lý ngôn ngữ tự nhiên (NLP): Amazon Translate: Dịch thuật đa ngôn ngữ với độ trễ thấp. Amazon Textract: Bóc tách dữ liệu từ văn bản scan/hóa đơn cực nhanh. Amazon Lex: Xây dựng Chatbot hội thoại (Conversational Interfaces) thông minh. Cá nhân hóa trải nghiệm: Amazon Personalize giúp tạo ra các đề xuất sản phẩm (recommendation) theo thời gian thực mà không cần chuyên môn sâu về ML. 3. Quy trình Machine Learning với SageMaker Các diễn giả đã demo quy trình từ Feature Engineering (xử lý dữ liệu thô) đến Training và Tuning tham số. Đặc biệt là demo việc mang code Python (Scikit-learn, TensorFlow) có sẵn để chạy trên hạ tầng mạnh mẽ của AWS.\nTrải nghiệm cá nhân Đây là một buổi workshop rất \u0026ldquo;sát sườn\u0026rdquo; với sinh viên tụi em.\nEm rất ấn tượng phần chia sẻ về Amazon Personalize. Trước giờ em cứ nghĩ làm tính năng \u0026ldquo;Gợi ý sản phẩm\u0026rdquo; (Recommendation System) rất khó và tốn công code thuật toán, nhưng với AWS thì việc này đơn giản hơn rất nhiều. Một điểm cộng lớn là diễn giả chia sẻ rất thực tế về vấn đề Chi phí (Cost). Slide về \u0026ldquo;Monitoring Cost Daily\u0026rdquo; giúp em nhận ra rằng làm Cloud không chỉ là code chạy được, mà còn phải biết cách tối ưu để không bị \u0026ldquo;thủng ví\u0026rdquo; vào cuối tháng. Không khí tại FPTU rất năng lượng, được nghe các anh đi trước chia sẻ kinh nghiệm thực chiến làm em có thêm động lực để học thi chứng chỉ AWS sắp tới. Một số hình ảnh khi tham gia sự kiện Tóm lại: Event này giúp em \u0026ldquo;khai sáng\u0026rdquo; rất nhiều về bộ công cụ AI Services. Thay vì tự build model từ con số 0, em đã biết cách tận dụng các API có sẵn của AWS để giải quyết bài toán nhanh hơn và xịn hơn.\n"
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/vi/5-workshop/5.3-infrastructure/5.3.2-dynamodb/",
	"title": "Tạo bảng DynamoDB",
	"tags": [],
	"description": "",
	"content": "Chúng ta cần tạo 3 bảng (Tables) trong Amazon DynamoDB để lưu trữ thông tin người dùng, các phiên chat và nội dung tin nhắn.\nTruy cập dịch vụ DynamoDB -\u0026gt; Chọn Tables ở menu trái -\u0026gt; Bấm Create table.\n1. Tạo bảng Users Bảng này dùng để lưu thông tin người dùng đăng nhập.\nTable name: Nhập Users Partition key: Nhập user_id (Kiểu dữ liệu: String) Table settings: Chọn Customize settings. Table class: Chọn DynamoDB Standard. Read/write capacity settings: Chọn On-demand. Bấm Create table. 2. Tạo bảng ChatSessions Bảng này lưu trữ danh sách các cuộc hội thoại.\nThực hiện tương tự như bảng Users. Table name: Nhập ChatSessions Partition key: Nhập session_id (Kiểu dữ liệu: String) Table settings: Chọn Customize settings -\u0026gt; On-demand. Bấm Create table. 3. Tạo bảng ChatMessages Bảng này lưu chi tiết từng tin nhắn trong cuộc hội thoại.\nTable name: Nhập ChatMessages Partition key: Nhập session_id (Kiểu dữ liệu: String) Sort key: Nhập timestamp (Kiểu dữ liệu: String) Lưu ý: Bảng này bắt buộc phải có Sort key để sắp xếp tin nhắn theo thời gian. Table settings: Chọn Customize settings -\u0026gt; On-demand. Bấm Create table. Kết quả Sau khi hoàn thành, hãy kiểm tra lại danh sách Tables. Bạn cần thấy 3 bảng với trạng thái Active như hình dưới đây: "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/vi/5-workshop/5.4-backed/5.4.2-other-lambdas/",
	"title": "Tạo các Lambda phụ trợ",
	"tags": [],
	"description": "",
	"content": "Ngoài hàm tìm kiếm, chúng ta cần thêm 2 hàm nữa để hoàn thiện tính năng của trợ lý ảo. Thực hiện các bước tạo tương tự như hàm ragsearch, chỉ thay đổi thông số cấu hình.\n1. Hàm tạo hợp đồng (generate_contract) Create Function: Name: generate_contract Runtime: Python 3.12 Code: Copy từ file generate_contract.py trong source code -\u0026gt; Deploy. import json import os import uuid import datetime import logging from typing import Dict, Any, List import boto3 from botocore.exceptions import ClientError logger = logging.getLogger() logger.setLevel(logging.INFO) # ------------------------------------------------------------------- # Config \u0026amp; clients # ------------------------------------------------------------------- AWS_REGION = os.getenv(\u0026#34;AWS_REGION\u0026#34;, \u0026#34;ap-southeast-1\u0026#34;) TEMPLATE_BUCKET = os.getenv(\u0026#34;TEMPLATE_BUCKET\u0026#34;) TEMPLATE_METADATA_KEY = os.getenv(\u0026#34;TEMPLATE_METADATA_KEY\u0026#34;, \u0026#34;index/template_metadata.jsonl\u0026#34;) MODEL_ID = os.getenv(\u0026#34;MODEL_ID\u0026#34;, \u0026#34;anthropic.claude-3-haiku-20240307-v1:0\u0026#34;) # Lambda RAG-search (đã triển khai ở giai đoạn 2.3) RAG_FUNCTION_NAME = os.getenv(\u0026#34;RAG_FUNCTION_NAME\u0026#34;, \u0026#34;ragsearch\u0026#34;) if not TEMPLATE_BUCKET: raise RuntimeError(\u0026#34;TEMPLATE_BUCKET env var is required\u0026#34;) s3 = boto3.client(\u0026#34;s3\u0026#34;, region_name=AWS_REGION) bedrock = boto3.client(\u0026#34;bedrock-runtime\u0026#34;, region_name=AWS_REGION) lambda_client = boto3.client(\u0026#34;lambda\u0026#34;, region_name=AWS_REGION) # ------------------------------------------------------------------- # Global cache template metadata # ------------------------------------------------------------------- TEMPLATE_CACHE = { \u0026#34;loaded\u0026#34;: False, \u0026#34;by_id\u0026#34;: {} # dict[doc_id] = metadata dict } def load_template_metadata_if_needed(): if TEMPLATE_CACHE[\u0026#34;loaded\u0026#34;]: return logger.info(\u0026#34;Loading template metadata from s3://%s/%s ...\u0026#34;, TEMPLATE_BUCKET, TEMPLATE_METADATA_KEY) try: obj = s3.get_object(Bucket=TEMPLATE_BUCKET, Key=TEMPLATE_METADATA_KEY) except ClientError as e: logger.error(\u0026#34;Failed to load template metadata: %s\u0026#34;, e) raise by_id = {} for line in obj[\u0026#34;Body\u0026#34;].iter_lines(): if not line: continue try: rec = json.loads(line.decode(\u0026#34;utf-8\u0026#34;)) except json.JSONDecodeError: logger.warning(\u0026#34;Invalid JSON line in template metadata, skipped\u0026#34;) continue doc_id = rec.get(\u0026#34;doc_id\u0026#34;) if not doc_id: continue by_id[doc_id] = rec TEMPLATE_CACHE[\u0026#34;by_id\u0026#34;] = by_id TEMPLATE_CACHE[\u0026#34;loaded\u0026#34;] = True logger.info(\u0026#34;Loaded %d template metadata records\u0026#34;, len(by_id)) def parse_event_body(event: Dict[str, Any]) -\u0026gt; Dict[str, Any]: if \u0026#34;body\u0026#34; not in event: return event body = event[\u0026#34;body\u0026#34;] if event.get(\u0026#34;isBase64Encoded\u0026#34;): import base64 body = base64.b64decode(body).decode(\u0026#34;utf-8\u0026#34;) try: data = json.loads(body) except json.JSONDecodeError: raise ValueError(\u0026#34;Request body must be valid JSON\u0026#34;) return data def load_template_raw_text(metadata: Dict[str, Any]) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34; MVP: thử đọc nội dung file template từ S3 như text (nhiều file .doc/.docx sẽ không đọc được, nhưng không sao, chỉ dùng tham khảo cấu trúc nếu decode được). Sau này nếu anh muốn xử lý chuẩn DOCX thì ta thêm python-docx. \u0026#34;\u0026#34;\u0026#34; source_raw_path = metadata.get(\u0026#34;source_raw_path\u0026#34;) if not source_raw_path: return \u0026#34;\u0026#34; try: obj = s3.get_object(Bucket=TEMPLATE_BUCKET, Key=source_raw_path) except ClientError as e: logger.warning(\u0026#34;Failed to load template file %s: %s\u0026#34;, source_raw_path, e) return \u0026#34;\u0026#34; try: content_bytes = obj[\u0026#34;Body\u0026#34;].read() text = content_bytes.decode(\u0026#34;utf-8\u0026#34;, errors=\u0026#34;ignore\u0026#34;) return text except Exception: return \u0026#34;\u0026#34; # ------------------------------------------------------------------- # RAG integration # ------------------------------------------------------------------- def build_rag_query(template_metadata: Dict[str, Any], contract_info: Dict[str, Any]) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34; Xây dựng query gửi cho RAG-search: - Nêu loại hợp đồng (template_type, title). - Đính kèm JSON contract_info. Mục tiêu: tìm các điều luật liên quan đến loại hợp đồng và các nội dung chính. \u0026#34;\u0026#34;\u0026#34; template_type = template_metadata.get(\u0026#34;template_type\u0026#34;) or \u0026#34;\u0026#34; title = template_metadata.get(\u0026#34;title\u0026#34;) or \u0026#34;\u0026#34; header = f\u0026#34;Loại hợp đồng: {template_type}. Tiêu đề: {title}.\u0026#34; body = json.dumps(contract_info, ensure_ascii=False, indent=2) query = ( header + \u0026#34;\\n\\nDưới đây là thông tin chi tiết về hợp đồng (JSON):\\n\u0026#34; + body + \u0026#34;\\n\\nHãy tìm các văn bản pháp luật Việt Nam liên quan trực tiếp tới loại hợp đồng này, \u0026#34; \u0026#34;đặc biệt là về: thời hạn thuê/mua, nghĩa vụ các bên, chấm dứt hợp đồng, phạt vi phạm, \u0026#34; \u0026#34;bồi thường thiệt hại, quyền sử dụng tài sản.\u0026#34; ) return query def call_rag_lambda(query: str, language: str = \u0026#34;vi\u0026#34;) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34; Gọi trực tiếp Lambda rag_search (invokeFunction). Lambda rag_search hiện tại trả về dạng: { \u0026#34;statusCode\u0026#34;: 200, \u0026#34;body\u0026#34;: \u0026#34;{\\\u0026#34;query\\\u0026#34;:..., \\\u0026#34;results\\\u0026#34;: [...]}\u0026#34; } \u0026#34;\u0026#34;\u0026#34; if not RAG_FUNCTION_NAME: return {} payload = { \u0026#34;query\u0026#34;: query, \u0026#34;language\u0026#34;: language, \u0026#34;top_k\u0026#34;: 6, \u0026#34;filters\u0026#34;: { \u0026#34;source_type\u0026#34;: [\u0026#34;legal\u0026#34;] # Có thể thêm \u0026#34;field\u0026#34;: [\u0026#34;Xây dựng - Đô thị\u0026#34;] nếu muốn tập trung BĐS } } try: response = lambda_client.invoke( FunctionName=RAG_FUNCTION_NAME, InvocationType=\u0026#34;RequestResponse\u0026#34;, Payload=json.dumps(payload).encode(\u0026#34;utf-8\u0026#34;), ) except Exception as e: logger.warning(\u0026#34;RAG Lambda invoke failed: %s\u0026#34;, e) return {} try: raw_payload = response[\u0026#34;Payload\u0026#34;].read() resp_payload = json.loads(raw_payload) except Exception as e: logger.warning(\u0026#34;Failed to parse RAG Lambda raw payload: %s\u0026#34;, e) return {} # Trường hợp rag_search vẫn đang dùng make_response(statusCode, body) if isinstance(resp_payload, dict) and \u0026#34;statusCode\u0026#34; in resp_payload: status = resp_payload.get(\u0026#34;statusCode\u0026#34;, 500) if status != 200: logger.warning(\u0026#34;RAG Lambda returned status %s: %s\u0026#34;, status, resp_payload.get(\u0026#34;body\u0026#34;)) return {} body = resp_payload.get(\u0026#34;body\u0026#34;) or \u0026#34;{}\u0026#34; try: return json.loads(body) except json.JSONDecodeError: logger.warning(\u0026#34;RAG Lambda body is not valid JSON\u0026#34;) return {} # Nếu sau này anh sửa rag_search trả raw dict, có thể rơi vào đây if isinstance(resp_payload, dict): return resp_payload return {} def build_legal_context_text(rag_result: Dict[str, Any]) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34; Nhận kết quả từ RAG (rag_search) và build thành một block text để nhét vào prompt. \u0026#34;\u0026#34;\u0026#34; chunks = rag_result.get(\u0026#34;results\u0026#34;) or [] if not chunks: return \u0026#34;\u0026#34; lines: List[str] = [] for i, c in enumerate(chunks, start=1): title = c.get(\u0026#34;title\u0026#34;) or \u0026#34;\u0026#34; article_no = c.get(\u0026#34;article_no\u0026#34;) or \u0026#34;\u0026#34; article_title = c.get(\u0026#34;article_title\u0026#34;) or \u0026#34;\u0026#34; text = c.get(\u0026#34;text\u0026#34;) or \u0026#34;\u0026#34; header = f\u0026#34;[Trích dẫn pháp luật {i} – {title}\u0026#34; if article_no: header += f\u0026#34;, {article_no}\u0026#34; if article_title: header += f\u0026#34;: {article_title}\u0026#34; header += \u0026#34;]\u0026#34; lines.append(header) lines.append(text) lines.append(\u0026#34;\u0026#34;) return \u0026#34;\\n\u0026#34;.join(lines) def retrieve_legal_context_for_template( template_metadata: Dict[str, Any], contract_info: Dict[str, Any], language: str ) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34; Hàm tổng: build query -\u0026gt; call RAG -\u0026gt; build text. Nếu có lỗi hoặc không có kết quả, trả về \u0026#34;\u0026#34; để không chặn flow chính. \u0026#34;\u0026#34;\u0026#34; try: query = build_rag_query(template_metadata, contract_info) rag_result = call_rag_lambda(query=query, language=language) context_text = build_legal_context_text(rag_result) return context_text except Exception as e: logger.warning(\u0026#34;retrieve_legal_context_for_template failed: %s\u0026#34;, e) return \u0026#34;\u0026#34; # ------------------------------------------------------------------- # LLM: sinh hợp đồng # ------------------------------------------------------------------- def build_system_prompt() -\u0026gt; str: return ( \u0026#34;Bạn là một luật sư hợp đồng tại Việt Nam, chuyên soạn thảo hợp đồng thuê/mua bán/chuyển nhượng bất động sản.\\n\u0026#34; \u0026#34;- Ngôn ngữ: tiếng Việt, văn phong rõ ràng, chặt chẽ, nhưng dễ hiểu.\\n\u0026#34; \u0026#34;- Hãy soạn thảo hợp đồng dựa trên thông tin đầu vào (contract_info), loại hợp đồng (template_type) \u0026#34; \u0026#34;và các trích dẫn pháp luật được cung cấp.\\n\u0026#34; \u0026#34;- Nếu thông tin đầu vào chưa đầy đủ, hãy điền các điều khoản theo thông lệ phổ biến, \u0026#34; \u0026#34;nhưng không bịa số liệu quá cụ thể.\\n\u0026#34; \u0026#34;- Luôn đảm bảo quyền và nghĩa vụ của các bên cân bằng, ưu tiên tuân thủ pháp luật Việt Nam.\\n\u0026#34; \u0026#34;- Output CHỈ là nội dung hợp đồng hoàn chỉnh dạng text thuần, không thêm giải thích.\\n\u0026#34; ) def build_user_prompt( template_metadata: Dict[str, Any], contract_info: Dict[str, Any], template_raw_text: str, legal_context: str ) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34; Prompt cho model: cung cấp - Thông tin template (type, title, ...). - contract_info (JSON). - context pháp luật từ RAG. - (Optional) snippet template_raw_text nếu đọc được. \u0026#34;\u0026#34;\u0026#34; title = template_metadata.get(\u0026#34;title\u0026#34;) or \u0026#34;\u0026#34; template_type = template_metadata.get(\u0026#34;template_type\u0026#34;) or \u0026#34;\u0026#34; template_desc = f\u0026#34;Loại hợp đồng: {template_type}. Tiêu đề mẫu: {title}.\u0026#34; user_parts: List[str] = [] user_parts.append(template_desc) # Thông tin pháp luật từ RAG if legal_context: user_parts.append( \u0026#34;\\nDưới đây là một số trích dẫn pháp luật và điều khoản liên quan do hệ thống truy xuất được \u0026#34; \u0026#34;(hãy dùng làm căn cứ khi soạn thảo, nhưng không cần trích nguyên văn toàn bộ):\\n\u0026#34; ) user_parts.append(legal_context) # contract_info user_parts.append(\u0026#34;\\nDưới đây là thông tin đầu vào (contract_info) ở dạng JSON:\\n\u0026#34;) user_parts.append(json.dumps(contract_info, ensure_ascii=False, indent=2)) # snippet từ template gốc (nếu có) if template_raw_text: max_chars = 3000 snippet = template_raw_text[:max_chars] user_parts.append( \u0026#34;\\nDưới đây là một phần nội dung gốc của mẫu hợp đồng (nếu đọc được, chỉ dùng tham khảo cấu trúc, \u0026#34; \u0026#34;không cần copy y nguyên):\\n\u0026#34; ) user_parts.append(snippet) user_parts.append( \u0026#34;\\nYÊU CẦU:\\n\u0026#34; \u0026#34;- Hãy soạn thảo TOÀN BỘ hợp đồng hoàn chỉnh, có đầy đủ phần mở đầu, điều khoản chi tiết, \u0026#34; \u0026#34;điều khoản chung, điều khoản về giải quyết tranh chấp, chữ ký.\\n\u0026#34; \u0026#34;- Trả về nội dung hợp đồng ở dạng text thuần (plain text), mỗi điều khoản nên cách nhau ít nhất một dòng trống.\\n\u0026#34; \u0026#34;- Không thêm bất kỳ giải thích nào ngoài nội dung hợp đồng.\u0026#34; ) return \u0026#34;\\n\u0026#34;.join(user_parts) def call_bedrock_generate_contract(system_prompt: str, user_prompt: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34; Gọi Bedrock (Claude Haiku) để sinh hợp đồng, trả về text thuần. \u0026#34;\u0026#34;\u0026#34; logger.info(\u0026#34;Calling Bedrock model %s for contract generation ...\u0026#34;, MODEL_ID) try: response = bedrock.converse( modelId=MODEL_ID, system=[{\u0026#34;text\u0026#34;: system_prompt}], messages=[ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: [{\u0026#34;text\u0026#34;: user_prompt}], } ], inferenceConfig={ \u0026#34;maxTokens\u0026#34;: 4096, \u0026#34;temperature\u0026#34;: 0.2, \u0026#34;topP\u0026#34;: 0.9, }, ) except ClientError as e: logger.error(\u0026#34;Bedrock invocation failed: %s\u0026#34;, e) raise try: output_message = response[\u0026#34;output\u0026#34;][\u0026#34;message\u0026#34;] content_list = output_message.get(\u0026#34;content\u0026#34;, []) if not content_list: raise ValueError(\u0026#34;Empty content from model\u0026#34;) model_text = content_list[0].get(\u0026#34;text\u0026#34;, \u0026#34;\u0026#34;) except Exception as e: logger.error(\u0026#34;Failed to extract text from Bedrock response: %s\u0026#34;, e) raise return model_text def to_html_from_text(contract_text: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34; Đơn giản: mỗi dòng -\u0026gt; \u0026lt;p\u0026gt;, dòng trống -\u0026gt; \u0026lt;br\u0026gt;. \u0026#34;\u0026#34;\u0026#34; lines = contract_text.splitlines() html_lines: List[str] = [] html_lines.append(\u0026#34;\u0026lt;html\u0026gt;\u0026lt;body\u0026gt;\u0026#34;) for line in lines: line = line.strip() if not line: html_lines.append(\u0026#34;\u0026lt;br\u0026gt;\u0026#34;) else: esc = ( line.replace(\u0026#34;\u0026amp;\u0026#34;, \u0026#34;\u0026amp;amp;\u0026#34;) .replace(\u0026#34;\u0026lt;\u0026#34;, \u0026#34;\u0026amp;lt;\u0026#34;) .replace(\u0026#34;\u0026gt;\u0026#34;, \u0026#34;\u0026amp;gt;\u0026#34;) ) html_lines.append(f\u0026#34;\u0026lt;p\u0026gt;{esc}\u0026lt;/p\u0026gt;\u0026#34;) html_lines.append(\u0026#34;\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt;\u0026#34;) return \u0026#34;\\n\u0026#34;.join(html_lines) def save_generated_to_s3(contract_text: str, contract_html: str) -\u0026gt; Dict[str, str]: \u0026#34;\u0026#34;\u0026#34; Lưu contract_text và contract_html lên S3, trả về paths. \u0026#34;\u0026#34;\u0026#34; now = datetime.datetime.utcnow() y = now.year m = now.month contract_id = str(uuid.uuid4()) base_prefix = f\u0026#34;generated/contracts/{y}/{m:02d}/{contract_id}\u0026#34; text_key = f\u0026#34;{base_prefix}.txt\u0026#34; html_key = f\u0026#34;{base_prefix}.html\u0026#34; try: s3.put_object( Bucket=TEMPLATE_BUCKET, Key=text_key, Body=contract_text.encode(\u0026#34;utf-8\u0026#34;), ContentType=\u0026#34;text/plain; charset=utf-8\u0026#34;, ) s3.put_object( Bucket=TEMPLATE_BUCKET, Key=html_key, Body=contract_html.encode(\u0026#34;utf-8\u0026#34;), ContentType=\u0026#34;text/html; charset=utf-8\u0026#34;, ) except ClientError as e: logger.warning(\u0026#34;Failed to upload generated contract to S3: %s\u0026#34;, e) return {} return { \u0026#34;contract_text_path\u0026#34;: text_key, \u0026#34;contract_html_path\u0026#34;: html_key, } def make_response(status_code: int, body: Dict[str, Any]) -\u0026gt; Dict[str, Any]: return { \u0026#34;statusCode\u0026#34;: status_code, \u0026#34;headers\u0026#34;: { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;Access-Control-Allow-Origin\u0026#34;: \u0026#34;*\u0026#34;, # demo }, \u0026#34;body\u0026#34;: json.dumps(body, ensure_ascii=False), } # ------------------------------------------------------------------- # Lambda handler # ------------------------------------------------------------------- def lambda_handler(event, context): logger.info(\u0026#34;Received event: %s\u0026#34;, json.dumps(event)[:1000]) try: data = parse_event_body(event) template_id = (data.get(\u0026#34;template_id\u0026#34;) or \u0026#34;\u0026#34;).strip() if not template_id: return make_response(400, {\u0026#34;error\u0026#34;: \u0026#34;template_id is required\u0026#34;}) contract_info = data.get(\u0026#34;contract_info\u0026#34;) or {} if not isinstance(contract_info, dict): return make_response(400, {\u0026#34;error\u0026#34;: \u0026#34;contract_info must be an object\u0026#34;}) language = (data.get(\u0026#34;language\u0026#34;) or \u0026#34;vi\u0026#34;).lower() # 1. Load template metadata load_template_metadata_if_needed() metadata = TEMPLATE_CACHE[\u0026#34;by_id\u0026#34;].get(template_id) if not metadata: return make_response(404, {\u0026#34;error\u0026#34;: f\u0026#34;Template not found for template_id={template_id}\u0026#34;}) # 2. (Optional) load raw text của file template từ S3 template_raw_text = load_template_raw_text(metadata) # 3. Lấy context pháp luật từ RAG legal_context = retrieve_legal_context_for_template(metadata, contract_info, language) # 4. Build prompts system_prompt = build_system_prompt() user_prompt = build_user_prompt(metadata, contract_info, template_raw_text, legal_context) # 5. Gọi Bedrock để sinh hợp đồng contract_text = call_bedrock_generate_contract(system_prompt, user_prompt) # 6. Convert sang HTML contract_html = to_html_from_text(contract_text) # 7. Lưu lên S3 #s3_paths = save_generated_to_s3(contract_text, contract_html) s3_paths = {} # 8. Build response resp_body = { \u0026#34;template_id\u0026#34;: template_id, \u0026#34;template_title\u0026#34;: metadata.get(\u0026#34;title\u0026#34;), \u0026#34;template_type\u0026#34;: metadata.get(\u0026#34;template_type\u0026#34;), \u0026#34;language\u0026#34;: language, \u0026#34;contract_text\u0026#34;: contract_text, \u0026#34;contract_html\u0026#34;: contract_html, \u0026#34;s3_paths\u0026#34;: s3_paths, \u0026#34;debug\u0026#34;: { \u0026#34;used_template_file\u0026#34;: metadata.get(\u0026#34;source_raw_path\u0026#34;), \u0026#34;source_type\u0026#34;: metadata.get(\u0026#34;source_type\u0026#34;), \u0026#34;rag_used\u0026#34;: bool(legal_context), }, } return make_response(200, resp_body) except ValueError as ve: logger.warning(\u0026#34;Bad request: %s\u0026#34;, ve) return make_response(400, {\u0026#34;error\u0026#34;: str(ve)}) except ClientError as ce: logger.error(\u0026#34;AWS client error: %s\u0026#34;, ce) return make_response(502, {\u0026#34;error\u0026#34;: \u0026#34;Upstream AWS error\u0026#34;, \u0026#34;details\u0026#34;: str(ce)}) except Exception as e: logger.error(\u0026#34;Unexpected error: %s\u0026#34;, e) return make_response(500, {\u0026#34;error\u0026#34;: \u0026#34;Internal server error\u0026#34;, \u0026#34;details\u0026#34;: str(e)}) Configuration: Memory: 512 MB Timeout: 30 sec Environment variables: MODEL_ID: ahropic.claude-3-haiku-20240307-v1:0nt RAG_FUNCTION_NAME: ragsearch TEMPLATE_BUCKET: \u0026lt;Tên-Bucket-S3-Của-Bạn\u0026gt; TEMPLATE_METADATA_KEY: index/template_metadata.jsonl Permissions: Add AmazonBedrockFullAccess và AmazonS3FullAccess. 2. Hàm Chat tổng quát (CallLLM) Create Function: Name: CallLLM Runtime: Python 3.12 Code: Copy từ file CallLLM.py trong source code -\u0026gt; Deploy. import json import os import base64 import logging from dataclasses import dataclass from typing import Optional, Tuple, Dict, Any import boto3 from botocore.exceptions import ClientError # ----------------------------------------------------------------------------- # 1. Logging \u0026amp; Config # ----------------------------------------------------------------------------- logger = logging.getLogger() logger.setLevel(logging.INFO) bedrock = boto3.client(\u0026#34;bedrock-runtime\u0026#34;) lambda_client = boto3.client(\u0026#34;lambda\u0026#34;) # ENV: tên Lambda RAG-search, ví dụ \u0026#39;rag_search\u0026#39; RAG_FUNCTION_NAME = os.getenv(\u0026#34;RAG_FUNCTION_NAME\u0026#34;) class Config: MODEL_ID: str = os.getenv( \u0026#34;MODEL_ID\u0026#34;, \u0026#34;anthropic.claude-3-haiku-20240307-v1:0\u0026#34; ) ALLOWED_FORMATS = {\u0026#34;pdf\u0026#34;, \u0026#34;doc\u0026#34;, \u0026#34;docx\u0026#34;, \u0026#34;txt\u0026#34;, \u0026#34;md\u0026#34;, \u0026#34;html\u0026#34;} MAX_FILE_SIZE_BYTES: int = int(10 * 1024 * 1024) # ~10MB SYSTEM_PROMPT = \u0026#34;\u0026#34;\u0026#34; Bạn là một trợ lý pháp lý tự động, hỗ trợ người dùng phổ thông phân tích rủi ro trong hợp đồng tiếng Việt. NGUYÊN TẮC: - Chỉ phân tích dựa trên nội dung hợp đồng được cung cấp. - Luôn nhấn mạnh rằng kết quả chỉ mang tính chất tham khảo, không thay thế luật sư. - Nếu không chắc về điều luật cụ thể, hãy để trống các trường liên quan đến điều/khoản, KHÔNG tự bịa số điều luật. - Ưu tiên ngôn ngữ dễ hiểu, súc tích, dùng tiếng Việt. NHIỆM VỤ: - Tóm tắt ngắn gọn nội dung chính của hợp đồng. - Xác định các rủi ro chính (nếu có), phân loại rủi ro và đánh giá mức độ nghiêm trọng. - Đề xuất cách chỉnh sửa/bo sung điều khoản để giảm rủi ro. - Nếu có thể, chỉ ra các căn cứ pháp luật liên quan (tên luật, điều, khoản); nếu không chắc, hãy để trống. YÊU CẦU QUAN TRỌNG VỀ OUTPUT: - CHỈ trả về một JSON hợp lệ (valid JSON), không chứa bất kỳ giải thích, bình luận hay text nào ở bên ngoài. - KHÔNG dùng markdown, KHÔNG dùng ```json. - JSON phải có cấu trúc CHÍNH XÁC như sau: { \u0026#34;summary\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;overall_risk_level\u0026#34;: \u0026#34;LOW | MEDIUM | HIGH | CRITICAL\u0026#34;, \u0026#34;risk_items\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;string, ví dụ R1, R2,...\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;string, tiêu đề ngắn cho rủi ro\u0026#34;, \u0026#34;clause_excerpt\u0026#34;: \u0026#34;string, trích đoạn điều khoản liên quan (nếu xác định được, nếu không thì chuỗi rỗng)\u0026#34;, \u0026#34;risk_types\u0026#34;: [ \u0026#34;LegalCompliance | Financial | FraudScam | UnclearTerm | ImbalancedObligation\u0026#34; ], \u0026#34;severity\u0026#34;: \u0026#34;LOW | MEDIUM | HIGH | CRITICAL\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;string, mô tả chi tiết rủi ro bằng tiếng Việt\u0026#34;, \u0026#34;recommendation\u0026#34;: \u0026#34;string, gợi ý chỉnh sửa cụ thể bằng tiếng Việt\u0026#34;, \u0026#34;law_references\u0026#34;: [ { \u0026#34;law_name\u0026#34;: \u0026#34;string, có thể để chuỗi rỗng nếu không biết chắc\u0026#34;, \u0026#34;article\u0026#34;: \u0026#34;string, có thể để chuỗi rỗng nếu không biết chắc\u0026#34;, \u0026#34;clause\u0026#34;: \u0026#34;string, có thể để chuỗi rỗng nếu không biết chắc\u0026#34;, \u0026#34;note\u0026#34;: \u0026#34;string, ghi chú thêm nếu cần, có thể để chuỗi rỗng\u0026#34; } ] } ], \u0026#34;disclaimer\u0026#34;: \u0026#34;string, lời cảnh báo rằng đây không phải tư vấn pháp lý chính thức\u0026#34; } - Nếu không tìm thấy rủi ro nào đáng kể, hãy để risk_items là một mảng rỗng [] và overall_risk_level là \u0026#34;LOW\u0026#34;. - Đảm bảo JSON trả về là hợp lệ, không có dấu phẩy thừa, không có comment. \u0026#34;\u0026#34;\u0026#34;.strip() # ----------------------------------------------------------------------------- # 2. Data structures # ----------------------------------------------------------------------------- @dataclass class ContractInput: language: str contract_text: Optional[str] = None file_name: Optional[str] = None file_format: Optional[str] = None file_bytes: Optional[bytes] = None # context RAG do BE gửi lên (hoặc để None) rag_context: Optional[str] = None @property def has_file(self) -\u0026gt; bool: return self.file_bytes is not None and self.file_format is not None @property def has_text(self) -\u0026gt; bool: return self.contract_text is not None and self.contract_text.strip() != \u0026#34;\u0026#34; # ----------------------------------------------------------------------------- # 3. HTTP event parsing # ----------------------------------------------------------------------------- def parse_event_body(event: Dict[str, Any]) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34; Hỗ trợ cả trường hợp gọi qua API Gateway HTTP API (body là string) và invoke trực tiếp (event là dict). \u0026#34;\u0026#34;\u0026#34; if \u0026#34;body\u0026#34; not in event: # invoke trực tiếp return event body = event[\u0026#34;body\u0026#34;] if event.get(\u0026#34;isBase64Encoded\u0026#34;): body = base64.b64decode(body).decode(\u0026#34;utf-8\u0026#34;) try: data = json.loads(body) except json.JSONDecodeError: logger.warning(\u0026#34;Body is not valid JSON\u0026#34;) raise ValueError(\u0026#34;Request body must be valid JSON\u0026#34;) return data # ----------------------------------------------------------------------------- # 4. Request -\u0026gt; ContractInput (validation) # ----------------------------------------------------------------------------- def parse_contract_input(data: Dict[str, Any]) -\u0026gt; ContractInput: language = (data.get(\u0026#34;language\u0026#34;) or \u0026#34;vi\u0026#34;).lower() # Lấy context_rag (nếu BE gửi lên) rag_context = (data.get(\u0026#34;context_rag\u0026#34;) or \u0026#34;\u0026#34;).strip() or None # File branch file_b64 = data.get(\u0026#34;file_bytes_base64\u0026#34;) file_format = (data.get(\u0026#34;file_format\u0026#34;) or \u0026#34;\u0026#34;).lower() file_name = data.get(\u0026#34;file_name\u0026#34;) or None if file_b64 and file_format: if file_format not in Config.ALLOWED_FORMATS: raise ValueError( f\u0026#34;Unsupported file_format \u0026#39;{file_format}\u0026#39;. \u0026#34; f\u0026#34;Allowed: {sorted(Config.ALLOWED_FORMATS)}\u0026#34; ) try: file_bytes = base64.b64decode(file_b64) except Exception as e: logger.warning(\u0026#34;Invalid base64 for file_bytes_base64: %s\u0026#34;, e) raise ValueError(\u0026#34;file_bytes_base64 is not valid base64\u0026#34;) if len(file_bytes) \u0026gt; Config.MAX_FILE_SIZE_BYTES: raise ValueError( \u0026#34;File is too large for Bedrock document input \u0026#34; \u0026#34;(limit khoảng 4.5 MB).\u0026#34; ) return ContractInput( language=language, file_name=file_name, file_format=file_format, file_bytes=file_bytes, rag_context=rag_context, ) # Text branch contract_text = (data.get(\u0026#34;contract_text\u0026#34;) or \u0026#34;\u0026#34;).strip() if not contract_text: raise ValueError( \u0026#34;Either contract_text or (file_bytes_base64 + file_format) is required\u0026#34; ) return ContractInput( language=language, contract_text=contract_text, rag_context=rag_context, ) # ----------------------------------------------------------------------------- # 5. RAG hook: dùng Lambda invoke thay vì API Gateway # ----------------------------------------------------------------------------- def retrieve_legal_context(contract_text: str, language: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34; Gọi trực tiếp Lambda rag_search (invokeFunction), trả về text context để nhét vào prompt LLM. Nếu RAG lỗi hoặc chưa cấu hình, trả chuỗi rỗng để không chặn flow chính. Yêu cầu: - ENV: RAG_FUNCTION_NAME = tên hàm Lambda rag_search - IAM: lambda:InvokeFunction trên hàm đó \u0026#34;\u0026#34;\u0026#34; if not RAG_FUNCTION_NAME: return \u0026#34;\u0026#34; # Payload gửi sang rag_search payload = { \u0026#34;query\u0026#34;: contract_text, \u0026#34;language\u0026#34;: language, \u0026#34;top_k\u0026#34;: 8, \u0026#34;filters\u0026#34;: { \u0026#34;source_type\u0026#34;: [\u0026#34;legal\u0026#34;], } } try: response = lambda_client.invoke( FunctionName=RAG_FUNCTION_NAME, InvocationType=\u0026#34;RequestResponse\u0026#34;, Payload=json.dumps(payload).encode(\u0026#34;utf-8\u0026#34;), ) except Exception as e: print(f\u0026#34;[WARN] RAG Lambda invoke failed: {e}\u0026#34;) return \u0026#34;\u0026#34; try: raw_payload = response[\u0026#34;Payload\u0026#34;].read() resp_payload = json.loads(raw_payload) except Exception as e: print(f\u0026#34;[WARN] Failed to parse RAG Lambda raw payload: {e}\u0026#34;) return \u0026#34;\u0026#34; # Trường hợp rag_search đang trả theo format API (statusCode + body) if isinstance(resp_payload, dict) and \u0026#34;statusCode\u0026#34; in resp_payload: status = resp_payload.get(\u0026#34;statusCode\u0026#34;, 500) if status != 200: print(f\u0026#34;[WARN] RAG Lambda returned status {status}: {resp_payload.get(\u0026#39;body\u0026#39;)}\u0026#34;) return \u0026#34;\u0026#34; body = resp_payload.get(\u0026#34;body\u0026#34;) or \u0026#34;{}\u0026#34; try: result = json.loads(body) except json.JSONDecodeError: print(\u0026#34;[WARN] RAG Lambda body is not valid JSON\u0026#34;) return \u0026#34;\u0026#34; else: # Nếu sau này rag_search trả raw dict, dùng luôn if isinstance(resp_payload, dict): result = resp_payload else: return \u0026#34;\u0026#34; chunks = result.get(\u0026#34;results\u0026#34;) or [] if not chunks: return \u0026#34;\u0026#34; lines = [] for i, c in enumerate(chunks, start=1): title = c.get(\u0026#34;title\u0026#34;) or \u0026#34;\u0026#34; article_no = c.get(\u0026#34;article_no\u0026#34;) or \u0026#34;\u0026#34; article_title = c.get(\u0026#34;article_title\u0026#34;) or \u0026#34;\u0026#34; text = c.get(\u0026#34;text\u0026#34;) or \u0026#34;\u0026#34; header = f\u0026#34;[Trích dẫn {i} – {title}\u0026#34; if article_no: header += f\u0026#34;, {article_no}\u0026#34; if article_title: header += f\u0026#34;: {article_title}\u0026#34; header += \u0026#34;]\u0026#34; lines.append(header) lines.append(text) lines.append(\u0026#34;\u0026#34;) context_str = \u0026#34;\\n\u0026#34;.join(lines) return context_str # ----------------------------------------------------------------------------- # 6. Prompt builder \u0026amp; Bedrock client # ----------------------------------------------------------------------------- def build_user_prompt_text(contract_text: str, context: str | None = None) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34; Prompt dùng cho mode TEXT. Có thêm context (RAG) nếu không rỗng. \u0026#34;\u0026#34;\u0026#34; base = ( \u0026#34;Dưới đây là nội dung hợp đồng cần phân tích rủi ro. \u0026#34; \u0026#34;Hãy đọc kỹ và TRẢ VỀ DUY NHẤT một JSON hợp lệ theo đúng cấu trúc đã được mô tả trong hướng dẫn hệ thống.\\n\\n\u0026#34; \u0026#34;=== NỘI DUNG HỢP ĐỒNG ===\\n\u0026#34; f\u0026#34;{contract_text}\\n\u0026#34; \u0026#34;=== HẾT NỘI DUNG HỢP ĐỒNG ===\u0026#34; ) if context: return ( base + \u0026#34;\\n\\n=== THÔNG TIN THAM KHẢO TỪ THƯ VIỆN PHÁP LUẬT / MẪU HỢP ĐỒNG (RAG) ===\\n\u0026#34; + context + \u0026#34;\\n=== HẾT THÔNG TIN THAM KHẢO ===\u0026#34; ) return base def call_bedrock_text(contract_text: str, language: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34; Gọi Bedrock Converse API với TEXT và trả về raw text từ model. Có sẵn hook context từ RAG. \u0026#34;\u0026#34;\u0026#34; context = retrieve_legal_context(contract_text, language=language) user_prompt = build_user_prompt_text(contract_text, context=context) logger.info(\u0026#34;Calling Bedrock model %s with TEXT ...\u0026#34;, Config.MODEL_ID) try: response = bedrock.converse( modelId=Config.MODEL_ID, system=[{\u0026#34;text\u0026#34;: SYSTEM_PROMPT}], messages=[ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: [{\u0026#34;text\u0026#34;: user_prompt}], } ], inferenceConfig={ \u0026#34;maxTokens\u0026#34;: 4096, \u0026#34;temperature\u0026#34;: 0.2, \u0026#34;topP\u0026#34;: 0.9, }, ) logger.info(\u0026#34;Received response from Bedrock (text mode)\u0026#34;) except ClientError as e: logger.error(\u0026#34;Bedrock invocation failed (text mode): %s\u0026#34;, e) raise try: output_message = response[\u0026#34;output\u0026#34;][\u0026#34;message\u0026#34;] content_list = output_message.get(\u0026#34;content\u0026#34;, []) if not content_list: raise ValueError(\u0026#34;Empty content from model (text mode)\u0026#34;) model_text = content_list[0].get(\u0026#34;text\u0026#34;, \u0026#34;\u0026#34;) except Exception as e: logger.error(\u0026#34;Failed to extract text from Bedrock response (text mode): %s\u0026#34;, e) raise return model_text def call_bedrock_document( file_bytes: bytes, file_format: str, file_name: Optional[str], language: str, context: Optional[str] = None, ) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34; Gọi Bedrock Converse API với DOCUMENT (pdf/docx/...) và trả về raw text từ model. file_format: pdf, doc, docx, txt, md, html context: chuỗi RAG (luật, mẫu hợp đồng) đã được build ở BE (context_rag) \u0026#34;\u0026#34;\u0026#34; if not file_name: file_name = f\u0026#34;uploaded_contract.{file_format}\u0026#34; # Nhét context RAG (nếu có) vào user_text user_text = ( \u0026#34;Tài liệu đính kèm là một hợp đồng cần phân tích rủi ro. \u0026#34; \u0026#34;Hãy đọc toàn bộ tài liệu và TRẢ VỀ DUY NHẤT một JSON hợp lệ theo đúng schema đã được mô tả trong system prompt.\u0026#34; ) if context: user_text += ( \u0026#34;\\n\\n=== THÔNG TIN THAM KHẢO TỪ THƯ VIỆN PHÁP LUẬT / MẪU HỢP ĐỒNG (RAG) ===\\n\u0026#34; f\u0026#34;{context}\\n\u0026#34; \u0026#34;=== HẾT THÔNG TIN THAM KHẢO ===\u0026#34; ) logger.info( \u0026#34;Calling Bedrock model %s with DOCUMENT (format=%s, name=%s, size=%d bytes, has_context=%s) ...\u0026#34;, Config.MODEL_ID, file_format, file_name, len(file_bytes), bool(context) ) try: response = bedrock.converse( modelId=Config.MODEL_ID, system=[{\u0026#34;text\u0026#34;: SYSTEM_PROMPT}], messages=[ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: [ {\u0026#34;text\u0026#34;: user_text}, { \u0026#34;document\u0026#34;: { \u0026#34;format\u0026#34;: file_format, \u0026#34;name\u0026#34;: file_name, \u0026#34;source\u0026#34;: {\u0026#34;bytes\u0026#34;: file_bytes}, } }, ], } ], inferenceConfig={ \u0026#34;maxTokens\u0026#34;: 4096, \u0026#34;temperature\u0026#34;: 0.2, \u0026#34;topP\u0026#34;: 0.9, }, ) logger.info(\u0026#34;Received response from Bedrock (document mode)\u0026#34;) except ClientError as e: logger.error(\u0026#34;Bedrock invocation failed (document mode): %s\u0026#34;, e) raise try: output_message = response[\u0026#34;output\u0026#34;][\u0026#34;message\u0026#34;] content_list = output_message.get(\u0026#34;content\u0026#34;, []) if not content_list: raise ValueError(\u0026#34;Empty content from model (document mode)\u0026#34;) model_text = content_list[0].get(\u0026#34;text\u0026#34;, \u0026#34;\u0026#34;) except Exception as e: logger.error( \u0026#34;Failed to extract text from Bedrock document response: %s\u0026#34;, e ) raise return model_text # ----------------------------------------------------------------------------- # 7. Model JSON parsing # ----------------------------------------------------------------------------- def parse_model_json(model_output_text: str) -\u0026gt; Tuple[Dict[str, Any], Optional[str]]: \u0026#34;\u0026#34;\u0026#34; Phiên bản robust: cố gắng trích JSON object đầu tiên trong output. \u0026#34;\u0026#34;\u0026#34; try: text = model_output_text.strip() # 1. Tìm vị trí dấu \u0026#39;{\u0026#39; đầu tiên start_idx = text.find(\u0026#34;{\u0026#34;) # 2. Tìm vị trí dấu \u0026#39;}\u0026#39; cuối cùng end_idx = text.rfind(\u0026#34;}\u0026#34;) if start_idx != -1 and end_idx != -1: text = text[start_idx : end_idx + 1] else: raise ValueError(\u0026#34;No JSON object found in output\u0026#34;) analysis = json.loads(text) return analysis, None except Exception as e: logger.warning(\u0026#34;Model output parsing failed: %s\u0026#34;, e) logger.info(\u0026#34;Raw output causing error: %s\u0026#34;, model_output_text) analysis = { \u0026#34;summary\u0026#34;: f\u0026#34;AI đã trả về kết quả nhưng không đúng định dạng JSON. (Lỗi: {str(e)})\u0026#34;, \u0026#34;overall_risk_level\u0026#34;: \u0026#34;UNKNOWN\u0026#34;, \u0026#34;risk_items\u0026#34;: [], \u0026#34;disclaimer\u0026#34;: \u0026#34;Lỗi kỹ thuật từ phía AI Parser.\u0026#34; } return analysis, model_output_text # ----------------------------------------------------------------------------- # 8. Core use case: analyze_contract # ----------------------------------------------------------------------------- def analyze_contract(contract_input: ContractInput) -\u0026gt; Tuple[Dict[str, Any], Optional[str]]: \u0026#34;\u0026#34;\u0026#34; Core logic: - Chọn mode TEXT hoặc DOCUMENT. - Gọi Bedrock tương ứng. - Parse JSON từ output. \u0026#34;\u0026#34;\u0026#34; if contract_input.has_file: model_output_text = call_bedrock_document( file_bytes=contract_input.file_bytes, file_format=contract_input.file_format, file_name=contract_input.file_name, language=contract_input.language, context=contract_input.rag_context, # context RAG do BE truyền (nếu có) ) elif contract_input.has_text: model_output_text = call_bedrock_text( contract_text=contract_input.contract_text, language=contract_input.language, ) else: raise ValueError(\u0026#34;No valid input provided\u0026#34;) analysis, raw = parse_model_json(model_output_text) return analysis, raw # ----------------------------------------------------------------------------- # 9. HTTP response helper # ----------------------------------------------------------------------------- def make_response(status_code: int, body: Dict[str, Any]) -\u0026gt; Dict[str, Any]: return { \u0026#34;statusCode\u0026#34;: status_code, \u0026#34;headers\u0026#34;: { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;Access-Control-Allow-Origin\u0026#34;: \u0026#34;*\u0026#34;, }, \u0026#34;body\u0026#34;: json.dumps(body, ensure_ascii=False), } # ----------------------------------------------------------------------------- # 10. Lambda handler # ----------------------------------------------------------------------------- def lambda_handler(event, context): logger.info(\u0026#34;Received event: %s\u0026#34;, json.dumps(event)[:1000]) try: data = parse_event_body(event) contract_input = parse_contract_input(data) analysis, raw_model_output = analyze_contract(contract_input) response_body = { \u0026#34;analysis\u0026#34;: analysis, \u0026#34;model\u0026#34;: Config.MODEL_ID, \u0026#34;raw_model_output\u0026#34;: raw_model_output, \u0026#34;language\u0026#34;: contract_input.language, } return make_response(200, response_body) except ValueError as ve: logger.warning(\u0026#34;Bad request: %s\u0026#34;, ve) return make_response(400, {\u0026#34;error\u0026#34;: str(ve)}) except ClientError as ce: logger.error(\u0026#34;Bedrock client error: %s\u0026#34;, ce) return make_response( 502, {\u0026#34;error\u0026#34;: \u0026#34;Bedrock invocation failed\u0026#34;, \u0026#34;details\u0026#34;: str(ce)}, ) except Exception as e: logger.error(\u0026#34;Unexpected error: %s\u0026#34;, e) return make_response( 500, {\u0026#34;error\u0026#34;: \u0026#34;Internal server error\u0026#34;, \u0026#34;details\u0026#34;: str(e)}, ) Configuration: Memory: 1024 MB Timeout: 50 sec Environment variables: MODEL_ID: ahropic.claude-3-haiku-20240307-v1:0nt RAG_FUNCTION_NAME: ragsearch S3_BUCKET_RAW: \u0026lt;Tên-Bucket-S3-Của-Bạn\u0026gt; Permissions: Add AmazonBedrockFullAccess và AmazonS3FullAccess. "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/vi/1-worklog/1.10-week10/",
	"title": "Worklog Tuần 10",
	"tags": [],
	"description": "",
	"content": "📘 Báo cáo công việc Tuần 10 – AWS Journey 1. Mục tiêu hàng tuần Trong Tuần 10, quá trình khám phá mở rộng sang lĩnh vực Trí tuệ nhân tạo (AI) và Học máy (ML) của AWS. Mục tiêu chính là phân biệt giữa việc xây dựng các mô hình tùy chỉnh và sử dụng các dịch vụ AI được huấn luyện trước. Các mục tiêu chính bao gồm:\nHệ sinh thái AI AWS – Hiểu bối cảnh của các dịch vụ ML (SageMaker) so với Dịch vụ AI (Rekognition, Comprehend, Kendra). Amazon SageMaker – Trải nghiệm vòng đời ML đầy đủ: Xây dựng (Build), Huấn luyện (Train) và Triển khai (Deploy). Thị giác máy tính \u0026amp; NLP – Triển khai phân tích hình ảnh và xử lý ngôn ngữ tự nhiên mà không cần chuyên môn sâu về khoa học dữ liệu bằng cách sử dụng AWS API. Tìm kiếm thông minh – Thiết lập khả năng tìm kiếm doanh nghiệp với Amazon Kendra. Tuần này làm nổi bật cách AWS dân chủ hóa AI, cho phép các nhà phát triển thêm trí thông minh vào ứng dụng thông qua các lệnh gọi API hoặc xây dựng các mô hình tùy chỉnh với cơ sở hạ tầng được quản lý.\n2. Tóm tắt công việc chi tiết 🗂 Bảng hoạt động Ngày Nhiệm vụ Ngày bắt đầu Ngày hoàn thành Tham khảo Thứ Hai - Tổng quan về AI/ML trên AWS\n- Tìm hiểu các dịch vụ hỗ trợ ML: SageMaker, Rekognition, Comprehend, Kendra, Translate, Polly 10/11/2025 10/11/2025 AWS Journey Thứ Ba - Thực hành với Amazon SageMaker:\n- Tạo Notebook Instance\n- Huấn luyện mô hình đơn giản (Hồi quy tuyến tính / Phân loại ảnh)\n- Triển khai endpoint và kiểm tra dự đoán 11/11/2025 11/11/2025 AWS Journey Thứ Tư - Làm quen với Amazon Rekognition\n- Demo nhận diện khuôn mặt và vật thể trong ảnh/video\n- Tích hợp Rekognition API vào ứng dụng web nhỏ 12/11/2025 12/11/2025 AWS Journey Thứ Năm - Thực hành Amazon Comprehend (xử lý ngôn ngữ tự nhiên)\n- Thử nghiệm Amazon Kendra (tìm kiếm thông minh theo ngữ cảnh)\n- So sánh ưu điểm và hạn chế của từng dịch vụ 13/11/2025 13/11/2025 AWS Journey Thứ Sáu - Tổng hợp kiến thức Tuần 10 (Quy trình phát triển AI/ML)\n- Ứng dụng thực tế của AI/ML trong doanh nghiệp\n- Viết báo cáo kết quả thực hành và hướng mở rộng 14/11/2025 14/11/2025 AWS Journey 3. Chi tiết triển khai kỹ thuật 3.1 Amazon SageMaker (Custom ML) Notebook Instance: Khởi chạy một instance Jupyter Notebook ml.t2.medium. Huấn luyện: Sử dụng thuật toán tích hợp sẵn (như XGBoost) để huấn luyện mô hình trên tập dữ liệu mẫu (ví dụ: dự đoán giá nhà hoặc bộ dữ liệu MNIST). Triển khai: Triển khai mô hình đã huấn luyện tới một HTTPS Endpoint thời gian thực. Suy luận (Inference): Gọi endpoint bằng Python (Boto3) để tạo dự đoán từ dữ liệu mới. 3.2 Amazon Rekognition (Thị giác máy tính) Phân tích hình ảnh: Tải ảnh lên S3 và sử dụng API DetectLabels để nhận diện vật thể (ví dụ: \u0026ldquo;Xe hơi\u0026rdquo;, \u0026ldquo;Cây\u0026rdquo;, \u0026ldquo;Người\u0026rdquo;) với điểm tin cậy. Phân tích khuôn mặt: Sử dụng DetectFaces để ước tính độ tuổi, cảm xúc và giới tính. Tích hợp: Viết một script đơn giản kích hoạt hàm Lambda khi có ảnh được tải lên S3 để tự động gắn thẻ bằng Rekognition. 3.3 Amazon Comprehend (NLP) Phân tích cảm xúc: Xử lý văn bản đánh giá của khách hàng để xác định cảm xúc (Tích cực, Tiêu cực, Trung lập). Nhận diện thực thể: Trích xuất các thực thể cụ thể (Ngày tháng, Địa điểm, Tên riêng) từ các tài liệu văn bản phi cấu trúc. 3.4 Amazon Kendra (Tìm kiếm thông minh) Tạo chỉ mục: Tạo Kendra Index (Phiên bản Developer). Nguồn dữ liệu: Kết nối một S3 bucket chứa các file PDF hướng dẫn sử dụng làm cơ sở tri thức. Truy vấn: Thử nghiệm các truy vấn ngôn ngữ tự nhiên (ví dụ: \u0026ldquo;Làm thế nào để reset thiết bị?\u0026rdquo;) trong bảng điều khiển và nhận được câu trả lời chính xác được trích xuất từ tài liệu. 4. Thành tựu Đến cuối Tuần 10, các kết quả sau đã đạt được:\n✔ Thành công về mặt chức năng Thành công huấn luyện và lưu trữ một mô hình Học máy sử dụng Amazon SageMaker. Triển khai các tính năng Thị giác máy tính (Nhận diện khuôn mặt/Vật thể) thông qua lệnh gọi API. Trích xuất thông tin chi tiết từ văn bản bằng Amazon Comprehend. Thiết lập công cụ tìm kiếm tài liệu hoạt động tốt bằng Amazon Kendra. ✔ Phát triển kỹ năng Hiểu rõ sự phân biệt giữa Dịch vụ AI (API cấp cao cho nhà phát triển) và Dịch vụ ML (SageMaker cho Nhà khoa học dữ liệu). Có kinh nghiệm về chi phí Suy luận mô hình (Inference) và quản lý endpoint. Học cách tích hợp khả năng AI vào các ứng dụng hiện có bằng AWS SDK. 5. Thách thức gặp phải \u0026amp; Giải pháp Thách thức 1: Quản lý chi phí SageMaker\nVấn đề: SageMaker Notebooks và Endpoints tính phí theo giờ ngay cả khi nhàn rỗi. Giải pháp: Tạo quy trình \u0026ldquo;Dọn dẹp\u0026rdquo; để xóa Endpoints và Stop Notebook instances ngay sau khi hoàn thành bài lab để tránh hóa đơn ngoài ý muốn. Thách thức 2: Quyền IAM cho Rekognition\nVấn đề: Lỗi AccessDeniedException khi Rekognition cố gắng đọc ảnh từ S3 bucket. Giải pháp: Cập nhật Bucket Policy và IAM Role để cấp quyền s3:GetObject rõ ràng cho user/role đang gọi Rekognition API. Thách thức 3: Thời gian lập chỉ mục Kendra\nVấn đề: Kendra mất khá nhiều thời gian (30+ phút) để tạo chỉ mục và đồng bộ dữ liệu. Giải pháp: Hiểu rằng Kendra là dịch vụ cấp doanh nghiệp cần thời gian cấp phát; lên kế hoạch tác vụ để làm việc với Comprehend trong khi chờ Kendra khởi tạo. "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/vi/1-worklog/1.11-week11/",
	"title": "Worklog Tuần 11",
	"tags": [],
	"description": "",
	"content": "📘 Báo cáo công việc Tuần 11 – AWS Journey 1. Mục tiêu hàng tuần Tuần 11 tập trung vào Hiện đại hóa Ứng dụng thông qua Kiến trúc Serverless (Không máy chủ). Mục tiêu chính là chuyển đổi từ việc quản lý cơ sở hạ tầng (EC2) sang tập trung vào mã nguồn và logic nghiệp vụ bằng cách sử dụng các dịch vụ hướng sự kiện. Các mục tiêu chính bao gồm:\nMô hình Serverless – Hiểu sự chuyển dịch từ kiến trúc Nguyên khối (Monolithic) sang Vi dịch vụ (Microservices) và lợi ích của \u0026ldquo;No-Ops\u0026rdquo;. Bộ công cụ Serverless cốt lõi – Làm chủ AWS Lambda (Tính toán), API Gateway (Giao diện) và DynamoDB (Dữ liệu NoSQL). Bảo mật \u0026amp; Xác thực – Triển khai quản lý danh tính người dùng với Amazon Cognito. Cơ sở hạ tầng dưới dạng mã – Sử dụng AWS Serverless Application Model (SAM) để định nghĩa và triển khai tài nguyên serverless. Tuần này cung cấp bộ kỹ năng để xây dựng các ứng dụng có khả năng mở rộng cao, tiết kiệm chi phí và \u0026ldquo;thu nhỏ về 0\u0026rdquo; khi không sử dụng.\n2. Tóm tắt công việc chi tiết 🗂 Bảng hoạt động Ngày Nhiệm vụ Ngày bắt đầu Ngày hoàn thành Tham khảo Thứ Hai - Giới thiệu về khái niệm Hiện đại hóa và Serverless\n- So sánh kiến trúc Nguyên khối và Vi dịch vụ\n- Phân tích lợi ích: Chi phí, Khả năng mở rộng, Vận hành 17/11/2025 17/11/2025 AWS Journey Thứ Ba - Thực hành AWS Lambda: tạo hàm, cấu hình triggers (S3/APIGW)\n- Xem nhật ký log trong CloudWatch\n- Triển khai logic xử lý API cơ bản 18/11/2025 18/11/2025 AWS Journey Thứ Tư - Tích hợp API Gateway với Lambda (tạo REST API)\n- Kết nối dữ liệu với DynamoDB (Thao tác CRUD)\n- Test API sử dụng Postman 19/11/2025 19/11/2025 AWS Journey Thứ Năm - Cấu hình Cognito để xác thực người dùng (User Pool)\n- Tích hợp xác thực Cognito vào API Gateway\n- Quản lý quyền truy cập qua IAM Role 20/11/2025 20/11/2025 AWS Journey Thứ Sáu - Thực hành triển khai Serverless App hoàn chỉnh dùng AWS SAM\n- Kiểm thử, ghi nhật ký và tối ưu hóa hiệu năng\n- Tổng hợp kiến thức và báo cáo tuần 21/11/2025 21/11/2025 AWS Journey 3. Chi tiết triển khai kỹ thuật 3.1 AWS Lambda \u0026amp; Logic Runtime: Tạo hàm Lambda sử dụng Python 3.9. Handler: Triển khai hàm lambda_handler(event, context) để phân tích dữ liệu đầu vào JSON. Triggers: Cấu hình API Gateway làm nguồn sự kiện kích hoạt hàm. Logging: Sử dụng lệnh print() để gửi nhật ký có cấu trúc đến CloudWatch phục vụ gỡ lỗi. 3.2 Tích hợp API Gateway \u0026amp; DynamoDB Loại API: Xây dựng REST API. Tích hợp: Sử dụng Lambda Proxy Integration để chuyển toàn bộ đối tượng yêu cầu HTTP sang hàm xử lý. Cơ sở dữ liệu: Tạo bảng DynamoDB (Items) với ItemId làm Khóa phân vùng (Partition Key). Sử dụng thư viện boto3 trong Lambda để thực hiện các lệnh put_item, get_item và scan dựa trên phương thức HTTP (POST, GET). 3.3 Bảo mật với Amazon Cognito User Pool: Tạo User Pool để xử lý đăng ký và đăng nhập. App Client: Tạo client ID cho ứng dụng. Authorizer: Cấu hình Cognito User Pool Authorizer trong API Gateway. Xác thực: Xác minh rằng các yêu cầu API không có mã thông báo (token) Authorization (JWT) hợp lệ sẽ bị từ chối với lỗi 401 Unauthorized. 3.4 AWS SAM (Mô hình Ứng dụng Serverless) Template: Định nghĩa tài nguyên (Hàm, API, Bảng) trong tệp template.yaml. Build: Chạy sam build để biên dịch các gói phụ thuộc. Deploy: Thực hiện sam deploy --guided để đóng gói mã lên S3 và tự động tạo CloudFormation stack. Kiểm thử cục bộ: Sử dụng sam local invoke để kiểm tra hàm trên máy cá nhân trước khi triển khai. 4. Thành tựu Đến cuối Tuần 11, các kết quả sau đã đạt được:\n✔ Thành công về mặt chức năng Chuyển đổi thành công từ quản lý máy chủ sang triển khai các hàm (functions). Xây dựng một Serverless CRUD API hoạt động hoàn chỉnh. Bảo mật các endpoint API bằng JWT Tokens từ Cognito. Tự động hóa triển khai bằng AWS SAM, thay thế các thao tác thủ công trên console. ✔ Phát triển kỹ năng Hiểu thấu đáo mô hình Kiến trúc Hướng sự kiện (Event-Driven Architecture). Học cách xử lý các hạn chế của tính toán Phi trạng thái (Stateless). Làm chủ quy trình phân rã vấn đề thành các vi dịch vụ (Dịch vụ xác thực, Dịch vụ dữ liệu, Dịch vụ logic). 5. Thách thức gặp phải \u0026amp; Giải pháp Thách thức 1: Lỗi CORS\nVấn đề: Gọi API từ trình duyệt web (frontend) dẫn đến lỗi Chia sẻ tài nguyên chéo nguồn (CORS). Giải pháp: Bật CORS trong cài đặt API Gateway và đảm bảo hàm Lambda trả về header Access-Control-Allow-Origin: * trong đối tượng phản hồi. Thách thức 2: Quyền hạn Lambda\nVấn đề: Lỗi AccessDeniedException khi Lambda cố gắng ghi dữ liệu vào DynamoDB. Giải pháp: Cập nhật IAM Execution Role của Lambda để bao gồm quyền dynamodb:PutItem và dynamodb:GetItem cho ARN của bảng cụ thể. Thách thức 3: Khởi động lạnh (Cold Starts)\nVấn đề: Cuộc gọi API đầu tiên sau một thời gian không hoạt động mất 2-3 giây để phản hồi. Giải pháp: Chấp nhận đây là sự đánh đổi của Serverless. Tối ưu hóa các thư viện import trong mã Python để giảm thời gian khởi tạo. "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/vi/1-worklog/1.12-week12/",
	"title": "Worklog Tuần 12",
	"tags": [],
	"description": "",
	"content": "📘 Báo cáo công việc Tuần 12 – AWS Journey 1. Mục tiêu hàng tuần Tuần 12 đánh dấu sự kết thúc của lộ trình học tập AWS. Mục tiêu chính là tổng hợp và ứng dụng tất cả các kỹ năng đã học trong 11 tuần qua vào một Dự án Cuối khóa (Capstone Project) toàn diện. Các mục tiêu chính bao gồm:\nÔn tập tổng thể – Rà soát lại các dịch vụ cốt lõi (EC2, VPC, S3, RDS, Lambda) để đảm bảo hiểu sâu. Dự án Capstone – Thiết kế, xây dựng và triển khai một \u0026ldquo;Hệ thống Xử lý Đơn hàng E-Commerce\u0026rdquo; hoàn chỉnh từ con số không. Vận hành xuất sắc – Triển khai CI/CD, Giám sát và các phương pháp bảo mật tốt nhất. Tự đánh giá – Đánh giá kiến trúc dựa trên Well-Architected Framework và chuẩn bị cho kỳ thi chứng chỉ. Tuần này chuyển đổi vị thế từ \u0026ldquo;Người học\u0026rdquo; sang \u0026ldquo;Kỹ sư/Kiến trúc sư đám mây\u0026rdquo; sẵn sàng cho các thách thức thực tế.\n2. Tóm tắt công việc chi tiết 🗂 Bảng hoạt động Ngày Nhiệm vụ Ngày bắt đầu Ngày hoàn thành Tham khảo Thứ Hai - Ôn tập dịch vụ cốt lõi: EC2, S3, RDS, DynamoDB, IAM, VPC, Lambda, CloudFront\n- Xác định yêu cầu và kiến trúc cho dự án cuối khóa 24/11/2025 24/11/2025 AWS Journey Thứ Ba - Bắt đầu triển khai dự án:\n- Thiết kế VPC (Subnet Public/Private), Security Groups\n- Cấu hình S3 cho tài nguyên tĩnh, CloudFront cho CDN 25/11/2025 25/11/2025 AWS Journey Thứ Tư - Triển khai Backend:\n- Xây dựng Lambda functions và API Gateway\n- Kết nối Database (RDS/DynamoDB) và xử lý logic\n- Tích hợp CloudWatch Logs \u0026amp; Alarms 26/11/2025 26/11/2025 AWS Journey Thứ Năm - Hoàn thiện dự án:\n- Thêm xác thực Cognito (Đăng ký/Đăng nhập)\n- Hoàn tất pipeline CI/CD (CodePipeline/CodeBuild)\n- Kiểm thử hệ thống đầu cuối (End-to-end) 27/11/2025 27/11/2025 AWS Journey Thứ Sáu - Viết báo cáo và tài liệu cuối kỳ\n- Chuẩn bị bài thuyết trình (Sơ đồ kiến trúc, phân tích chi phí, bảo mật)\n- Tổng kết toàn bộ hành trình và tự đánh giá năng lực 28/11/2025 28/11/2025 AWS Journey 3. Chi tiết triển khai kỹ thuật (Dự án Capstone) 3.1 Phạm vi dự án: \u0026ldquo;Serverless E-Commerce Backend\u0026rdquo; Tổng quan kiến trúc: Ứng dụng web 3 tầng (3-tier) sử dụng công nghệ serverless. Frontend: Lưu trữ trên S3 + phân phối qua CloudFront. Backend: API Gateway + Lambda. Cơ sở dữ liệu: DynamoDB (Danh mục sản phẩm) + RDS MySQL (Lịch sử đơn hàng). Xác thực: Amazon Cognito. 3.2 Thiết lập cơ sở hạ tầng Thiết kế VPC: Tạo VPC tùy chỉnh với 2 Public Subnets (NAT Gateway, Load Balancer) và 2 Private Subnets (Lambda, RDS). Security Groups: Định nghĩa quy tắc nghiêm ngặt: RDS chỉ chấp nhận traffic từ Lambda SG trên cổng 3306. Lambda chỉ chấp nhận traffic từ các VPC endpoints nội bộ. 3.3 Logic ứng dụng \u0026amp; Dữ liệu Hàm Lambda: Phát triển các hàm Python cho CreateOrder, GetProducts, và ProcessPayment. Tích hợp Database: Sử dụng Boto3 để quét (scan) DynamoDB lấy thông tin sản phẩm. Sử dụng PyMySQL để thực hiện giao dịch SQL ghi đơn hàng vào RDS. Giám sát: Tạo CloudWatch Dashboard để trực quan hóa Độ trễ API và Tỷ lệ lỗi (4xx/5xx). 3.4 Tự động hóa \u0026amp; Vận hành CI/CD: Xây dựng đường ống bằng AWS CodePipeline: Nguồn (Source): GitHub. Build: AWS CodeBuild (Chạy unit tests). Deploy: CloudFormation/SAM deploy. Tối ưu chi phí: Thiết lập chính sách vòng đời S3 cho logs và cài đặt cảnh báo ngân sách (AWS Budget) cho dự án. 4. Thành tựu Đến cuối Tuần 12 (và toàn bộ khóa học), các kết quả sau đã đạt được:\n✔ Thành công dự án Hoàn thành và bàn giao một ứng dụng đám mây chuẩn production kết hợp hơn 10 dịch vụ AWS. Chứng minh khả năng tích hợp kho dữ liệu Quan hệ (RDS) và Phi quan hệ (DynamoDB) trong cùng một hệ thống. Đạt được kiến trúc Bảo mật (Cognito/IAM), Tin cậy (Multi-AZ) và Hiệu năng cao (CloudFront/Caching). ✔ Phát triển chuyên môn Làm chủ quy trình xây dựng ứng dụng đám mây từ Phân tích yêu cầu → Thiết kế → Triển khai → Vận hành. Phát triển kỹ năng khắc phục sự cố (troubleshooting) mạnh mẽ cho các hệ thống phân tán phức tạp. Hoàn thành lộ trình học tập và hoàn toàn sẵn sàng cho kỳ thi AWS Certified Solutions Architect – Associate. 5. Thách thức gặp phải \u0026amp; Giải pháp Thách thức 1: Kết nối Lambda trong VPC\nVấn đề: Hàm Lambda trong Private Subnet mất kết nối internet (không thể gọi API công khai hoặc DynamoDB). Giải pháp: Cấu hình NAT Gateway trong Public Subnet và cập nhật Route Tables. Đồng thời sử dụng VPC Endpoint cho DynamoDB để giữ lưu lượng nội bộ và giảm chi phí NAT. Thách thức 2: Lỗi Build trong CodePipeline\nVấn đề: File buildspec.yml thất bại do thiếu thư viện Python phụ thuộc. Giải pháp: Cập nhật pha install trong file buildspec để thực thi lệnh pip install -r requirements.txt. Thách thức 3: CORS với Cognito\nVấn đề: API Gateway từ chối các yêu cầu có token hợp lệ do thiếu header CORS trên phản hồi lỗi 401. Giải pháp: Cấu hình \u0026ldquo;Gateway Responses\u0026rdquo; trong API Gateway để bao gồm header CORS ngay cả đối với các lỗi Unauthorized. "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/vi/5-workshop/5.4-backed/5.4.3-api-secrets/",
	"title": "API Gateway &amp; Secrets",
	"tags": [],
	"description": "",
	"content": "Bước cuối cùng của phần Backend là mở cổng kết nối (API Gateway) và cấu hình bảo mật.\n1. Tạo API Gateway Trigger Mở lại Lambda Function ragsearch.\nỞ phần Function overview, bấm Add trigger. Chọn nguồn: API Gateway.\nIntent: Create a new API. API type: HTTP API. Security: IAM. Bấm Add.\n👉 Quan trọng: Sau khi tạo xong, hãy copy đường dẫn API Endpoint (có dạng https://...amazonaws.com) và lưu lại vào Notepad.\n2. Tạo Secrets Manager Tìm kiếm dịch vụ Secrets Manager -\u0026gt; Chọn Store a new secret. Secret type: Chọn Other type of secret. Key/value pairs: Key: JWT_SECRET Value: (Nhập một mật khẩu tự chọn bất kỳ) Bấm Next -\u0026gt; Đặt tên Secret (tùy ý) -\u0026gt; Next -\u0026gt; Store. 3. Cập nhật ARN cho Lambda Các Lambda function cần biết địa chỉ (ARN) của nhau để gọi lẫn nhau.\nMở 3 tab trình duyệt tương ứng với 3 Lambda functions (ragsearch, generate_contract, CallLLM). Copy Function ARN của từng hàm (Nằm ở góc trên bên phải phần Function overview). Quay lại tab cấu hình Environment variables của hàm ragsearch. Bấm Edit và thêm các biến sau: LAMBDA_RETRIEVAL_ARN: Dán ARN của hàm ragsearch. LAMBDA_REVIEW_ARN: Dán ARN của hàm CallLLM. LAMBDA_GENERATE_ARN: Dán ARN của hàm generate_contract. Bấm Save. "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/vi/3-blogstranslated/",
	"title": "Các bài blogs đã dịch",
	"tags": [],
	"description": "",
	"content": "Blog 1 - Tùy chỉnh mô hình, RAG, hay cả hai: Nghiên cứu điển hình với Amazon Nova Trong bối cảnh Generative AI phát triển mạnh mẽ, việc lựa chọn chiến lược tối ưu để nâng cao độ chính xác của mô hình ngôn ngữ lớn (LLM) cho các miền dữ liệu cụ thể là một thách thức lớn. Bài viết này thực hiện một nghiên cứu điển hình sử dụng các mô hình Amazon Nova để so sánh hiệu quả giữa ba phương pháp tiếp cận phổ biến: Tinh chỉnh mô hình (Fine-tuning), Thế hệ tăng cường truy xuất (RAG), và phương pháp Kết hợp (Hybrid). Thông qua các thử nghiệm thực tế trên bộ dữ liệu tài liệu AWS, bài viết phân tích sâu về hiệu suất, chi phí và độ phức tạp kỹ thuật, giúp các nhà phát triển và doanh nghiệp đưa ra quyết định sáng suốt nhất cho ứng dụng AI của mình.\nBlog 2 - Di chuyển ứng dụng CDK phiên bản 1 sang CDK phiên bản 2 với Amazon Q Developer Với việc AWS CDK phiên bản 1 chính thức ngừng hỗ trợ, các nhà phát triển đang đối mặt với nhu cầu cấp thiết phải nâng cấp lên phiên bản 2 để đảm bảo tính bảo mật và tận dụng các tính năng mới nhất. Bài viết này hướng dẫn chi tiết cách sử dụng Amazon Q Developer – trợ lý lập trình được hỗ trợ bởi AI – để tự động hóa và tăng tốc quá trình di chuyển này. Từ việc cập nhật các phần phụ thuộc (dependencies), sửa đổi câu lệnh nhập (imports) đến gỡ lỗi và tạo tài liệu, bạn sẽ khám phá cách Amazon Q giúp việc hiện đại hóa cơ sở hạ tầng dưới dạng mã (IaC) trở nên đơn giản, nhanh chóng và ít lỗi hơn.\nBlog 3 - Melting The Ice - Cách Natural Intelligence đơn giản hóa việc chuyển đổi Data lake sang Apache Iceberg Việc di chuyển một Data Lake quy mô lớn đang hoạt động sang định dạng bảng hiện đại như Apache Iceberg thường đi kèm với rủi ro gián đoạn hệ thống và phức tạp về mặt kỹ thuật. Bài viết này là câu chuyện thành công của Natural Intelligence trong việc chuyển đổi từ Apache Hive sang Apache Iceberg mà không có thời gian chết (zero downtime). Các tác giả chia sẻ chi tiết về chiến lược di chuyển \u0026ldquo;lai\u0026rdquo; (hybrid migration strategy) đầy sáng tạo, sử dụng cơ chế đồng bộ hóa thay đổi dữ liệu (CDC) và lược đồ tự động giữa hệ thống cũ và mới. Đây là tài liệu tham khảo quý giá về kiến trúc và quy trình cho các tổ chức muốn hiện đại hóa nền tảng dữ liệu lớn một cách an toàn và hiệu quả.\n"
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/vi/4-eventparticipated/4.3-event3/",
	"title": "Event 3",
	"tags": [],
	"description": "",
	"content": "AWS Cloud Mastery Series #1: AI/ML/GenAI on AWS Thời gian: Thứ Bảy, ngày 15/11/2025, 8:30 – 12:00 Địa điểm: AWS Vietnam Office, Tòa nhà Bitexco Financial Tower, Quận 1, TP.HCM Vai trò: Người tham dự (Learner)\nMục Đích Của Sự Kiện Có cái nhìn tổng quan về bức tranh AI/ML tại Việt Nam và hệ sinh thái các dịch vụ AI toàn diện của AWS. Đi sâu vào kỹ thuật (Deep-dive) với Amazon SageMaker để hiểu quy trình MLOps từ chuẩn bị dữ liệu đến triển khai mô hình. Nắm vững các công nghệ lõi của Generative AI trên Amazon Bedrock, bao gồm Prompt Engineering và RAG. Tìm hiểu về xu hướng chuyển dịch từ Chatbot sang Agentic AI. Danh Sách Diễn Giả Danh Hoàng Hiếu Nghị: Chuyên gia về Amazon Bedrock AgentCore và hệ sinh thái Agent. Đinh Lê Hoàng Anh: Demo các dịch vụ Pre-trained AI Services và ứng dụng thực tế (AMZ Photo). Lâm Tuấn Kiệt: Chuyên gia về Generative AI, kỹ thuật Prompt Engineering và kiến trúc RAG. Nội Dung Nổi Bật Sự kiện tập trung vào 3 mảng chính của AWS AI Stack:\nAI Services (Pre-trained):\nGiới thiệu các dịch vụ \u0026ldquo;mì ăn liền\u0026rdquo; như Amazon Rekognition (xử lý ảnh), Transcribe (giọng nói sang văn bản), Polly (văn bản sang giọng nói). Demo ứng dụng AMZ Photo: Quản lý ảnh thông minh, tích hợp tìm kiếm khuôn mặt và chatbot đa phương thức sử dụng framework Pipecat. Generative AI với Amazon Bedrock:\nSo sánh các Foundation Models (Claude, Llama, Titan). Các kỹ thuật tối ưu hóa Prompt: Zero-shot, Few-shot và Chain-of-Thought (CoT). Kiến trúc RAG (Retrieval-Augmented Generation): Kết hợp mô hình ngôn ngữ với dữ liệu doanh nghiệp thông qua Vector Database. Agentic AI \u0026amp; AgentCore:\nSự tiến hóa từ Generative AI Assistants sang Agentic AI (có khả năng tự thực thi tác vụ). Giới thiệu AgentCore: Các thành phần như Memory, Identity, Gateway và Code Interpreter giúp Agent vận hành ở quy mô lớn. Những Gì Học Được Tư duy về Prompt Engineering: Prompt không chỉ là câu lệnh đơn giản mà là một kỹ thuật lập trình ngôn ngữ tự nhiên. Việc sử dụng Chain-of-Thought giúp mô hình giải quyết các bài toán logic phức tạp tốt hơn nhiều so với cách hỏi thông thường. Quy trình RAG chuẩn: Hiểu rõ luồng dữ liệu (Data Ingestion -\u0026gt; Embeddings -\u0026gt; Vector Store -\u0026gt; Semantic Search) để giải quyết vấn đề \u0026ldquo;ảo giác\u0026rdquo; (hallucination) của AI. Sức mạnh của Pre-trained Services: Không nhất thiết phải xây dựng model từ đầu. Việc kết hợp các API có sẵn (như Rekognition, Textract) giúp giảm đáng kể thời gian đưa sản phẩm ra thị trường (Time-to-market). Kiến trúc Agent: Hiểu được sự khác biệt giữa một Chatbot thụ động và một Agent chủ động có khả năng sử dụng công cụ (Tools) và truy cập trình duyệt (Browser Viewer). Ứng Dụng Vào Công Việc Tối ưu hóa Chatbot nội bộ: Áp dụng ngay kiến trúc RAG và model Titan Text Embeddings V2 để nâng cấp chatbot hỗ trợ tra cứu tài liệu kỹ thuật của công ty, đảm bảo câu trả lời chính xác dựa trên nguồn dữ liệu thực tế. Tự động hóa xử lý tài liệu: Đề xuất sử dụng Amazon Textract kết hợp với Comprehend để trích xuất và phân loại thông tin từ các hóa đơn/hợp đồng giấy tờ, thay thế quy trình nhập liệu thủ công. Nghiên cứu Agentic Workflow: Thử nghiệm xây dựng một Agent đơn giản bằng LangGraph hoặc Amazon Bedrock Agents để tự động hóa quy trình tổng hợp tin tức thị trường hàng ngày cho team Business. Trải Nghiệm Cá Nhân Tham dự AWS Cloud Mastery Series tại văn phòng AWS Việt Nam là một trải nghiệm rất giá trị về mặt chuyên môn đối với em:\nPhần chia sẻ của anh Lâm Tuấn Kiệt rất thực tế, đặc biệt là các ví dụ minh họa về Prompt Engineering. Nó giúp em nhận ra những sai lầm trước đây khi tương tác với LLM và cách cải thiện chất lượng câu trả lời một cách khoa học. Demo của anh Đinh Lê Hoàng Anh với ứng dụng AMZ Photo rất ấn tượng. Việc tích hợp đa phương thức (Multimodal) và xử lý thời gian thực cho thấy tiềm năng to lớn của việc kết hợp các dịch vụ AWS lại với nhau. Chủ đề về Agentic AI của anh Danh Hoàng Hiếu Nghị mang tính định hướng cao. Dù kiến trúc AgentCore khá phức tạp, nhưng nó mở ra góc nhìn mới về việc AI không chỉ là công cụ hỗ trợ mà có thể trở thành \u0026ldquo;nhân viên ảo\u0026rdquo; thực thụ. Một số hình ảnh tại sự kiện Tổng kết: Một buổi workshop chuyên sâu, kết hợp cân bằng giữa lý thuyết nền tảng và demo thực chiến. Các kiến thức về RAG và Agentic AI sẽ là trọng tâm nghiên cứu của em trong thời gian tới.\n"
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/vi/5-workshop/5.3-infrastructure/",
	"title": "Thiết lập cơ sở hạ tầng",
	"tags": [],
	"description": "",
	"content": "Giới thiệu Trong mô-đun này, chúng ta sẽ bắt đầu xây dựng nền móng (Foundation) cho ứng dụng Smart Contract Assistant. Trước khi triển khai các đoạn mã xử lý logic (Lambda) hay giao diện người dùng (Frontend), chúng ta cần chuẩn bị nơi để lưu trữ dữ liệu.\nChúng ta sẽ thiết lập hai dịch vụ quan trọng của AWS:\nAmazon S3 (Simple Storage Service):\nĐóng vai trò là \u0026ldquo;Data Lake\u0026rdquo;. Nơi chứa các tài liệu hợp đồng mẫu (Templates). Nơi chứa dữ liệu văn bản luật đã được mã hóa (Embeddings) để phục vụ cho tính năng tìm kiếm thông minh (RAG). Amazon DynamoDB:\nĐóng vai trò là Cơ sở dữ liệu NoSQL hiệu năng cao. Lưu trữ hồ sơ người dùng, quản lý phiên đăng nhập. Lưu trữ lịch sử chat thời gian thực với độ trễ thấp. Danh sách các bước thực hiện Chúng ta sẽ lần lượt đi qua các nội dung sau:\nTạo Amazon S3 Bucket: Tạo bucket và cấu trúc thư mục, upload dữ liệu mẫu. Tạo bảng DynamoDB: Khởi tạo 3 bảng dữ liệu cần thiết (Users, ChatSessions, ChatMessages). Lưu ý quan trọng: Hãy đảm bảo bạn đang chọn Region Asia Pacific (Singapore) ap-southeast-1 trước khi bắt đầu tạo bất kỳ tài nguyên nào dưới đây.\n"
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/vi/4-eventparticipated/",
	"title": "Các events đã tham gia",
	"tags": [],
	"description": "",
	"content": " Trong phần này, các bạn cần liệt kê và mô tả chi tiết các sự kiện (event) mà mình đã tham gia trong suốt quá trình thực tập hoặc làm việc.\nMỗi sự kiện nên được trình bày theo định dạng Event 1, Event 2, Event 3…, kèm theo các thông tin:\nTên sự kiện Thời gian tổ chức Địa điểm (nếu có) Vai trò của bạn trong sự kiện (người tham dự, hỗ trợ tổ chức, diễn giả, v.v.) Mô tả ngắn gọn nội dung và hoạt động chính trong sự kiện Kết quả hoặc giá trị đạt được (bài học, kỹ năng mới, đóng góp cho nhóm/dự án) Việc liệt kê này giúp thể hiện rõ sự tham gia thực tế của bạn, cũng như các kỹ năng mềm và kinh nghiệm bạn đã tích lũy qua từng sự kiện. Trong quá trình thực tập, em đã tham gia 2 events, với mỗi event là một trải nghiệm đáng nhớ với những kiến thức mới, hay và bổ ích, cùng với đó là những món quà và những khoảnh khắc rất tuyệt vời.\nEvent 1 Tên sự kiện: Vietnam Cloud Day 2025 : Ho Chi Minh City Connect Edition for Builders\nThời gian: Thứ năm, ngày 18/09/2025, 9:00 – 17:30\nĐịa điểm: Tầng 36, tòa nhà Bitexco, số 02 đường Hải Triều, phường Sài Gòn, thành phố Hồ Chí Minh\nVai trò trong sự kiện: Người tham dự\nMô tả: Sự kiện công nghệ chuyên sâu dành cho cộng đồng Builder, cập nhật các xu hướng chiến lược như Agentic AI, giải pháp dữ liệu nền tảng (Data Foundation) và quy trình phát triển phần mềm định hướng AI (AI-Driven Development Lifecycle - AI-DLC).\nKết quả đạt được: Nắm bắt được tư duy và kiến trúc cốt lõi của Agentic AI; hiểu sâu về quy trình AI-DLC để chuyển đổi vai trò từ viết code sang kiến trúc và review; cập nhật kiến thức quan trọng về bảo mật đa lớp cho ứng dụng GenAI.\nEvent 2 Tên sự kiện: Workshop Data Science on AWS\nThời gian: 9:30 - 11:45, ngày 16/10/2025\nĐịa điểm: Hall A - Đại học FPT TP.HCM (FPTU HCMC)\nVai trò trong sự kiện: Người tham dự\nMô tả: Buổi workshop thực chiến về Khoa học dữ liệu trên nền tảng AWS, tập trung vào hệ sinh thái AI/ML stack, các dịch vụ AI (Vision, Speech, Text) và quy trình huấn luyện mô hình với Amazon SageMaker.\nKết quả đạt được: Hiểu rõ bức tranh toàn cảnh về AWS AI/ML Stack; biết cách tích hợp nhanh các AI Services (Rekognition, Polly, Lex) vào ứng dụng; nắm được quy trình Feature Engineering và cách tối ưu chi phí khi chạy mô hình trên Cloud.\nEvent 3 Tên sự kiện: AWS Cloud Mastery Series #1: AI/ML/GenAI on AWS\nThời gian: 8:30 – 12:00, Thứ Bảy, ngày 15/11/2025\nĐịa điểm: Văn phòng AWS Vietnam, Bitexco Financial Tower, TP.HCM\nVai trò trong sự kiện: Người tham dự\nMô tả: Chuỗi series chuyên sâu về Generative AI, tập trung vào kỹ thuật Prompt Engineering, kiến trúc RAG (Retrieval-Augmented Generation) và xây dựng AI Agents với Amazon Bedrock.\nKết quả đạt được: Hiểu sâu về sự khác biệt giữa các mô hình nền tảng (Claude, Llama, Titan); thành thạo kỹ thuật Chain-of-Thought prompting; nắm vững kiến trúc Bedrock Agent để xây dựng chatbot thông minh biết sử dụng công cụ (Tools).\nEvent 4 Tên sự kiện: AWS Cloud Mastery Series #2: DevOps on AWS\nThời gian: 8:30 – 17:00, Thứ Hai, ngày 17/11/2025\nĐịa điểm: Văn phòng AWS Vietnam, Bitexco Financial Tower, TP.HCM\nVai trò trong sự kiện: Người tham dự\nMô tả: Sự kiện chuyên sâu cả ngày về văn hóa và công cụ DevOps trên AWS. Nội dung bao gồm xây dựng quy trình CI/CD tự động (CodePipeline), quản lý hạ tầng bằng code (Infrastructure as Code - IaC) với AWS CDK, kỹ thuật Containerization (ECS/EKS) và giám sát hệ thống toàn diện (Observability).\nKết quả đạt được: Thay đổi tư duy quản trị hệ thống sang \u0026ldquo;Automation is King\u0026rdquo; (tự động hóa là vua); nắm vững bộ công cụ AWS Developer Tools để loại bỏ thao tác thủ công; biết cách dùng AWS CDK để định nghĩa hạ tầng bằng ngôn ngữ lập trình; hiểu rõ kiến trúc Microservices và quy trình triển khai an toàn (Blue/Green).\nEvent 5 Tên sự kiện: AWS Cloud Mastery Series #3: AWS Well-Architected Security Pillar\nThời gian: 8:30 – 12:00, Thứ Bảy, ngày 29/11/2025\nĐịa điểm: Văn phòng AWS Vietnam, Bitexco Financial Tower, TP.HCM\nVai trò trong sự kiện: Người tham dự\nMô tả: Buổi workshop tập trung hoàn toàn vào trụ cột Bảo mật (Security Pillar), bao gồm các nguyên tắc Zero Trust, quản lý định danh hiện đại (Modern IAM), phát hiện mối đe dọa bằng code (Detection-as-Code) và tự động hóa quy trình phản ứng sự cố (Automated Incident Response).\nKết quả đạt được: Hình thành tư duy \u0026ldquo;No ClickOps\u0026rdquo; trong bảo mật (mọi cấu hình phải là code); hiểu sâu về Mô hình Trách nhiệm chung (Shared Responsibility Model); biết cách sử dụng EventBridge và Lambda để tạo hệ thống tự động khắc phục lỗ hổng ngay khi phát hiện; tự tin hơn trong việc thiết kế kiến trúc mạng an toàn nhiều lớp.\n"
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/vi/4-eventparticipated/4.4-event4/",
	"title": "Event 4",
	"tags": [],
	"description": "",
	"content": "AWS Cloud Mastery Series #2: DevOps on AWS Thời gian: Thứ Hai, ngày 17/11/2025, 8:30 – 17:00 Địa điểm: AWS Vietnam Office, Tòa nhà Bitexco Financial Tower, Quận 1, TP.HCM Vai trò: Người tham dự (Learner)\nMục Đích Của Sự Kiện Chuyển đổi tư duy từ quản trị hệ thống truyền thống sang DevOps Mindset và văn hóa CI/CD. Làm chủ bộ công cụ AWS Developer Tools (CodeCommit, CodeBuild, CodeDeploy, CodePipeline) để tự động hóa quy trình phát triển. Tiếp cận kỹ thuật Infrastructure as Code (IaC) với CloudFormation và AWS CDK. Hiểu sâu về công nghệ Containerization (Docker, ECR, ECS/EKS) và cách vận hành Microservices trên AWS. Nắm vững kiến thức về Monitoring \u0026amp; Observability để giám sát hệ thống full-stack. Danh Sách Diễn Giả Đội ngũ chuyên gia kỹ thuật từ AWS (AWS Technical Team): Các Solutions Architect (SA) chuyên sâu về DevOps và Modern Application Development. Chuyên gia về Container và Serverless computing. Nội Dung Nổi Bật Sự kiện diễn ra cả ngày với lộ trình từ tư duy đến thực hành chi tiết:\nCI/CD Pipeline trên AWS:\nDemo luồng quy trình đầy đủ: Source Control (CodeCommit) -\u0026gt; Build \u0026amp; Test (CodeBuild) -\u0026gt; Deploy (CodeDeploy) -\u0026gt; Orchestration (CodePipeline). Các chiến lược triển khai an toàn như Blue/Green Deployment và Canary Release để giảm thiểu rủi ro downtime. Infrastructure as Code (IaC):\nSự khác biệt giữa việc click chuột trên Console (ClickOps) và viết code định nghĩa hạ tầng. Giới thiệu AWS CDK (Cloud Development Kit): Cho phép dùng ngôn ngữ lập trình quen thuộc (Python, TS, Java) để tạo hạ tầng thay vì viết YAML dài ngoằng trong CloudFormation. Container Services:\nKiến trúc tham chiếu cho Microservices sử dụng Amazon ECS và Fargate (Serverless compute for containers). Quy trình đóng gói Docker Image, lưu trữ trên ECR và scan bảo mật tự động. Observability:\nKhông chỉ là log và metric cơ bản, mà là khả năng \u0026ldquo;thấu hiểu\u0026rdquo; hệ thống qua Distributed Tracing với AWS X-Ray. Những Gì Học Được DevOps Metrics: Hiểu về các chỉ số DORA (Deployment Frequency, Lead Time for Changes\u0026hellip;) để đo lường hiệu quả của team. Chiến lược Git: Biết cách chọn mô hình phân nhánh phù hợp (GitFlow vs Trunk-based development) cho từng quy mô dự án. Container Workflow: Nắm vững vòng đời của một container từ lúc viết Dockerfile đến khi chạy trên môi trường Production với Auto Scaling. Drift Detection: Cách phát hiện và xử lý khi cấu hình thực tế trên Cloud bị lệch so với code IaC đã định nghĩa. Ứng Dụng Vào Công Việc Tự động hóa Build \u0026amp; Deploy: Đề xuất team chuyển từ deploy thủ công (copy paste file) sang sử dụng AWS CodePipeline kết hợp với Github Actions hiện tại để giảm lỗi con người. Triển khai IaC: Bắt đầu sử dụng AWS CDK cho các dự án mới để dễ dàng quản lý version của hạ tầng và tái sử dụng code (Constructs). Cải thiện giám sát: Tích hợp CloudWatch Alarms và X-Ray vào các service quan trọng để nhận cảnh báo sớm trước khi khách hàng phàn nàn về lỗi. Trải Nghiệm Cá Nhân Tham gia trọn vẹn một ngày về DevOps giúp em \u0026ldquo;vỡ\u0026rdquo; ra nhiều điều về quy trình làm phần mềm chuyên nghiệp:\nPhần Demo về CI/CD Pipeline thực sự \u0026ldquo;đã mắt\u0026rdquo;. Chỉ cần push code lên repo, hệ thống tự động chạy test, build docker image và update lên server mà không cần đụng tay vào. Em rất thích phần chia sẻ về AWS CDK. Trước đây nhìn file CloudFormation template dài cả nghìn dòng em rất ngại, nhưng với CDK thì việc định nghĩa VPC, Subnet, EC2 chỉ tốn vài dòng code Python ngắn gọn, logic như viết app vậy. Kiến trúc ECS được vẽ trên bảng (như trong ảnh chụp) giúp em hình dung rõ ràng luồng đi của request từ Internet Gateway -\u0026gt; ALB -\u0026gt; Target Group -\u0026gt; Container, và cách chia Public/Private subnet để bảo mật. Một số hình ảnh tại sự kiện Tổng kết: \u0026ldquo;Automation is King\u0026rdquo;. Sự kiện giúp em nhận ra rằng mọi thao tác lặp lại quá 2 lần đều nên được tự động hóa. DevOps không chỉ là tool, nó là văn hóa làm việc hiệu quả.\n"
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/vi/5-workshop/5.4-backed/",
	"title": "Xây dựng Backend (Lambda)",
	"tags": [],
	"description": "",
	"content": "Giới thiệu Trong phần này, chúng ta sẽ xây dựng phần lõi xử lý (Backend) cho ứng dụng Smart Contract Assistant. Hệ thống sử dụng kiến trúc Serverless với AWS Lambda để xử lý các yêu cầu từ người dùng và tương tác với AI (Amazon Bedrock).\nCác thành phần chính Chúng ta sẽ lần lượt thực hiện các bước sau:\nTạo Lambda RAG Search: Xây dựng hàm tìm kiếm thông minh, kết nối với dữ liệu vector trong S3. Tạo các Lambda phụ trợ: Xây dựng hàm tạo hợp đồng (generate_contract) và hàm chat tổng quát (CallLLM). Cấu hình API \u0026amp; Bảo mật: Thiết lập API Gateway để mở cổng kết nối và Secrets Manager để lưu trữ khóa bí mật. "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/vi/4-eventparticipated/4.5-event5/",
	"title": "Event 5",
	"tags": [],
	"description": "",
	"content": "AWS Cloud Mastery Series #3: AWS Well-Architected Security Pillar Thời gian: Thứ Bảy, ngày 29/11/2025, 8:30 – 12:00 Địa điểm: AWS Vietnam Office, Tòa nhà Bitexco Financial Tower, Quận 1, TP.HCM Vai trò: Người tham dự (Learner)\nMục Đích Của Sự Kiện Hiểu sâu về Security Pillar trong AWS Well-Architected Framework, chuyển dịch tư duy từ bảo mật vành đai (Perimeter Security) sang Zero Trust. Nắm vững kiến trúc Modern IAM để quản lý định danh và truy cập quy mô lớn. Học cách xây dựng hệ thống \u0026ldquo;miễn dịch\u0026rdquo; tự động với Detection-as-Code và Automated Remediation. Thực hành các chiến lược bảo vệ hạ tầng (Infrastructure Protection) và dữ liệu (Data Protection) trước các vector tấn công phổ biến. Danh Sách Diễn Giả Trần Đức Anh - Cloud Security Engineer Trainee, AWS Cloud Club Captain SGU. Nguyễn Tuấn Thành - Cloud Engineer Trainee. Nguyễn Đỗ Thành Đạt - Cloud Engineer Trainee. (Cùng đội ngũ kỹ thuật từ AWS Cloud Clubs và First Cloud Journey) Nội Dung Nổi Bật Buổi workshop đi sâu vào 5 trụ cột chính của bảo mật trên AWS:\nIdentity \u0026amp; Access Management (IAM):\nNguyên tắc \u0026ldquo;Kill long-lived credentials\u0026rdquo;: Thay thế Access Key lâu dài bằng IAM Roles và Temporary Tokens. Sử dụng IAM Access Analyzer để phát hiện các policy quá lỏng lẻo (như Principal: *). Demo cấu hình SSO (Single Sign-On) để quản lý truy cập tập trung cho nhiều tài khoản AWS. Detection \u0026amp; Continuous Monitoring:\nTriển khai Detection-as-Code: Dùng CloudFormation/Terraform để bật GuardDuty và Security Hub trên toàn bộ Organization thay vì click tay. Runtime Monitoring: GuardDuty Agent cài sâu vào EC2/EKS để phát hiện các tiến trình lạ (process injection) hoặc file access bất thường. Infrastructure Protection:\nChiến lược Defense in Depth (Phòng thủ chiều sâu): Kết hợp WAF, Shield ở Edge -\u0026gt; Network Firewall ở VPC -\u0026gt; Security Group ở Instance. Phân biệt rõ ràng vai trò của Security Group (Stateful) và NACL (Stateless) trong việc chống lại các cuộc tấn công DDoS hay Lateral Movement. Incident Response (IR):\nTự động hóa quy trình phản ứng sự cố: CloudTrail log -\u0026gt; EventBridge filter -\u0026gt; Lambda function tự động cách ly EC2 bị nhiễm malware hoặc xoay (rotate) credential bị lộ. Những Gì Học Được Tư duy \u0026ldquo;No ClickOps\u0026rdquo;: Trong bảo mật, việc cấu hình thủ công là kẻ thù. Mọi rule bảo mật, từ Security Group đến GuardDuty detector, đều phải được định nghĩa bằng Code (IaC) và version control. Shared Responsibility Model: Hiểu rõ phần nào AWS lo (Security of the Cloud) và phần nào mình phải lo (Security in the Cloud), đặc biệt là việc mã hóa dữ liệu (Encryption at rest/in transit). Sức mạnh của EventBridge: Nó không chỉ là message bus mà là \u0026ldquo;hệ thần kinh trung ương\u0026rdquo; giúp kết nối việc phát hiện (Detection) với hành động khắc phục (Remediation) trong thời gian thực. Micro-segmentation: Không bao giờ tin tưởng mạng nội bộ. Việc chia nhỏ VPC thành các subnet public/private và kẹp chặt bằng Security Group là bắt buộc. Ứng Dụng Vào Công Việc Rà soát IAM ngay lập tức: Kiểm tra lại toàn bộ IAM User trong dự án công ty, xóa các Access Key không dùng, và bật MFA cho 100% tài khoản, đặc biệt là tài khoản Root. Triển khai GuardDuty: Đề xuất bật GuardDuty ở mức Organization để có ngay khả năng phát hiện các mối đe dọa cơ bản (như đào coin, scan port) mà không tốn nhiều công sức cấu hình. Secret Management: Dừng ngay việc lưu hardcode password trong code hay file .env. Chuyển sang dùng AWS Secrets Manager và thiết lập tự động xoay vòng (Automatic Rotation) cho DB credentials. Trải Nghiệm Cá Nhân Thực sự là một buổi sáng \u0026ldquo;căng não\u0026rdquo; nhưng cực kỳ chất lượng. Dù diễn giả là các bạn Trainee/Student nhưng kiến thức rất sâu và thực chiến:\nEm rất ấn tượng với slide \u0026ldquo;Prevention - Nobody has time for incidents\u0026rdquo;. Câu nói \u0026ldquo;Public buckets = your data on the evening news\u0026rdquo; nghe vừa hài hước vừa thấm thía về tầm quan trọng của việc chặn S3 public access. Phần demo về Automated Remediation làm em mở mang tầm mắt. Trước giờ cứ nghĩ Incident Response là phải có người ngồi trực monitor, nhưng hóa ra có thể viết Lambda để tự động \u0026ldquo;xử đẹp\u0026rdquo; các mối nguy ngay khi nó vừa xuất hiện. Slide về Network Attack Vectors vẽ rất trực quan đường đi của Hacker từ Internet qua các lớp bảo vệ, giúp em hình dung rõ hơn về kiến trúc mạng an toàn. Một số hình ảnh tại sự kiện Tổng kết: Bảo mật không phải là rào cản, mà là yếu tố then chốt để doanh nghiệp đi nhanh và an toàn hơn. Sự kiện giúp em tự tin hơn hẳn khi đề xuất các giải pháp security cho dự án sắp tới.\n"
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/vi/5-workshop/5.5-fullstack/",
	"title": "Triển khai Fullstack",
	"tags": [],
	"description": "",
	"content": "Giới thiệu Chúng ta đã có các thành phần rời rạc: S3, DynamoDB, Lambda Functions. Bây giờ là lúc \u0026ldquo;lắp ráp\u0026rdquo; chúng lại thành một ứng dụng hoàn chỉnh.\nTrong phần này, chúng ta sẽ thực hiện:\nDeploy Backend Code: Sử dụng Serverless Framework để tự động cấu hình các kết nối, tạo User Pool (Cognito) để quản lý đăng nhập và cập nhật code backend. Deploy Frontend (Amplify): Đưa mã nguồn giao diện web lên GitLab, kết nối với AWS Amplify để build và hosting website. Sau khi hoàn thành phần này, bạn sẽ có một đường link website hoạt động thực tế.\n"
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/vi/5-workshop/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": "Xây dựng Trợ lý Hợp đồng Thông minh (Smart Contract Assistant) trên AWS Giới thiệu Nền tảng AI Contract Intelligence là một dịch vụ web dành cho cá nhân và các nhóm người dùng nhỏ (freelancer, chủ doanh nghiệp nhỏ, nhân sự hành chính/pháp lý) làm việc với hợp đồng hằng ngày nhưng không có chuyên môn pháp lý sâu. Giải pháp sử dụng Amazon Bedrock và kiến trúc AWS serverless hoàn toàn để phân tích hợp đồng, làm nổi bật rủi ro, gợi ý chỉnh sửa điều khoản, và tạo tóm tắt cũng như mẫu hợp đồng mới.\nĐược xây dựng trên AWS Amplify, Lambda, API Gateway, DynamoDB, S3, Cognito, EventBridge và CloudWatch, nền tảng cung cấp khả năng rà soát hợp đồng bằng AI với độ trễ thấp, chi phí thấp và bảo mật cao, được tối ưu cho người dùng đơn lẻ hoặc các nhóm nhỏ mà không cần tính năng phức tạp như hệ thống doanh nghiệp.\nMục tiêu chính của ứng dụng là hỗ trợ người dùng (như luật sư, nhân viên pháp chế hoặc chủ doanh nghiệp) thực hiện các tác vụ phức tạp như:\nTra cứu thông tin: Hỏi đáp về các điều khoản luật dựa trên kho dữ liệu văn bản pháp lý có sẵn. Soạn thảo tự động: Yêu cầu AI tạo ra các bản nháp hợp đồng dựa trên các template mẫu và thông tin cung cấp. Phân tích: Tóm tắt và kiểm tra nội dung hợp đồng. Kiến trúc giải pháp Giải pháp được xây dựng hoàn toàn trên kiến trúc Serverless (Không máy chủ) của AWS, giúp tối ưu hóa chi phí vận hành và khả năng mở rộng. Điểm nhấn của kiến trúc là việc áp dụng kỹ thuật RAG (Retrieval-Augmented Generation), cho phép AI truy xuất thông tin chính xác từ kho dữ liệu riêng của bạn thay vì chỉ dựa vào kiến thức đã được huấn luyện trước.\nCác thành phần chính của hệ thống bao gồm:\nAI \u0026amp; LLM (Amazon Bedrock):\nĐây là \u0026ldquo;trái tim\u0026rdquo; của ứng dụng. Chúng ta sử dụng Amazon Bedrock để truy cập các mô hình ngôn ngữ lớn (LLM) tiên tiến như Claude 3 (Haiku/Sonnet) thông qua API. Bedrock chịu trách nhiệm hiểu ngữ cảnh câu hỏi, tổng hợp thông tin từ tài liệu và sinh ra câu trả lời tự nhiên. Backend \u0026amp; Computing (AWS Lambda \u0026amp; API Gateway):\nSử dụng AWS Lambda để chạy các đoạn code Python xử lý logic nghiệp vụ (tìm kiếm vector, gọi API Bedrock, xử lý dữ liệu đầu vào) mà không cần quản lý máy chủ. Amazon API Gateway đóng vai trò là cửa ngõ, tiếp nhận các yêu cầu từ phía người dùng (Frontend) và định tuyến chúng đến các hàm Lambda tương ứng. Database \u0026amp; Storage (Amazon S3 \u0026amp; DynamoDB):\nAmazon S3: Đóng vai trò là kho lưu trữ (\u0026ldquo;Data Lake\u0026rdquo;) chứa các file hợp đồng mẫu (.docx, .pdf), các file metadata và dữ liệu vector đã được nhúng (embeddings) phục vụ cho việc tìm kiếm ngữ nghĩa. Amazon DynamoDB: Cơ sở dữ liệu NoSQL hiệu năng cao dùng để lưu trữ thông tin người dùng, quản lý phiên làm việc (Sessions) và lịch sử tin nhắn chat, đảm bảo độ trễ thấp nhất khi truy xuất. Frontend \u0026amp; Authentication (AWS Amplify \u0026amp; Cognito):\nGiao diện người dùng được xây dựng bằng React/VueJS và được deploy tự động thông qua AWS Amplify. Amazon Cognito cung cấp cơ chế đăng ký, đăng nhập và bảo mật, đảm bảo chỉ những người dùng được ủy quyền mới có thể truy cập ứng dụng. Mục tiêu học tập Sau khi hoàn thành workshop này, bạn sẽ nắm vững:\nCách triển khai một ứng dụng Full-stack trên AWS từ con số 0. Hiểu và ứng dụng mô hình RAG để kết hợp dữ liệu doanh nghiệp với sức mạnh của LLM. Kỹ năng làm việc với Infrastructure as Code thông qua Serverless Framework. Cấu hình và tích hợp các dịch vụ AWS cốt lõi: S3, DynamoDB, Lambda, API Gateway và Bedrock. Nội dung thực hành Tổng quan về workshop Các bước chuẩn bị Thiết lập cơ sở hạ tầng (S3, DynamoDB) Xây dựng Backend (Lambda, IAM) Triển khai Fullstack (Amplify) Dọn dẹp tài nguyên "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/vi/5-workshop/5.6-cleanup/",
	"title": "Dọn dẹp tài nguyên",
	"tags": [],
	"description": "",
	"content": "0. Sao lưu dữ liệu (Tùy chọn) Nếu bạn muốn giữ lại dữ liệu trước khi xóa, hãy thực hiện bước này. Nếu không, hãy bỏ qua và xuống bước 1.\nDynamoDB: Vào từng bảng (ChatMessages, ChatSessions, Users) -\u0026gt; Tab Explore items -\u0026gt; Chọn Scan -\u0026gt; Chọn tất cả -\u0026gt; Export to CSV. S3: Vào Bucket -\u0026gt; Chọn tất cả file -\u0026gt; Actions -\u0026gt; Download. 1. Xóa API Gateway Tại sao phải xóa trước? API Gateway đang kết nối với Lambda. Xóa nó trước giúp ngắt các kết nối phụ thuộc.\nTruy cập API Gateway Console.\nTìm và xóa lần lượt 2 API sau:\nragsearch-API (API tạo thủ công). dev-ai-contract-backend (API do Serverless tạo). Chọn API -\u0026gt; Bấm Delete -\u0026gt; Xác nhận xóa.\n2. Xóa Lambda Functions Truy cập Lambda Console. Bạn sẽ thấy danh sách khoảng 6 hàm (3 hàm tạo tay và 3 hàm do Serverless tạo).\nTích chọn ô checkbox bên cạnh tất cả các hàm sau:\nragsearch CallLLM generate_contract ai-contract-backend-dev-api ai-contract-backend-dev-postConfirmation ai-contract-backend-dev-custom-resource-existing-cup Bấm nút Actions -\u0026gt; Chọn Delete.\nXác nhận xóa. 3. Xóa Cognito User Pool Truy cập Cognito Console -\u0026gt; User pools. Bấm vào tên pool: ai-contract-backend-user-pool-dev. Bấm Delete. Chọn phương thức xác thực xóa (thường là gõ tên pool) và xác nhận. 4. Xóa DynamoDB Tables Truy cập DynamoDB Console -\u0026gt; Tables. Tích chọn 3 bảng: ChatMessages ChatSessions Users Bấm Delete. Lưu ý: Bỏ tích ô Create a backup (để tránh tốn thời gian và chi phí lưu backup) -\u0026gt; Gõ confirm để xác nhận xóa. 5. Xóa S3 Buckets Lưu ý: AWS yêu cầu Bucket phải Trống (Empty) thì mới cho phép Xóa (Delete).\nThực hiện quy trình sau cho cả 2 bucket (Bucket chính contract-app-demo... và bucket deployment ai-contract-backend-dev-serverless...):\nLàm trống Bucket:\nChọn Bucket -\u0026gt; Bấm Empty. Gõ permanently delete để xác nhận -\u0026gt; Bấm Empty. Bấm Exit để quay lại. Xóa Bucket:\nKhi bucket đã trống, tích chọn Bucket đó -\u0026gt; Bấm Delete. Gõ tên bucket để xác nhận -\u0026gt; Bấm Delete bucket. 6. Xóa Secrets Manager Truy cập Secrets Manager Console. Bấm vào tên secret bạn đã tạo (ví dụ: JWT_SECRET\u0026hellip;). Bấm Actions -\u0026gt; Delete secret. Đặt thời gian chờ (Waiting period) là tối thiểu (7 ngày) -\u0026gt; Xác nhận xóa. 7. Xóa Amplify App Truy cập AWS Amplify. Chọn App SmartContractAssistant (hoặc tên bạn đã đặt). Ở menu bên trái, chọn App settings -\u0026gt; General settings. Bấm nút Delete app ở góc phải -\u0026gt; Xác nhận. Tổng kết Xin chúc mừng! Bạn đã hoàn thành workshop và dọn dẹp sạch sẽ tài nguyên.\nDanh sách tài nguyên đã xóa:\n✅ 2 API Gateway ✅ 6 Lambda Functions ✅ 1 Cognito User Pool ✅ 3 DynamoDB Tables ✅ 2 S3 Buckets ✅ 1 Secrets Manager Secret ✅ 1 Amplify App "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/vi/6-self-evaluation/",
	"title": "Tự đánh giá",
	"tags": [],
	"description": "",
	"content": "Trong suốt thời gian thực tập tại AWS First Cloud Journey từ 04/09/2025 đến 09/12/2025, em đã có cơ hội học hỏi, rèn luyện và áp dụng kiến thức đã được trang bị tại trường vào môi trường làm việc thực tế.\nEm đã trực tiếp tham gia xây dựng và triển khai kiến trúc Serverless cho dự án Agreeme. Qua quá trình này, em đã tích lũy được những kinh nghiệm cụ thể:\nVề kỹ thuật: Nắm vững cách vận hành và kết hợp các dịch vụ AWS cốt lõi (AWS Lambda, API Gateway, DynamoDB), thực hành tư duy Infrastructure as Code (IaC) để quản lý hạ tầng và xây dựng quy trình CI/CD tự động hóa việc kiểm thử, triển khai. Về tư duy: Hình thành tư duy thiết kế hệ thống có khả năng mở rộng (Scalability) và bước đầu tiếp cận bài toán tối ưu hóa chi phí (Cost Optimization) trên Cloud. Về kỹ năng mềm: Rèn luyện kỹ năng làm việc nhóm (Teamwork) hiệu quả trong môi trường Agile/Scrum, kỹ năng quản lý task và báo cáo tiến độ minh bạch. Về tác phong, em luôn cố gắng hoàn thành tốt nhiệm vụ, tuân thủ nội quy, và tích cực trao đổi với đồng nghiệp để nâng cao hiệu quả công việc.\nĐể phản ánh một cách khách quan quá trình thực tập, em xin tự đánh giá bản thân dựa trên các tiêu chí dưới đây:\nSTT Tiêu chí Mô tả Tốt Khá Trung bình 1 Kiến thức và kỹ năng chuyên môn Hiểu biết về ngành, áp dụng kiến thức vào thực tế, kỹ năng sử dụng công cụ, chất lượng công việc ☐ ✅ ☐ 2 Khả năng học hỏi Tiếp thu kiến thức mới, học hỏi nhanh ✅ ☐ ☐ 3 Chủ động Tự tìm hiểu, nhận nhiệm vụ mà không chờ chỉ dẫn ✅ ☐ ☐ 4 Tinh thần trách nhiệm Hoàn thành công việc đúng hạn, đảm bảo chất lượng ☐ ✅ ☐ 5 Kỷ luật Tuân thủ giờ giấc, nội quy, quy trình làm việc ✅ ☐ ☐ 6 Tính cầu tiến Sẵn sàng nhận feedback và cải thiện bản thân ✅ ☐ ☐ 7 Giao tiếp Trình bày ý tưởng, báo cáo công việc rõ ràng ✅ ☐ ☐ 8 Hợp tác nhóm Làm việc hiệu quả với đồng nghiệp, tham gia nhóm ✅ ☐ ☐ 9 Ứng xử chuyên nghiệp Tôn trọng đồng nghiệp, đối tác, môi trường làm việc ✅ ☐ ☐ 10 Tư duy giải quyết vấn đề Nhận diện vấn đề, đề xuất giải pháp, sáng tạo ☐ ✅ ☐ 11 Đóng góp vào dự án/tổ chức Hiệu quả công việc, sáng kiến cải tiến, ghi nhận từ team ✅ ☐ ☐ 12 Tổng thể Đánh giá chung về toàn bộ quá trình thực tập ✅ ☐ ☐ Cần cải thiện Để phát triển thành một kỹ sư chuyên nghiệp hơn, em nhận thấy cần tập trung cải thiện các nhóm kỹ năng sau:\n1. Về chuyên môn kỹ thuật (Technical Depth) Tư duy tối ưu hóa (Optimization Mindset): Không chỉ dừng lại ở việc code chạy được, cần chú trọng hơn đến việc tối ưu hiệu năng (Performance) và tối ưu chi phí (Cost optimization) cho các dịch vụ AWS. Kiến thức bảo mật (Security Awareness): Nắm vững và áp dụng chặt chẽ hơn các nguyên tắc bảo mật (IAM roles, Security Groups) ngay từ giai đoạn phát triển đầu tiên. 2. Về quy trình làm việc (Process \u0026amp; Discipline) Kỷ luật trong tài liệu hóa (Documentation): Cần duy trì thói quen viết tài liệu kỹ thuật (Technical Documentation) và comment code rõ ràng, đầy đủ để thuận tiện cho việc bảo trì. Quản lý thời gian: Cải thiện khả năng ước lượng thời gian (Estimation) cho các task phức tạp để đảm bảo deadline chính xác hơn. 3. Về kỹ năng mềm (Soft Skills) Giao tiếp chủ động: Cần chủ động báo cáo các khó khăn (blockers) sớm hơn thay vì cố gắng tự giải quyết quá lâu. Tiếng Anh chuyên ngành: Nâng cao khả năng đọc hiểu tài liệu chuyên sâu và giao tiếp kỹ thuật bằng tiếng Anh. "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/vi/7-feedback/",
	"title": "Chia sẻ, đóng góp ý kiến",
	"tags": [],
	"description": "",
	"content": " Môi trường làm việc Môi trường làm việc rất thân thiện và cởi mở. Các thành viên trong FCJ luôn sẵn sàng hỗ trợ khi mình gặp khó khăn, kể cả ngoài giờ làm việc. Không gian làm việc gọn gàng, thoải mái, giúp mình tập trung tốt hơn.\nSự hỗ trợ của mentor / team admin Mentor hướng dẫn rất chi tiết, giải thích rõ ràng khi mình chưa hiểu và luôn khuyến khích mình đặt câu hỏi.\nSự phù hợp giữa công việc và chuyên ngành học Công việc mình được giao phù hợp với kiến thức mình đã học ở trường, đồng thời mở rộng thêm những mảng mới mà mình chưa từng được tiếp cận. Nhờ vậy, mình vừa củng cố kiến thức nền tảng, vừa học thêm kỹ năng thực tế liên quan đến DevOps, Cloud.\nCơ hội học hỏi \u0026amp; phát triển kỹ năng Trong quá trình thực tập, mình học được nhiều kỹ năng mới như sử dụng công cụ quản lý dự án, kỹ năng làm việc nhóm, và cả cách giao tiếp chuyên nghiệp trong môi trường công ty. Mentor cũng chia sẻ nhiều kinh nghiệm thực tế giúp mình định hướng tốt hơn cho sự nghiệp.\nVăn hóa \u0026amp; tinh thần đồng đội Văn hóa công ty rất tích cực: mọi người tôn trọng lẫn nhau, làm việc nghiêm túc nhưng vẫn vui vẻ. Khi có dự án gấp, mọi người cùng nhau cố gắng, hỗ trợ không phân biệt vị trí. Điều này giúp mình cảm thấy mình là một phần của tập thể, dù chỉ là thực tập sinh.\nChính sách / phúc lợi cho thực tập sinh Công ty có hỗ trợ phụ cấp thực tập và tạo điều kiện về thời gian linh hoạt khi cần thiết. Ngoài ra, việc được tham gia các buổi đào tạo nội bộ là một điểm cộng lớn.\n"
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/vi/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/vi/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]
[
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/3-blogstranslated/3.1-blog1/",
	"title": "Blog 1",
	"tags": [],
	"description": "",
	"content": "Model Customization, RAG, or Both: A Case Study with Amazon Nova by Flora Wang, Anila Joshi, Baishali Chaudhury, Sungmin Hong, Jae Oh Woo, and Rahul Ghosh on 10 APR 2025 in Advanced (300), Amazon Bedrock, Amazon Machine Learning, Amazon Nova, Amazon SageMaker, Generative AI, Technical How-to\nAs enterprises and developers increasingly seek to optimize their language models for specific tasks, the decision between model customization and Retrieval Augmented Generation (RAG) becomes critical. In this post, we aim to address this growing need by providing clear, actionable guidance and best practices on when to use each approach, helping you make informed decisions tailored to your unique requirements and goals.\nThe introduction of Amazon Nova models represents a significant advancement in the AI landscape, offering new opportunities for Large Language Model (LLM) optimization. In this post, we demonstrate how to effectively implement model customization and RAG using Amazon Nova models as a base. We conducted a comprehensive comparative study between model customization and RAG using the latest Amazon Nova models and share these valuable insights.\nApproach and Foundation Model Overview In this section, we discuss the differences between fine-tuning and RAG, present common use cases for each approach, and provide an overview of the foundation model used for our experiments.\nDemystifying RAG and Model Customization RAG is a technique to enhance the capabilities of pre-trained models by allowing them to access external, domain-specific data sources. It combines two components: external knowledge retrieval and response generation. This allows pre-trained language models to dynamically incorporate external data during the response generation process, enabling more contextually accurate and updated outputs. Unlike fine-tuning, in RAG, the model does not undergo any training, and model weights are not updated to learn domain knowledge. While fine-tuning implicitly uses domain-specific information by embedding necessary knowledge directly into the model, RAG explicitly utilizes domain-specific information through external retrieval.\nModel customization refers to adjusting a pre-trained language model to better suit specific tasks, domains, or datasets. Fine-tuning is one such technique that helps introduce task-specific or domain-specific knowledge to improve model performance. It adjusts model parameters to better align with the nuances of the target task while leveraging its general knowledge.\nCommon Use Cases for Each Approach RAG is optimal for use cases requiring dynamic or frequently updated data (such as customer support FAQs and e-commerce catalogs), domain-specific insights (such as legal or medical Q\u0026amp;A), scalable solutions for broad applications (such as software-as-a-service (SaaS) platforms), multimodal data retrieval (such as document summarization), and strict adherence to secure or sensitive data (such as financial and regulatory systems).\nIn contrast, fine-tuning thrives in scenarios requiring precise customization (such as personalized chatbots or creative writing), high accuracy for narrow tasks (such as code generation or specialized summarization), ultra-low latency (such as real-time customer interactions), stability with static datasets (such as domain-specific glossaries), and cost-effective scaling for high-volume tasks (such as call center automation).\nWhile RAG excels in real-time grounding in external data and fine-tuning specializes in static, structured, and personalized workflows, the choice between them often depends on nuanced factors. This post provides a comprehensive comparison of RAG and fine-tuning, clarifying the strengths, limitations, and contexts where each approach delivers the best performance.\nIntroduction to Amazon Nova Models Amazon Nova is a new generation of foundation models (FMs) providing state-of-the-art intelligence and industry-leading price performance. Amazon Nova Pro and Amazon Nova Lite are multimodal models excelling in accuracy and speed, with Amazon Nova Lite optimized for fast processing and low cost. Amazon Nova Micro focuses on ultra-low latency text tasks. They provide fast inference, support agentic workflows with Amazon Bedrock Knowledge Bases and RAG, and enable fine-tuning on text and multimodal data. Optimized for cost-efficient performance, they are trained on data in over 200 languages.\nSolution Overview To evaluate the effectiveness of RAG versus model customization, we designed a comprehensive experimental framework using a set of AWS-specific questions. Our study utilized Amazon Nova Micro and Amazon Nova Lite as the base FMs and examined their performance across different configurations.\nWe structured our evaluation as follows:\nBase Model: Used Amazon Nova Micro and Amazon Nova Lite. Generated answers for AWS-specific questions without additional context. Base Model with RAG: Connected base models to Amazon Bedrock Knowledge Bases. Provided access to relevant AWS documentation and blogs. Model Customization: Fine-tuned both Amazon Nova models using 1,000 AWS-specific question-answer pairs generated from the same set of AWS articles. Deployed customized models via provisioned throughput. Generated answers for AWS-specific questions with the fine-tuned models. Hybrid Approach (Model Customization + RAG): Connected fine-tuned models to Amazon Bedrock Knowledge Bases. Provided fine-tuned models access to relevant AWS articles at inference time. In the following sections, we guide you through setting up the second and third approaches (base model with RAG and model customization with fine-tuning) in Amazon Bedrock.\nPrerequisites To follow this post, you need:\nAn AWS account and appropriate permissions. An Amazon Simple Storage Service (Amazon S3) bucket with two folders: one for your training data and one for your model output and training metrics. Implementing RAG with Base Amazon Nova Model In this section, we walk through the steps to implement RAG with a base model. To do so, we create a knowledge base. Complete the following steps:\nOn the Amazon Bedrock console, select Knowledge Bases in the navigation pane. In Knowledge Bases, select Create. On the Configure data source page, provide the following: Specify the Amazon S3 location of the documents. Specify the chunking strategy. Select Next. On the Select embedding model and configure vector store page, provide the following: In the Embedding model section, select the embedding model used to embed the chunks. In the Vector database section, create a new vector store or use an existing one where embeddings will be stored for retrieval. Select Next. On the Review and create page, review the settings and select Create knowledge base. Fine-tuning Amazon Nova Models via Amazon Bedrock API In this section, we provide detailed instructions on how to fine-tune and host custom Amazon Nova models using Amazon Bedrock. The following diagram illustrates the solution architecture.\nCreating a Fine-Tuning Job Fine-tuning Amazon Nova models through the Amazon Bedrock API is a streamlined process:\nOn the Amazon Bedrock console, select us-east-1 as your AWS Region. At the time of writing, Amazon Nova fine-tuning is only available in us-east-1. Select Custom Models under Foundation Models in the navigation pane. Under Customize models, select Create fine-tuning job. For Source model, select Select model. Select Amazon as the provider and choose your preferred Amazon Nova model. Select Apply. For Fine-tuned model name, enter a unique name for the fine-tuned model. For Job name, enter a name for the fine-tuning job. In Input data, enter the S3 bucket locations for source (training data) and destination (model output and training metrics) and optionally your validation dataset location. Configuring Hyperparameters For Amazon Nova models, you can customize the following hyperparameters:\nParameter Range/Constraints Epochs 1–5 Batch Size Fixed at 1 Learning Rate 0.000001–0.0001 Learning Rate Warmup Steps 0–100 Preparing Dataset for Compatibility with Amazon Nova Models Similar to other LLMs, Amazon Nova requires prompt-completion pairs, also known as Q\u0026amp;A pairs, for Supervised Fine-Tuning (SFT). This dataset must contain the ideal results you want the language model to generate for specific tasks or prompts. Refer to the Amazon Nova Data Preparation Guide for best practices and example formatting.\nChecking Fine-Tuning Job Status After creating the fine-tuning job, select Custom Models in the navigation pane. You will find the current fine-tuning job listed under Jobs. You can use this page to track the status.\nWhen the status changes to Completed, you can select the job name to navigate to the Training job overview page to find specifications, S3 locations, and hyperparameters used.\nHosting the Fine-Tuned Model with Provisioned Throughput Once the job is successful:\nGo to Custom Models \u0026gt; Models and select your model. Select Purchase Provisioned Throughput. Choose a commitment term (no commitment, 1 month, or 6 months). Evaluation Framework and Results Multi-LLM Judge to Reduce Bias We used a scoring system (0-10) with two judges: Claude 3.5 Sonnet and Llama 3.1 70B.\nResponse Quality Comparison Fine-tuning and RAG both improved response quality by ~30% for Nova Lite. The hybrid approach (Fine-tuning + RAG) improved quality by 83%.\nNotably, Nova Micro (smaller model) with the hybrid approach performed nearly as well as larger models.\nLatency and Token Usage Fine-tuning reduced base model latency by ~50%, while RAG reduced it by ~30%.\nFine-tuning reduced average total tokens by over 60%, whereas RAG doubled token usage due to context switching.\nConclusion We recommend combining Model Customization and RAG for Q\u0026amp;A tasks to maximize performance. Fine-tuning is superior for tone/style adjustment and latency-sensitive tasks, while RAG is essential for dynamic data.\nAbout the Authors\nẢnh đại diện Giới thiệu về các tác giả Mengdie (Flora) Wang is a Data Scientist in the AWS General AI Innovation Center, where she works with customers to architect and deploy scalable general AI solutions to solve their unique business challenges. She specializes in custom modeling techniques and agent-based AI systems, helping organizations unlock the full potential of general AI technology. Prior to joining AWS, Flora earned her Master\u0026rsquo;s in Computer Science from the University of Minnesota, where she developed expertise in machine learning and artificial intelligence. Sungmin Hong is a Senior Applied Scientist at Amazon\u0026rsquo;s General AI Innovation Center, where he helps accelerate the diversity of use cases for AWS customers. Prior to joining Amazon, Sungmin was a postdoctoral fellow at Harvard Medical School. He holds a PhD in Computer Science from New York University. Outside of work, he prides himself on keeping his houseplants alive for 3+ years. Jae Oh Woo is a Senior Applied Scientist at the AWS General AI Innovation Center, where he specializes in developing custom solutions and custom models for a variety of use cases. He has a strong passion for interdisciplinary research that connects theoretical foundations with practical applications in the rapidly evolving field of general AI. Prior to joining Amazon, Jae Oh was a Simons Postdoctoral Fellow at the University of Texas at Austin, where he conducted research across the departments of Mathematics and Electrical and Computer Engineering. He holds a PhD in Applied Mathematics from Yale University. Rahul Ghoshis an Applied Scientist at Amazon\u0026rsquo;s General AI Innovation Center, where he works with AWS customers across various verticals to accelerate the use of general AI. Rahul holds a PhD in Computer Science from the University of Minnesota. Baishali Chaudhury is an Applied Scientist in the General AI Innovation Center at AWS, where she focuses on advancing general AI solutions for real-world applications. She has a strong background in computer vision, machine learning, and AI for healthcare. Baishali holds a PhD in Computer Science from the University of South Florida and a PostDoc from Moffitt Cancer Center. Anila Joshi has over a decade of experience building AI solutions. As the AWSI Geographic Leader at the AWS General AI Innovation Center, Anila pioneers innovative applications of AI that push the boundaries of what is possible and accelerate customer adoption of AWS services by helping customers conceptualize, define, and deploy secure general AI solutions. "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/3-blogstranslated/3.2-blog2/",
	"title": "Blog 2",
	"tags": [],
	"description": "",
	"content": "Migrating CDK Version 1 Applications to CDK Version 2 with Amazon Q Developer by Dr. Rahul Sharad Gaikwad, Tamilselvan P, and Vinodkumar Mandalapu on April 30, 2025 on Amazon Q.\nIntroduction: AWS Cloud Development Kit (AWS CDK) is an open-source software development framework to define cloud infrastructure in code and provision it through AWS CloudFormation. As of June 1, 2023, AWS CDK version 1 is no longer supported. To avoid potential issues using an outdated version and to take advantage of the latest features and improvements, we recommend upgrading to AWS CDK version 2.\nAmazon Q Developer, a generative AI-powered assistant for software development, enhances the efficiency of software development teams. It facilitates the creation of deployment-ready Infrastructure as Code (IaC) for AWS CloudFormation, AWS CDK, and Terraform. By using Amazon Q, developers can accelerate IaC development, enhance code quality, and reduce the likelihood of configuration errors.\nThis post demonstrates how Amazon Q Developer supports upgrading an existing AWS CDK v1 application to AWS CDK v2.\nPrerequisites AWS Builder ID or AWS IAM Identity Center credentials controlled by your organization\nSupported IDE, such as Visual Studio Code\nAWS Toolkit IDE extension\nAuthentication and connection\nNode.js\nAWS CDK version 1\nAWS CDK version 2\nPlan In this blog post, I will explore a code example where I created a VPC, Subnet, and ECS Fargate cluster using AWS CDK version 1. Then, I will explain how you can use Amazon Q to convert the code from CDK v1 to CDK v2.\n1. To start this process, I began by asking Amazon Q Developer to provide the necessary steps to migrate from CDK version 1 to version 2, as outlined below.\nCan you provide the steps to migrate from cdk version 1 to version 2?\n2. In the screenshot above, Amazon Q Developer outlined several steps we can take to make the necessary changes. The first step is to update dependencies. If I need guidance on how to update dependencies, I can ask Amazon Q Developer for help again by requesting steps related to updating dependencies as shown below.\nCan you provide the steps to update dependencies?\n3. After updating the dependencies, the next step is to update the import statements. For guidance on how to update import statements, I can ask the Amazon Q Developer assistant for help again by asking for steps related to how to import statements as shown below.\n@workspace Can you provide the steps to update import statements?\nIn the screenshot above, if you notice, I prefixed the prompt with @workspace, which automatically includes the most relevant parts of my workspace code as context.\n4. If any errors occur while updating the code according to Amazon Q Developer\u0026rsquo;s recommendations, I can use Amazon Q Developer to debug the issue and provide the necessary input to resolve it.\n5. After completing the required steps, I can deploy the application using AWS CDK version 2 by running the cdk deploy command.\n6. In addition to other capabilities, Amazon Q also offers code review functionality. To start a code review, simply select Amazon Q and use the /review command. Then, I will have the option to review active files or the entire open workspace. Select your preference and Amazon Q will analyze your project and provide comprehensive review results.\n7. Amazon Q Developer can also generate documentation, including README files. To generate documentation, select Amazon Q and enter the /doc command. Amazon Q will automatically generate a README file for your project. I can then review the generated documentation, accept the changes, or provide specific instructions for further modifications.\nConclusion In this blog, I demonstrated how Amazon Q Developer can simplify and accelerate the process of upgrading from AWS CDK version 1 to version 2, ensuring your cloud infrastructure remains secure, efficient, and aligned with the latest AWS innovations. AWS CDK version 2 offers a unified, streamlined library with improved performance and ongoing support, making infrastructure management easier and more reliable.\nBy leveraging Amazon Q Developer, a generative AI-powered assistant, teams can automate Infrastructure as Code development, enhance code quality, and minimize configuration errors. Together, these tools empower development teams to confidently modernize and scale their AWS environments, turning the upgrade process into a seamless opportunity for innovation and growth.\nResources To learn more about Amazon Q Developer, check out the following resources:\nAmazon Q Developer Workshop\nAmazon Q Developer User Guide\nTo learn more about AWS CDK, check out the following resources:\nAWS CDK Workshop\nHow to use Amazon Q Developer to deploy a serverless web application with AWS CDK\nAbout the authors:\nProfile Photo About the authors Dr. Rahul Sharad Gaikwad is a Solutions Architect at AWS, driving cloud innovation through customer workload migration and modernization. As a Generative AI and DevOps enthusiast, he architects cutting-edge solutions and is recognized as an APJC HashiCorp Ambassador. He holds a PhD in AIOps and has received the Man of Excellence Award, Indian Achiever Award, Best PhD Thesis Award, Research Scholar of the Year Award, and Young Researcher Award. Vinodkumar Mandalapu is a DevOps Consultant at AWS, specializing in designing and deploying cloud-based infrastructure and deployment pipelines on AWS. With extensive experience in automating and streamlining software delivery, he has helped organizations of all sizes leverage the power of the cloud to drive innovation, improve scalability, and enhance operational efficiency. In his free time, he enjoys traveling and spending quality time with his son. Tamilselvan P is a DevOps Consultant at AWS, focused on architecting and implementing cloud-native systems as well as continuous delivery within the ecosystem. Leveraging his comprehensive expertise in orchestrating and refining software release processes, he has assisted customers across various industries and scales in harnessing cloud technology to innovate faster, increase scalability, and enhance operational performance. In his free time, he enjoys playing cricket. "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/3-blogstranslated/3.3-blog3/",
	"title": "Blog 3",
	"tags": [],
	"description": "",
	"content": "Melting The Ice - How Natural Intelligence Simplified a Data Lake Migration to Apache Iceberg by Yonatan Dolan and Haya Axelrod Stern, Zion Rubin, Michal Urbanowicz on April 28, 2025 in Advanced (300), Analytics, AWS Glue, Best Practices, Case Study, Technical How-to Permalink\nThis article was co-authored by Haya Axelrod Stern, Zion Rubin, and Michal Urbanowicz from Natural Intelligence.\nMany organizations choose data lakes for their flexibility and scalability in managing structured and unstructured data. However, migrating an existing data lake to a new table format like Apache Iceberg can present numerous technical and organizational challenges.\nNatural Intelligence (NI) is a leader in the multi-category marketplace sector. With prominent brands like Top10.com and BestMoney.com, NI helps millions of people make smart decisions every day. Recently, NI embarked on a journey to transform its traditional data lake from Apache Hive to Apache Iceberg.\nIn this article, NI shares their journey, the innovative solutions they developed, and key lessons that can guide other organizations looking to follow a similar path. The content focuses heavily on practical challenges and how they were resolved during the transition, rather than the complex technical specifications of Apache Iceberg itself.\nWhy Apache Iceberg? The data architecture at NI follows the Medallion Architecture (Bronze – Silver – Gold layers), described as follows:\nBronze layer: Raw, unprocessed data collected from various sources, stored in its native format in Amazon Simple Storage Service (Amazon S3) and ingested via Apache Kafka brokers.\nSilver layer: Contains cleaned and enriched data, processed using Apache Flink.\nGold layer: Stores analytics-ready datasets designed for Business Intelligence (BI) and reporting. Data in this layer is generated through Apache Spark pipelines and consumed by services such as Snowflake, Amazon Athena, Tableau, and Apache Druid. Data is stored in Apache Parquet format, with AWS Glue Catalog responsible for metadata management.\nWhile this architecture met NI\u0026rsquo;s data analytics needs, it lacked the flexibility required for a truly open and adaptable data platform. The Gold layer could only work with query engines that supported Hive and the AWS Glue Data Catalog. Although Amazon Athena could be used, for Snowflake, NI had to maintain a separate catalog to query external tables. This issue made evaluating or adopting alternative tools and engines difficult—unless they wanted to duplicate data, rewrite queries, or perform costly catalog synchronizations. As the business scaled, NI needed a data platform that could support multiple different query engines simultaneously with a single data catalog, while avoiding vendor lock-in.\nThe Power of Apache Iceberg Apache Iceberg emerged as the perfect solution—an open, flexible table format suited to NI\u0026rsquo;s Data Lake First approach. Iceberg offers several key advantages such as ACID transactions, schema evolution, time travel, improved performance, and more. But the primary strategic benefit lies in its ability to support multiple query engines simultaneously. It also has the following advantages:\nDecoupling of storage and compute: The open table format allows you to separate the storage layer from the query engine, enabling easy swapping and simultaneous support for multiple tools without data duplication.\nVendor independence: As an open table format, Apache Iceberg prevents vendor lock-in, giving you the flexibility to adapt to changing analytics needs.\nVendor adoption: Apache Iceberg is widely supported by major platforms and tools, providing seamless integration and long-term compatibility with the ecosystem.\nBy transitioning to Iceberg, NI was able to embrace a truly open data platform, providing long-term flexibility, scalability, and interoperability while maintaining a unified single source of truth for all analytics and reporting needs.\nChallenges Faced Migrating a live production data lake to Iceberg is challenging due to operational complexity and legacy constraints. The data service at NI runs hundreds of Spark and machine learning processes, manages thousands of tables, and supports over 400 dashboards—all operating 24/7. Any migration process needed to be executed without production interruption; coordinating such a migration while operations continued seamlessly was difficult.\nNI needed to accommodate diverse users with varying requirements and timelines, ranging from data engineers to data analysts, data scientists, and BI teams.\nAdding to the challenge were legacy constraints. Some existing tools did not fully support Iceberg, so it was necessary to maintain Hive-backed tables for compatibility. NI realized that not all consumers could adopt Iceberg immediately. A plan was needed to allow for incremental transition without downtime or disruption to ongoing operations.\nKey Components for Migration To ensure a smooth and successful transition, six critical components were identified:\nSupport ongoing operations: Ensure uninterrupted compatibility with existing systems and processes throughout the migration.\nUser transparency: Minimize disruption for users by keeping table names and access methods the same.\nGradual consumer migration: Allow users to switch to Iceberg at their own pace, avoiding the need for a simultaneous mass migration.\nETL flexibility: Allow shifting ETL pipelines to Iceberg without imposing constraints on development or deployment.\nCost effectiveness: Minimize storage and processing data duplication, as well as incremental costs during the transition phase.\nMinimize maintenance: Reduce the operational burden of maintaining two table formats (Hive and Iceberg) in parallel during the transition.\nEvaluating Traditional Migration Methods Apache Iceberg supports two main migration methods: In-place migration and Rewrite-based migration.\nIn-place migration\nHow it works: This method converts existing datasets to Iceberg tables without duplicating data, by creating Iceberg metadata based on existing data files, while keeping the original layout and format.\nPros:\nSaves storage costs, as there is no data duplication. Easy to implement, simple execution process. Keeps current table names and locations, so users don\u0026rsquo;t have to change access. No data movement required, minimal compute requirements → lower cost. Cons:\nRequires downtime: All write operations must pause during conversion, which was unacceptable for NI, as data and analytics processes are critical and operate 24/7. Cannot migrate gradually: All users must switch to Iceberg at the same time, increasing the risk of system disruption. Limited validation: No opportunity to check data correctness before completing the conversion; if there is an error, recovery from backups is needed. Technical limitations: Schema evolution during migration can be difficult; data type conflicts can cause the entire process to fail. Rewrite-based migration How it works: This method creates a new Iceberg table by rewriting and reorganizing existing data files into an optimal Iceberg format and structure, improving performance and data management.\nPros:\nNo downtime required during the migration process. Supports gradual consumer migration, allowing teams to adopt Iceberg at their own pace. Allows for thorough data validation before full switchover. Simple rollback mechanism, can easily revert if errors occur. Cons:\nIncreased resource costs: Requires double the storage capacity and processing power during the migration time. Maintenance complexity: Requires maintaining two parallel data pipelines, increasing operational burden. Consistency challenges: Hard to ensure two systems remain fully synchronized during the transition. Performance impact: Dual writes can increase latency and slow down pipelines. Why neither option was good enough NI found that neither method (In-place nor Rewrite-based) could fully meet the critical requirements:\nIn-place migration was unsuitable because the downtime requirement was unacceptable, and it did not support a gradual transition process. Rewrite-based migration was costly and complex in terms of operational management due to maintaining two parallel pipelines. Based on this analysis, NI developed a Hybrid approach — combining the advantages of both methods while minimizing and overcoming their limitations.\nThe Hybrid Solution The hybrid migration strategy was designed based on 5 core components, leveraging AWS analytics services to orchestrate, process, and manage state.\n1. Hive-to-Iceberg CDC (Change Data Capture): A system that automatically syncs Hive tables to Iceberg using a custom CDC process to support existing users. Unlike traditional row-level CDC, NI applied partition-level CDC—since Hive typically updates data by overwriting entire partitions. This approach helps maintain data consistency between Hive and Iceberg without changing data write logic, ensuring that both tables contain the same data during the migration phase.\n2. Continuous schema synchronization: During migration, schema evolution causes many maintenance challenges. NI implemented an automated schema synchronization process that compares schemas between Hive and Iceberg and adjusts differences while maintaining data type compatibility.\n3. Iceberg-to-Hive reverse CDC: Allows data teams to switch ETL (Extract, Transform, Load) jobs to write directly to Iceberg, while still maintaining compatibility with legacy processes using Hive. Reverse CDC automatically updates data from Iceberg back to Hive, ensuring that downstream pipelines that haven\u0026rsquo;t migrated yet continue to function normally. Thanks to this, the system can transition gradually without disrupting existing processes.\n4. Alias management in Snowflake: Using aliases in Snowflake ensures that Iceberg tables keep their original names, making the transition transparent to users. This approach minimizes reconfiguration on dependent teams and existing workflows.\n5. Table replacement: Once the entire system has switched to Iceberg, NI swaps the production tables, retaining the original table names, and completes the migration process.\nTechnical Deep Dive The migration process from Hive to Iceberg was built from several steps:\n1. Hive-to-Iceberg CDC Diagram: Goal: Keep Hive and Iceberg tables synchronized without duplicate effort.\nThe preceding figure shows how every partition written to the Hive table is automatically and transparently replicated to the Iceberg table using a CDC process. This process ensures that both tables are synchronized, allowing for a seamless and incremental migration without disrupting downstream systems. NI chose partition-level synchronization because legacy Hive ETL jobs wrote updates by overwriting entire partitions and updating partition locations. Adopting a similar approach in the CDC process helped ensure that it remained consistent with how data was originally managed, making the migration smoother and avoiding the need to rework row-level logic.\nImplementation:\nTo keep Hive and Iceberg tables synchronized without duplicate effort, a streamlined process was implemented. Whenever partitions in a Hive table are updated, the AWS Glue Catalog emits events like UpdatePartition. Amazon EventBridge captures these events, filters them for relevant databases and tables according to EventBridge rules, and triggers an AWS Lambda function. This function parses the event metadata and sends partition updates to an Apache Kafka topic.\nA Spark task running on Amazon EMR consumes messages from Kafka, which contain the updated partition details from the Data Catalog events. Using that event metadata, the Spark task queries the relevant Hive table and writes to the Iceberg table in Amazon S3 using the Spark Iceberg API overwritePartitions, as shown in the following example:\n{ \u0026#34;id\u0026#34;: \u0026#34;10397e54-c049-fc7b-76c8-59e148c7cbfc\u0026#34;, \u0026#34;detail-type\u0026#34;: \u0026#34;Glue Data Catalog Table State Change\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;aws.glue\u0026#34;, \u0026#34;time\u0026#34;: \u0026#34;2024-10-27T17:16:21Z\u0026#34;, \u0026#34;region\u0026#34;: \u0026#34;us-east-1\u0026#34;, \u0026#34;detail\u0026#34;: { \u0026#34;databaseName\u0026#34;: \u0026#34;dlk_visitor_funnel_dwh_production\u0026#34;, \u0026#34;changedPartitions\u0026#34;: [ \u0026#34;2024-10-27\u0026#34; ], \u0026#34;typeOfChange\u0026#34;: \u0026#34;UpdatePartition\u0026#34;, \u0026#34;tableName\u0026#34;: \u0026#34;fact_events\u0026#34; } } By targeting only modified partitions, the process (shown in the following figure) significantly reduced the need for expensive full-table rewrites. Iceberg\u0026rsquo;s robust metadata layers, including snapshots and manifest files, were seamlessly updated to capture these changes, providing efficient and accurate synchronization between the Hive and Iceberg tables. 2. Iceberg-to-Hive Reverse CDC Diagram Goal: Support Hive consumers while allowing ETL processes to switch to Iceberg.\nThe preceding figure shows the reverse process, where every partition written to the Iceberg table is automatically and transparently replicated to the Hive table using a CDC mechanism. This process helps ensure synchronization between the two systems, allowing seamless data updates for legacy systems that still rely on Hive while transitioning to Iceberg.\nImplementation:\nSynchronizing data from Iceberg tables back to Hive tables presented a different challenge. Unlike Hive tables, the Data Catalog does not track partition updates for Iceberg tables because partitions in Iceberg are managed internally rather than in the catalog. This meant NI could not rely on Glue Catalog events to detect partition changes.\nTo address this, NI implemented a solution similar to the previous process but adapted to Iceberg\u0026rsquo;s architecture. Apache Spark is used to query Iceberg\u0026rsquo;s metadata tables - specifically the snapshots and entries tables - to identify modified partitions since the last synchronization. The query used is:\nSELECT e.data_file.partition, MAX(s.committed_at) AS last_modified_time FROM $target_table.snapshots JOIN $target_table.entries e ON s.snapshot_id = e.snapshot_id WHERE s.committed_at \u0026gt; \u0026#39;$last_sync_time\u0026#39; GROUP BY e.data_file.partition; This query returns only the partitions that have been updated since the last sync, allowing it to focus entirely on changed data. Using this information, similar to the previous process, a Spark job retrieves the updated partitions from Iceberg and writes them back to the corresponding Hive table, providing seamless synchronization between both tables.\n3. Continuous Schema Synchronization Goal: Automatically update schemas to maintain consistency across Hive and Iceberg.\nThe preceding figure shows the automated schema synchronization process that helps ensure consistency between Hive and Iceberg table schemas by automatically syncing schema changes. In this example, adding a Channel column minimizes manual work and double maintenance during the extended migration period.\nImplementation:\nTo handle schema changes between Hive and Iceberg, a process was implemented to detect and reconcile differences automatically. When a schema change occurs in a Hive table, the Data Catalog emits an UpdateTable event. This event triggers a Lambda function (routed via EventBridge), which retrieves the updated schema from the Data Catalog for the Hive table and compares it with the Iceberg schema. It is important to note that in NI\u0026rsquo;s setup, schema changes originate from Hive because the Iceberg table is hidden behind aliases system-wide. Because Iceberg is primarily used for Snowflake, a one-way sync from Hive to Iceberg was sufficient. Therefore, there was no mechanism to detect or handle schema changes made directly in Iceberg, as they were not needed in the current workflow.\nDuring the schema reconciliation process (shown in the following figure), data types are normalized to help ensure compatibility — for example, converting Hive VARCHAR to Iceberg STRING. Any new fields or type changes are validated and applied to the Iceberg schema using a Spark task running on Amazon EMR. Amazon DynamoDB stores schema synchronization checkpoints, allowing tracking of changes over time and maintaining consistency between Hive and Iceberg schemas.\nBy automating this schema synchronization, maintenance overhead was significantly reduced, freeing developers from manually syncing schemas, making the long migration period significantly more manageable.\nThe preceding figure depicts an automated workflow for maintaining schema consistency between Hive and Iceberg tables. AWS Glue records table state change events from Hive, triggering an EventBridge event. This event invokes a Lambda function that fetches metadata from DynamoDB and compares schemas fetched from AWS Glue for both Hive and Iceberg tables. If a mismatch is detected, the schema in Iceberg is updated to help ensure alignment, minimizing manual intervention and supporting smooth operations during migration.\n4. Alias Management in Snowflake Goal: Allow Snowflake consumers to adopt Iceberg without changing query references.\nThe preceding figure shows Snowflake aliases enabling seamless migration by mapping queries like SELECT platform, COUNT(clickouts) FROM funnel.clickouts to Iceberg tables in the Glue Catalog. Even with suffix additions during the Iceberg migration, existing queries and workflows remain unchanged, minimizing disruption for BI tools and analysts.\nImplementation:\nTo help ensure a seamless experience for BI tools and analysts during migration, Snowflake aliases were used to map external tables to Iceberg metadata stored in the Data Catalog. By assigning aliases that matched the original Hive table names, existing queries and reports were preserved without disruption. For example, an external table was created in Snowflake and aliased to the original table name, as shown in the following query:\nCREATE OR REPLACE ICEBERG TABLE dlk_visitor_funnel_dwh_production.aggregated_cost EXTERNAL_VOLUME = \u0026#39;s3_dlk_visitor_funnel_dwh_production_iceberg_migration\u0026#39; CATALOG = \u0026#39;glue_dlk_visitor_funnel_dwh_production_iceberg_migration\u0026#39; CATALOG_TABLE_NAME = \u0026#39;aggregated_cost\u0026#39;; ALTER ICEBERG TABLE dlk_visitor_funnel_dwh_production.aggregated_cost REFRESH; Once the migration was complete, a simple change back to the alias was made to point to the new location or schema, making the transition seamless and minimizing any disruption to user workflows.\n5. Table Replacement Goal: Once all ETLs and relevant data processes were successfully converted to use Apache Iceberg capabilities and everything was working correctly with the sync flow, it was time to move to the final stage of the migration. The main objective was to maintain the original table names, avoiding the use of any prefixes like those used in the previous intermediate migration steps. This helps ensure that the configuration remains clean and free of unnecessary naming complexities.\nThe preceding figure shows the table replacement to complete the migration, where Hive on Amazon EMR is used to register Parquet files as Iceberg tables while keeping the original table names and avoiding data duplication, helping ensure a seamless and clean migration.\nImplementation:\nOne of the challenges was the inability to rename tables in AWS Glue, which prevented the use of a simple renaming method for existing sync flow tables. Additionally, AWS Glue does not support the Migrate procedure to create Iceberg metadata over existing data files while preserving the original table name. The strategy to overcome this limitation was to use the Hive metastore on an Amazon EMR cluster. By using Hive on Amazon EMR, NI could create final tables with their original names because it operates in a separate metastore environment, providing the flexibility to define any required schema and table name without interference.\nThe add_files procedure is used to systematically register all existing Parquet files, thereby building all necessary metadata in Hive. This is a critical step, as it helps ensure that all data files are cataloged and appropriately linked within the metastore.\nThe preceding figure shows the conversion of the production table to Iceberg using the add_files procedure to register existing Parquet files and generate Iceberg metadata. This helps ensure a smooth migration while preserving original data and avoiding duplication.\nThis setup allowed the use of existing Parquet files without duplicating data, thereby saving resources. Although the sync flow used separate storage buckets for the final architecture, NI chose to maintain the original buckets and clean up intermediate files. This resulted in a different folder structure on Amazon S3. Historical data has subdirectories for each partition in the original table directory, while new Iceberg data organizes subdirectories under a data folder. This difference was acceptable to avoid data duplication and preserve the original Amazon S3 buckets.\nTechnical Summary The AWS Glue Data Catalog serves as the central source of truth for schema and table updates, with Amazon EventBridge capturing Data Catalog events to trigger synchronization processes. AWS Lambda parses event metadata and manages schema synchronization, while Apache Kafka buffers events for real-time processing. Apache Spark on Amazon EMR handles data transformation and incremental updates, while Amazon DynamoDB maintains state, including synchronization checkpoints and table mappings. Finally, Snowflake seamlessly consumes Iceberg tables via aliases without disrupting existing workflows.\nMigration Results The migration process was completed with zero downtime; continuous operations were maintained throughout the process, supporting hundreds of pipelines and dashboards without interruption. The migration was executed with a cost-optimization mindset, with incremental updates and partition-level synchronization minimizing compute and storage resource usage. Ultimately, NI established a modern, vendor-neutral platform that allows for scaling evolving analytics and machine learning needs. It enables seamless integration with multiple compute and query engines, supporting further flexibility and innovation.\nConclusion Transforming Natural Intelligence to Apache Iceberg was a critical step in modernizing the company\u0026rsquo;s data infrastructure. By adopting a hybrid strategy and utilizing the power of event-driven architecture, NI helped ensure a seamless transition that balanced innovation with operational stability. The journey underscores the importance of careful planning, understanding the data ecosystem, and focusing on an organization-first approach.\nAbove all, business operations were centered, and continuity prioritized user experience. By doing so, NI unlocked the flexibility and scalability of their data lake while minimizing disruption, enabling teams to utilize advanced analytics capabilities, positioning the company at the forefront of modern data management and ready for the future.\nIf you are considering an Apache Iceberg migration or facing similar data infrastructure challenges, we encourage you to explore the possibilities. Embrace open formats, utilize automation, and design with your organization\u0026rsquo;s unique needs in mind. The journey can be complex, but the rewards in terms of scalability, flexibility, and innovation are well worth the effort. You can use AWS Prescriptive Guidance to help learn more about how to best use Apache Iceberg for your organization.\nAbout the authors\nProfile Photo About the authors Yonatan Dolan is a Principal Analytics Specialist at Amazon Web Services. Yonatan is an Apache Iceberg evangelist. Haya Stern is a Senior Director of Data at Natural Intelligence. She leads the development of NI\u0026rsquo;s large-scale data platform, focusing on enabling analytics, streamlining data workflows, and improving development efficiency. Over the past year, she led the successful migration from the previous data architecture to a modern data lakehouse based on Apache Iceberg and Snowflake. Zion Rubin is a Data Architect at Natural Intelligence with ten years of experience architecting large-scale big data platforms, currently focused on developing intelligent agent systems that turn complex data into real-time business insights. Michał Urbanowicz is a Cloud Data Engineer at Natural Intelligence with expertise in data warehouse migration and implementing robust retention, cleaning, and monitoring processes to ensure scalability and reliability. He also develops automation features that streamline and support campaign management operations in cloud-based environments. "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/5-workshop/5.3-infrastructure/5.3.1-amazon-s3-bucket/",
	"title": "Create Amazon S3 Bucket",
	"tags": [],
	"description": "",
	"content": "In this step, we will create an S3 Bucket to store contract templates and the necessary vector data for the AI.\n1. Create Bucket Access the S3 service on the AWS Console. Select the orange Create bucket button. General configuration: Bucket type: Select General purpose. Bucket name: Enter a globally unique name (e.g., contract-app-demo). Note: Bucket names can only contain lowercase letters, numbers, and hyphens; do not use uppercase letters. Scroll to the bottom, keeping other settings as default. Click Create bucket. 2. Create Folder Structure Once created, we need to create a folder structure to organize the data.\nClick on the Bucket name you just created to access it. Click the Create folder button. Create 4 folders one by one with the exact names below (you need to repeat the folder creation process 4 times): contract-templates index legal-corpus user-data After creating them, your bucket structure will look like this:\n3. Upload Sample Data Use the files in the contract-demo source code folder that you downloaded to your computer in the Preparation section:\nAccess the contract-templates folder on S3 -\u0026gt; Click Upload -\u0026gt; Select sample contract files (.docx/.pdf) from your computer -\u0026gt; Click Upload. Access the index folder on S3 -\u0026gt; Click Upload -\u0026gt; Select the template_metadata.jsonl file. Access the legal-corpus folder on S3 -\u0026gt; Click Upload -\u0026gt; Select legal data files (if available in the source code). Use the files in the contract-demo source code folder that you downloaded to your computer in the Preparation section. Perform the upload according to the following structure:\nAccess the index folder on S3 -\u0026gt; Click Upload -\u0026gt; Select the template_metadata.jsonl file -\u0026gt; Click Upload. Access the legal-corpus folder on S3. Here, create 2 additional subfolders named index and raw-documents. Access the index subfolder -\u0026gt; Click Upload -\u0026gt; Select the legal_chunks_with_emb.jsonl (or legal_metadata.json) file. Access the raw-documents subfolder -\u0026gt; Click Upload -\u0026gt; Select the raw legal document files (if available). "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/5-workshop/5.4-backed/5.4.1-rag-search/",
	"title": "Create Lambda RAG Search",
	"tags": [],
	"description": "",
	"content": "The ragsearch function is the most critical component, responsible for searching legal data and answering questions.\n1. Function Initialization Access the Lambda service -\u0026gt; Select Create function. Choose Author from scratch. Fill in the basic information: Function name: ragsearch Runtime: Python 3.12 Architecture: x86_64 Click Create function. 2. Update Code In the Code tab, open the lambda_function.py file. Clear all default content. Open the ragsearch.py file in the source code folder, copy the entire content, and paste it into the code window on the AWS Console. import json import os import math import logging from typing import List, Dict, Any, Tuple import boto3 from botocore.exceptions import ClientError logger = logging.getLogger() logger.setLevel(logging.INFO) # ----------------------------------------------------------------------------- # Config # ----------------------------------------------------------------------------- AWS_REGION = os.getenv(\u0026#34;AWS_REGION\u0026#34;, \u0026#34;ap-southeast-1\u0026#34;) EMBED_MODEL_ID = os.getenv(\u0026#34;EMBED_MODEL_ID\u0026#34;, \u0026#34;cohere.embed-multilingual-v3\u0026#34;) LEGAL_INDEX_BUCKET = os.getenv(\u0026#34;LEGAL_INDEX_BUCKET\u0026#34;) # Name of S3 bucket LEGAL_INDEX_KEY = os.getenv(\u0026#34;LEGAL_INDEX_KEY\u0026#34;, \u0026#34;index/legal_chunks_with_emb.jsonl\u0026#34;) if not LEGAL_INDEX_BUCKET: raise RuntimeError(\u0026#34;LEGAL_INDEX_BUCKET env var is required\u0026#34;) s3 = boto3.client(\u0026#34;s3\u0026#34;, region_name=AWS_REGION) bedrock = boto3.client(\u0026#34;bedrock-runtime\u0026#34;, region_name=AWS_REGION) # ----------------------------------------------------------------------------- # Global cache # ----------------------------------------------------------------------------- INDEX_CACHE = { \u0026#34;loaded\u0026#34;: False, \u0026#34;chunks\u0026#34;: [], # list of dict (metadata + text, no embedding) \u0026#34;vectors\u0026#34;: [] # list of list[float] } # ----------------------------------------------------------------------------- # Helpers # ----------------------------------------------------------------------------- def cosine_similarity(a: List[float], b: List[float]) -\u0026gt; float: if not a or not b or len(a) != len(b): return 0.0 dot = 0.0 na = 0.0 nb = 0.0 for x, y in zip(a, b): dot += x * y na += x * x nb += y * y if na == 0.0 or nb == 0.0: return 0.0 return dot / math.sqrt(na * nb) def get_embedding(text: str) -\u0026gt; List[float]: if not text or not text.strip(): raise ValueError(\u0026#34;Query text is empty\u0026#34;) model_id = EMBED_MODEL_ID # If Cohere Embed v3 (english/multilingual) if model_id.startswith(\u0026#34;cohere.embed-\u0026#34;): # Query → use search_query body_dict = { \u0026#34;texts\u0026#34;: [text], \u0026#34;input_type\u0026#34;: \u0026#34;search_query\u0026#34; # Can add \u0026#34;truncate\u0026#34;: \u0026#34;END\u0026#34; if needed } else: # Default: Titan embeddings body_dict = { \u0026#34;inputText\u0026#34;: text } body = json.dumps(body_dict) try: response = bedrock.invoke_model( modelId=model_id, body=body, contentType=\u0026#34;application/json\u0026#34;, accept=\u0026#34;application/json\u0026#34;, ) except ClientError as e: logger.error(\u0026#34;Bedrock invoke_model failed: %s\u0026#34;, e) raise response_body = json.loads(response[\u0026#34;body\u0026#34;].read()) # Parse output depending on model if model_id.startswith(\u0026#34;cohere.embed-\u0026#34;): # \u0026#34;embeddings\u0026#34;: [ [1024 floats] ] embeddings = response_body.get(\u0026#34;embeddings\u0026#34;) if not embeddings or not isinstance(embeddings, list): raise ValueError(\u0026#34;No embeddings found in Cohere response\u0026#34;) return embeddings[0] else: # Titan: {\u0026#34;embedding\u0026#34;: [..]} embedding = response_body.get(\u0026#34;embedding\u0026#34;) if not embedding: raise ValueError(\u0026#34;No embedding found in Titan response\u0026#34;) return embedding def load_index_if_needed(): if INDEX_CACHE[\u0026#34;loaded\u0026#34;]: return logger.info( \u0026#34;Loading legal index from s3://%s/%s ...\u0026#34;, LEGAL_INDEX_BUCKET, LEGAL_INDEX_KEY ) try: obj = s3.get_object(Bucket=LEGAL_INDEX_BUCKET, Key=LEGAL_INDEX_KEY) except ClientError as e: logger.error(\u0026#34;Failed to get index object from S3: %s\u0026#34;, e) raise chunks: List[Dict[str, Any]] = [] vectors: List[List[float]] = [] for line in obj[\u0026#34;Body\u0026#34;].iter_lines(): if not line: continue try: rec = json.loads(line.decode(\u0026#34;utf-8\u0026#34;)) except json.JSONDecodeError: logger.warning(\u0026#34;Invalid JSON line in index, skipped\u0026#34;) continue emb = rec.get(\u0026#34;embedding\u0026#34;) text = (rec.get(\u0026#34;text\u0026#34;) or \u0026#34;\u0026#34;).strip() if not emb or not text: # Skip record missing embedding / text continue # Separate embedding from metadata rec_no_emb = dict(rec) rec_no_emb.pop(\u0026#34;embedding\u0026#34;, None) vectors.append(emb) chunks.append(rec_no_emb) INDEX_CACHE[\u0026#34;chunks\u0026#34;] = chunks INDEX_CACHE[\u0026#34;vectors\u0026#34;] = vectors INDEX_CACHE[\u0026#34;loaded\u0026#34;] = True logger.info( \u0026#34;Loaded %d chunks with embeddings into cache\u0026#34;, len(chunks) ) def parse_event_body(event: Dict[str, Any]) -\u0026gt; Dict[str, Any]: if \u0026#34;body\u0026#34; not in event: return event body = event[\u0026#34;body\u0026#34;] if event.get(\u0026#34;isBase64Encoded\u0026#34;): body = base64.b64decode(body).decode(\u0026#34;utf-8\u0026#34;) # need import base64 if used try: data = json.loads(body) except json.JSONDecodeError: raise ValueError(\u0026#34;Request body must be valid JSON\u0026#34;) return data def apply_filters(rec: Dict[str, Any], filters: Dict[str, Any]) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34; filters format: { \u0026#34;source_type\u0026#34;: [\u0026#34;legal\u0026#34;, \u0026#34;template\u0026#34;], \u0026#34;doc_category\u0026#34;: [\u0026#34;luat\u0026#34;, \u0026#34;nghi_dinh\u0026#34;], \u0026#34;field\u0026#34;: [\u0026#34;Xây dựng - Đô thị\u0026#34;] } Returns True if record PASSES filter. \u0026#34;\u0026#34;\u0026#34; if not filters: return True # source_type st_list = filters.get(\u0026#34;source_type\u0026#34;) if st_list: st_val = rec.get(\u0026#34;source_type\u0026#34;) if st_val not in st_list: return False # doc_category cat_list = filters.get(\u0026#34;doc_category\u0026#34;) if cat_list: cat_val = rec.get(\u0026#34;doc_category\u0026#34;) if cat_val not in cat_list: return False # field field_list = filters.get(\u0026#34;field\u0026#34;) if field_list: field_val = rec.get(\u0026#34;field\u0026#34;) if field_val not in field_list: return False return True def search_index(query: str, top_k: int, filters: Dict[str, Any]) -\u0026gt; List[Dict[str, Any]]: load_index_if_needed() q_emb = get_embedding(query) scores: List[Tuple[float, int]] = [] for i, vec in enumerate(INDEX_CACHE[\u0026#34;vectors\u0026#34;]): s = cosine_similarity(q_emb, vec) scores.append((s, i)) # sort from high to low scores.sort(key=lambda x: x[0], reverse=True) results: List[Dict[str, Any]] = [] for score, idx in scores: if score \u0026lt;= 0: break rec = INDEX_CACHE[\u0026#34;chunks\u0026#34;][idx] if not apply_filters(rec, filters): continue # copy metadata + add score res = dict(rec) res[\u0026#34;score\u0026#34;] = score results.append(res) if len(results) \u0026gt;= top_k: break return results def make_response(status_code: int, body: Dict[str, Any]) -\u0026gt; Dict[str, Any]: return { \u0026#34;statusCode\u0026#34;: status_code, \u0026#34;headers\u0026#34;: { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;Access-Control-Allow-Origin\u0026#34;: \u0026#34;*\u0026#34;, # for demo }, \u0026#34;body\u0026#34;: json.dumps(body, ensure_ascii=False), } # ----------------------------------------------------------------------------- # Lambda handler # ----------------------------------------------------------------------------- def lambda_handler(event, context): logger.info(\u0026#34;Received event: %s\u0026#34;, json.dumps(event)[:1000]) try: body = parse_event_body(event) # ================================================================== # 🔥 NEW CODE: KEEP WARM HANDLING # ================================================================== if body.get(\u0026#34;keep_warm\u0026#34;): logger.info(\u0026#34;Keep-warm ping received. Checking index cache...\u0026#34;) # Important: Call this function to load file from S3 into RAM (if not present) # Helps the next user avoid waiting (Cold Start) load_index_if_needed() return make_response(200, {\u0026#34;message\u0026#34;: \u0026#34;Pong! I am warm and index is loaded.\u0026#34;}) # ================================================================== query = (body.get(\u0026#34;query\u0026#34;) or \u0026#34;\u0026#34;).strip() if not query: return make_response(400, {\u0026#34;error\u0026#34;: \u0026#34;query is required\u0026#34;}) language = (body.get(\u0026#34;language\u0026#34;) or \u0026#34;vi\u0026#34;).lower() top_k = body.get(\u0026#34;top_k\u0026#34;) or 10 try: top_k = int(top_k) if top_k \u0026lt;= 0: top_k = 10 except Exception: top_k = 10 filters = body.get(\u0026#34;filters\u0026#34;) or {} # Perform search (once index is confirmed loaded) results = search_index(query=query, top_k=top_k, filters=filters) resp = { \u0026#34;query\u0026#34;: query, \u0026#34;language\u0026#34;: language, \u0026#34;top_k\u0026#34;: top_k, \u0026#34;results\u0026#34;: results, } return make_response(200, resp) except ValueError as ve: return make_response(400, {\u0026#34;error\u0026#34;: str(ve)}) except ClientError as ce: logger.error(\u0026#34;AWS client error: %s\u0026#34;, ce) return make_response( 502, {\u0026#34;error\u0026#34;: \u0026#34;Upstream AWS error\u0026#34;, \u0026#34;details\u0026#34;: str(ce)}, ) except Exception as e: logger.error(\u0026#34;Unexpected error: %s\u0026#34;, e) return make_response( 500, {\u0026#34;error\u0026#34;: \u0026#34;Internal server error\u0026#34;, \u0026#34;details\u0026#34;: str(e)}, ) Click the Deploy button (grey button) to save changes. 3. Configuration Switch to the Configuration tab to set technical parameters.\nA. General configuration\nSelect General configuration on the left menu -\u0026gt; Click Edit. Memory: Increase to 3000 MB. Timeout: Increase to 1 min 0 sec. Click Save. B. Environment variables\nSelect Environment variables on the left menu -\u0026gt; Click Edit. Click Add environment variable and add the following 2 lines: Key: LEGAL_INDEX_BUCKET | Value: \u0026lt;Your-S3-Bucket-Name\u0026gt; Key: LEGAL_INDEX_KEY | Value: legal-corpus/index/legal_chunks_with_emb.jsonl Click Save. 4. Permissions Select Permissions on the left menu. Click on the Role name (blue link) to open the IAM Role page. In the opened IAM page: Click Add permissions -\u0026gt; Attach policies. Search for and check the following 2 policies: AmazonBedrockFullAccess AmazonS3FullAccess Click Add permissions. "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/5-workshop/5.5-fullstack/5.5.1-backend-deploy/",
	"title": "Deploy Serverless Backend",
	"tags": [],
	"description": "",
	"content": "In this step, we will configure and deploy the entire Backend (including Lambda, API Gateway, Cognito\u0026hellip;) automatically using the Serverless Framework.\n1. Configure Serverless File Open the project source code folder in VS Code.\nNavigate to the backend directory.\nOpen the serverless.yml file.\nLocate and modify the following lines (replace \u0026lt;your-bucket-name\u0026gt; with the S3 Bucket name you created in section 5.3):\nAt the S3_BUCKET_RAW environment variable line: S3_BUCKET_RAW: contract-app-demo-yourname In the iam -\u0026gt; statements -\u0026gt; Resource section (S3 access permissions): Resource: - \u0026#34;arn:aws:s3:::contract-app-demo-yourname\u0026#34; - \u0026#34;arn:aws:s3:::contract-app-demo-yourname/*\u0026#34; 2. Configure API Endpoint for Frontend Before deploying, let\u0026rsquo;s update the API address for the frontend code (even though this file is located in the backend folder, it serves the client).\nAccess AWS Console -\u0026gt; Lambda -\u0026gt; Select the ragsearch function.\nIn the Function overview -\u0026gt; Triggers section, copy the API Endpoint URL.\nReturn to VS Code, open the file at: backend/src/services/ragService.ts (or a similar path in your source code).\nFind the RAG_API_URL declaration line and paste the copied link:\nconst RAG_API_URL = \u0026#34;https://example.execute-api.ap-southeast-1.amazonaws.com\u0026#34;; Save the file (Ctrl + S). 3. Configure AWS CLI If you haven\u0026rsquo;t configured it yet or want to be sure, run the following commands in the Terminal (CMD/PowerShell) at the project root directory:\nRun the command: aws configure Enter the information from the Access Key .csv file you downloaded: AWS Access Key ID: \u0026lt;Enter Access Key ID\u0026gt; AWS Secret Access Key: \u0026lt;Enter Secret Access Key\u0026gt; Default region name: ap-southeast-1 Default output format: (Press Enter to skip) 4. Deploy Backend Still in the Terminal, navigate into the backend folder: cd backend Run the deploy command: npx serverless deploy ⏳ Wait: This process will take about 3-5 minutes. The Serverless Framework will package the code, create a CloudFormation stack, and push it to AWS.\n5. Check Result If the deployment is successful, the Terminal will display a green message with information about Service Information, endpoints, and functions.\nNow, return to the AWS Console, and you will see new resources such as the Amazon Cognito User Pool and the new Lambda functions created.\n"
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/4-eventparticipated/4.1-event1/",
	"title": "Event 1",
	"tags": [],
	"description": "",
	"content": "Vietnam Cloud Day 2025: Ho Chi Minh City Connect Edition for Builders Time: Thursday, September 18, 2025, 9:00 – 17:30 Location: Amazon Web Services Vietnam, 36th Floor, 2 Hai Trieu, Ben Nghe Ward, District 1, Ho Chi Minh City Role: Attendee\nEvent Objectives Update on top strategic technology trends: Agentic AI. Learn about Data Foundation solutions to address the \u0026ldquo;Data Silos\u0026rdquo; problem that 52% of CDOs are facing. Approach the new software development process: AI-Driven Development Lifecycle (AI-DLC). Grasp security standards for GenAI (MITRE ATLAS, OWASP, NIST) and risk layers. Speakers Eric Yeo - Country General Manager, AWS Vietnam Dr. Jens Lottner - CEO, Techcombank Ms. Trang Phung - CEO \u0026amp; Co-Founder, U2U Network Jaime Valles - VP, GM Asia Pacific and Japan, AWS Jeff Johnson - Managing Director, ASEAN, AWS Vu Van - Co-founder \u0026amp; CEO, ELSA Corp Nguyen Hoa Binh - Chairman, Nexttech Group Dieter Botha - CEO, TymeX Jun Kai Loke - AI/ML Specialist SA, AWS Kien Nguyen - Solutions Architect, AWS Tamelly Lim - Storage Specialist SA, AWS Binh Tran - Senior Solutions Architect, AWS Taiki Dang - Solutions Architect, AWS Michael Armentano - Principal WW GTM Specialist, AWS Hung Nguyen Gia - Head of Solutions Architect, AWS Son Do - Technical Account Manager, AWS Nguyen Van Hai - Director of Software Engineering, Techcombank Phuc Nguyen - Solutions Architect, AWS Alex Tran - AI Director, OCB Nguyen Minh Ngan - AI Specialist, OCB Nguyen Manh Tuyen - Head of Data Application, LPBank Securities Vinh Nguyen - Co-Founder \u0026amp; CTO, Ninety Eight Hung Hoang - Customer Solutions Manager, AWS Christal Poon - Specialist Solutions Architect, AWS Key Highlights 1. Agentic AI \u0026amp; Data Strategy Trends Current State: 88% of CDOs are moving forward with GenAI, yet 52% state their data foundation is not ready. Challenges: Businesses are held back by 3 types of \u0026ldquo;Silos\u0026rdquo;: Data Silos, People Silos, and Business Silos. Data Strategy: End-to-end model from Producers → Foundations → Consumers. Infrastructure: Amazon S3 (Data Lakes), Amazon Redshift (Data Warehouses), supporting open standard Apache Iceberg. Governance: Amazon DataZone assists with Data \u0026amp; AI Governance. New Tools: Introduction of Unified Studio integrating analytics and AI tools. 2. AI-Driven Development Lifecycle (AI-DLC) Speaker Binh Tran introduced the shift from AI-Assisted to AI-Driven Development. The AI-DLC process consists of 3 main phases:\nInception: Build Context on existing code, clarify intent (User Stories), plan (Units of Work), and Domain Modeling. Construction: AI generates code \u0026amp; Tests, adds architectural components. Operation: Deploy with IaC \u0026amp; tests, incident management. 3. Security for GenAI Speaker Taiki Dang emphasized that security must run in parallel with Generative AI.\nRisk Layers: Top layer (Consumer): Risks regarding IP, legal, hallucinations, safety. Middle layer (Tuner): Risks from Managed services, data retention policies. Bottom layer (Provider): Risks from training data. Frameworks \u0026amp; Standards: Apply MITRE ATLAS, OWASP Top 10 for LLM, NIST AI RMF, ISO 42001. Solutions: Use Amazon Bedrock Guardrails to prevent and mitigate risks (such as toxicity, PII leaks). 4. Analytics \u0026amp; Business Intelligence Speaker Christal Poon presented the transition from Amazon QuickSight to Amazon Q.\nFeatures for creating Dashboards, reports, and Data QA using natural language. Coming soon to Vietnam: Amazon Agentic AI Workbench (Quick Suite) with Quick Researcher and Quick Automate capabilities, keeping humans in the loop for control. Key Takeaways Agent Mindset: Clearly understand the structure of an AI Agent: Goals → Observation → Tools → Context → Action. Agent Core Architecture: Ensure all components are present: Runtime, Gateway, Memory, Observability, and Identity to deploy Agents to production safely and scalably. Multi-layer Security: Security extends beyond the application; it must control risks from training data and fine-tuning processes down to end-users (Consumer risks). Applying to Work Implement AI-DLC: Pilot the 7-step AI-DLC process in a new project, starting with using AI to \u0026ldquo;Build Context\u0026rdquo; and \u0026ldquo;Domain Modeling\u0026rdquo;. Enhance Security: Review current GenAI applications against the OWASP Top 10 for LLM checklist and integrate Bedrock Guardrails to filter harmful content. Modernize Data Stack: Evaluate the feasibility of migrating the current Data Warehouse to a Lakehouse architecture with Apache Iceberg on AWS to break down Data Silos. Personal Experience This event went much deeper technically (deep-dive) than I expected, providing significant practical value:\nI was very impressed by the sharing on AI-DLC by Mr. Binh Tran. It completely changed my perspective on coding: developers are no longer just writing code but becoming \u0026ldquo;architects\u0026rdquo; and \u0026ldquo;reviewers\u0026rdquo; for AI execution. The Scoping Matrix slide and the risk layers in GenAI gave me a more systematic view to justify new AI project deployments to the company\u0026rsquo;s Security department. I am very excited about the news that Amazon Agentic AI Workbench is coming to Vietnam, promising to solve the problem of automating market research processes (Quick Researcher) that the business team currently needs. Event Photos Summary: A \u0026ldquo;must-attend\u0026rdquo; event for Builders. Knowledge about Agentic AI and AI-DLC will be the compass for my technical development roadmap in the coming year.\n"
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/",
	"title": "Internship Report",
	"tags": [],
	"description": "",
	"content": "Internship Report Student Information: Full Name: Le Minh Tuan\nPhone Number: 0981 500 154\nEmail: tuanlmse184475@fpt.edu.vn\nUniversity: FPT University HCMC\nMajor: Artificial Intelligence\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 09/12/2025\nReport Content Worklog Proposal Các bài blogs đã dịch Các events đã tham gia Workshop Tự đánh giá Chia sẻ, đóng góp ý kiến "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/5-workshop/5.1-workshop-overview/",
	"title": "Overview",
	"tags": [],
	"description": "",
	"content": "Building a Smart Contract Assistant on AWS Introduction The AI Contract Intelligence platform is a web service designed for individuals and small teams (freelancers, small business owners, administrative/legal personnel) who work with contracts daily but lack deep legal expertise. The solution utilizes Amazon Bedrock and a fully serverless AWS architecture to analyze contracts, highlight risks, suggest clause revisions, and generate summaries as well as new contract templates.\nBuilt on AWS Amplify, Lambda, API Gateway, DynamoDB, S3, Cognito, EventBridge, and CloudWatch, the platform provides AI-powered contract review capabilities with low latency, low cost, and high security. It is optimized for single users or small teams without the need for complex enterprise-grade features.\nThe primary goal of the application is to assist users (such as lawyers, compliance officers, or business owners) in performing complex tasks such as:\nInformation Retrieval: Q\u0026amp;A regarding legal clauses based on an available repository of legal texts. Automated Drafting: Requesting the AI to generate contract drafts based on standard templates and provided information. Analysis: Summarizing and auditing contract content. Solution Architecture The system is designed using an Event-driven Serverless architecture, optimizing costs (pay-per-use) and enabling automatic scalability. We will apply RAG (Retrieval-Augmented Generation) techniques to provide private data context (vector data) to the AI model, ensuring answers are more accurate and realistic. The core AWS service components include:\nAmazon Bedrock (Generative AI):\nActs as the brain for natural language processing. We will use Foundation Models such as Claude 3 (Haiku/Sonnet) via API to handle intent understanding, text generation, and legal logic processing. AWS Lambda \u0026amp; Amazon API Gateway (Backend):\nAWS Lambda: Runs Python functions to handle business logic, such as vector searches, calling Bedrock APIs, and data orchestration. Amazon API Gateway: Creates secure HTTP endpoints (RESTful APIs) for the Frontend to communicate with the Backend. Amazon S3 (Storage):\nStores static files like sample contract documents (.docx, .pdf). Stores the vector database (embeddings) of legal texts to serve Semantic Search features. Amazon DynamoDB (Database):\nStores user information (Users). Manages work sessions (Chat Sessions) and saves the entire chat history (Chat Messages) with minimal latency. AWS Amplify \u0026amp; Amazon Cognito (Frontend \u0026amp; Auth):\nAWS Amplify: Automates deployment (CI/CD) and hosting for the web application (React/Vue). Amazon Cognito: Manages user identity, allowing secure sign-up/sign-in and granting API access. Learning Objectives Upon completion of this workshop, you will achieve:\nA clear understanding of the process for building a GenAI application from Backend to Frontend. Hands-on experience working with Amazon Bedrock and RAG techniques. The ability to set up and configure Serverless services: Lambda, DynamoDB, API Gateway. Skills in deploying modern web applications using AWS Amplify. "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/1-worklog/1.1-week1/",
	"title": "Week 1 Worklog",
	"tags": [],
	"description": "",
	"content": "📘 Week 1 Worklog – AWS Journey 1. Weekly Objectives The primary goals for Week 1 were to establish the foundational environment for the AWS journey and understand the core operational principles. Key objectives included:\nOnboarding: Familiarize with FCJ internship processes, communication channels, and regulations. Account Setup: Complete AWS Free Tier registration, configure AWS CLI, and enable security baselines (MFA, IAM). Core Services: Gain an overview of the AWS ecosystem (Compute, Storage, Networking, Database, Security). Hands-on Practice: proficiently use the AWS Management Console \u0026amp; AWS CLI v2. Infrastructure: Deploy and operate an EC2 t2.micro instance and perform basic EBS operations. Cost Control: Establish AWS Budgets to monitor spending. 2. Detailed Work Summary 🗂 Implementation Plan vs. Actual Category Plan Actual Status Onboarding \u0026amp; Rules Introduction, grasp communication channels Introduced, noted report standards ✅ Done AWS Overview Systematize service groups + Mindmap Completed, categorized notes taken ✅ Done Free Tier \u0026amp; Security Create account, enable MFA, create IAM user Enabled MFA; created user + Viewer group ✅ Done AWS CLI Install CLI, configure profile Profile acj-student set, sts test OK ✅ Done EC2/EBS/SSH Create EC2, SSH, attach EBS EC2 t2.micro + EBS 8GB gp3, SSH successful ✅ Done Cost Management Set Budgets $5/month Received test email alert ✅ Done 📅 Daily Activities Log Day Task Start Date Completion Date Reference Material Monday Onboarding: FCJ orientation, read regulations, learn report standards 08/09 08/09 AWS Journey Tuesday Research: Explore AWS ecosystem (Compute/Storage/Networking/DB/Security), create mindmap 09/09 09/09 AWS Journey Wednesday Account Setup: Create AWS Free Tier, enable MFA for root, create IAM user + Viewer group 10/09 10/09 AWS Journey Thursday CLI Setup: Install AWS CLI v2 (Windows), run aws configure (profile acj-student), check sts identity 11/09 11/09 AWS Journey Friday Theory: Study EC2 (instance types, AMI, EBS, SG, Elastic IP) + Free Tier checklist 12/09 12/09 AWS Journey Saturday Practice: Create EC2 t2.micro (AL2023), create/use key pair (.pem), SSH; attach EBS 8GB, format \u0026amp; mount 13/09 13/09 AWS Journey 3. Results \u0026amp; Evidence 3.1 Resources Created IAM: 01 Daily working user (Group: Viewer), MFA enabled for root account. EC2: t2.micro (Free Tier), AMI: Amazon Linux 2023. Security Group: Inbound rule opening 22/tcp restricted to My IP only. EBS: 8GB gp3 volume, formatted (xfs) and mounted to /data. Budgets: Monthly budget set to $5 USD with email alerts. CLI Region: Defaulted to ap-southeast-1 (Singapore). 3.2 CLI Commands Executed aws sts get-caller-identity --profile acj-student aws ec2 describe-regions --profile acj-student --output table aws ec2 describe-instances --profile acj-student --region ap-southeast-1 aws ec2 create-key-pair --key-name fcj-key --query \u0026#34;KeyMaterial\u0026#34; --output text \u0026gt; fcj-key.pem 3.3 Evidence Captured Screenshots saved: MFA enabled status, IAM user \u0026amp; group configuration, Budgets alert email, EC2 instance details, EBS volume attachment, and mount point verification. 4. Issues \u0026amp; Troubleshooting Issue Cause Resolution Result SSH Connection Failed Security Group did not authorize the correct IP Updated inbound rule -\u0026gt; My IP SSH OK CLI Missing Credentials Wrong profile or region selected Standardized profile acj-student, set default region ap-southeast-1 CLI OK Console Navigation Difficulty Unfamiliar User Interface Used Search bar + Pinned frequent services (EC2, IAM, S3, Budgets) Faster navigation 5. Key Takeaways Security First: Always adhere to the Least-privilege principle; use IAM users/roles instead of the Root account for daily tasks. Operations: Mastered the standard procedure for EC2 creation and EBS attach/format/mount lifecycle. Management: Understood the synchronization between Console ↔ CLI and the importance of Tagging (Project=FCJ, Owner=The Liems, Env=Dev) for resource tracking. Cost: Proactively using Budgets is essential to avoid billing surprises. 6. Cost \u0026amp; Security Summary Budgets: Limit set to $5 USD/month (Test email received). Security: Root MFA Enabled. .pem key stored securely. SSH access restricted to specific IP. Tagging Strategy: Project=FCJ, Owner=The Liems, Env=Dev. 7. Risks \u0026amp; Mitigation Strategies Risk: Exceeding Free Tier limits by forgetting to terminate resources. Mitigation: Set AWS Budgets + Perform weekly tag reviews to clean up resources. Risk: Leaking Key Pairs/Credentials. Mitigation: Store keys in a secure local directory, add to .gitignore, never commit to public repositories. "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/1-worklog/1.2-week2/",
	"title": "Week 2 Worklog",
	"tags": [],
	"description": "",
	"content": "📘 Week 2 Worklog – AWS Journey 1. Weekly Objectives During Week 2, the primary goal was to gain foundational hands-on experience with core AWS infrastructure services, including:\nAmazon S3 – Hosting a static website and managing bucket permissions. Amazon RDS (MySQL) – Provisioning a managed relational database and configuring connectivity. Amazon EC2 – Using an EC2 instance as a secure bastion host to access RDS. Amazon Route53 – Managing domains and mapping DNS records to AWS services. This week focuses on building fundamental cloud architecture components that serve as prerequisites for Week 3 tasks involving CloudFront, DynamoDB, and ElastiCache.\n2. Detailed Work Summary 🗂 Table of Activities Day Tasks Start Date Completion Date Reference Monday - Created an S3 bucket for static website content\n- Uploaded initial HTML/CSS demo files 15/09/2025 15/09/2025 AWS Journey Tuesday - Enabled Static Website Hosting on S3\n- Configured Bucket Policy to allow public read access\n- Tested website accessibility via S3 endpoint 16/09/2025 16/09/2025 AWS Journey Wednesday - Created an RDS MySQL instance (Free Tier)\n- Configured VPC Security Groups for inbound traffic\n- Recorded DB endpoint \u0026amp; credentials 17/09/2025 17/09/2025 AWS Journey Thursday - Launched an EC2 instance and installed MySQL client\n- Connected from EC2 → RDS using command line\n- Executed test queries and created sample tables 18/09/2025 18/09/2025 AWS Journey Friday - Explored Route53 functionality\n- Created a Hosted Zone and DNS records (A/CNAME)\n- Configured routing from custom domain → S3 static site\n- Validated website accessibility using domain name 19/09/2025 19/09/2025 AWS Journey 3. Technical Implementation Details 3.1 AWS S3 – Static Website Setup Created a new S3 bucket following naming conventions and regional placement. Uploaded static assets (HTML/CSS/Images). Enabled the Static Website Hosting feature. Configured index.html and error.html. Added a public-read Bucket Policy to serve content globally. Verified accessibility through the website endpoint: http://\u0026lt;bucket-name\u0026gt;.s3-website-\u0026lt;region\u0026gt;.amazonaws.com 3.2 Amazon RDS – Database Provisioning Launched RDS MySQL 8.0 instance under Free Tier. Applied secure Security Group rules (EC2 → RDS, port 3306). Stored the generated endpoint for later connection. Ensured DB subnet group and VPC configuration were valid for private access. 3.3 Amazon EC2 – Secure DB Connectivity Created a t2.micro EC2 instance in the same VPC as the RDS instance. Installed MySQL Client: sudo yum install mysql -y Successfully connected to RDS: mysql -h \u0026lt;rds-endpoint\u0026gt; -u admin -p Created a sample database and table for validation purposes. 3.4 Amazon Route53 – DNS Configuration Set up a new Hosted Zone. Added DNS records: A Record → S3 static website. CNAME Record for testing aliases. Waited for DNS propagation (typically 1–5 minutes). Successfully accessed the static site using a custom domain. 4. Achievements By the end of Week 2, the following outcomes were accomplished:\n✔ Functional Successes Completed a fully operational S3-hosted static website. Enabled access through both S3 website endpoint and Route53 domain. Successfully deployed and connected an RDS MySQL database. Verified secure communication between EC2 ↔ RDS. ✔ Skill Development Demonstrated understanding of: IAM roles \u0026amp; permissions. S3 access control. VPC networking \u0026amp; Security Groups. DNS routing concepts. Gained foundational experience with AWS core services. Strengthened understanding of end-to-end cloud application flow. Built confidence working with CLI-based operations (EC2 → RDS). Improved troubleshooting skills (DNS propagation, SG configuration, and public access policies). 5. Challenges Encountered \u0026amp; Resolutions Challenge 1: S3 Public Access Block\nIssue: Website not accessible due to default S3 public access block. Fix: Disabled “Block Public Access” settings and added a correct bucket policy. Challenge 2: EC2 -\u0026gt; RDS Connection Timeout\nIssue: Security Group did not allow inbound MySQL traffic. Fix: Modified RDS Security Group to accept traffic specifically from the EC2 Security Group on port 3306. Challenge 3: DNS Not Resolving Immediately\nIssue: Domain took time to propagate in Route53. Fix: Waited for TTL window and revalidated using dig / nslookup. "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/1-worklog/1.3-week3/",
	"title": "Week 3 Worklog",
	"tags": [],
	"description": "",
	"content": "📘 Week 3 Worklog – AWS Journey 1. Weekly Objectives During Week 3, the primary goal was to optimize application performance and expand data management skills beyond relational databases. Key objectives included:\nAmazon CloudFront – Understanding Content Delivery Networks (CDN) and optimizing static site delivery. Amazon DynamoDB – gaining hands-on experience with NoSQL database modeling and operations. Amazon ElastiCache (Redis) – Implementing in-memory caching to improve read performance. AWS CLI Integration – Advanced interaction with AWS services using command-line scripts. This week focuses on shifting from a basic architecture to a high-performance, scalable model using caching layers and managed NoSQL services.\n2. Detailed Work Summary 🗂 Table of Activities Day Tasks Start Date Completion Date Reference Monday - Introduction to CDN concepts and CloudFront benefits\n- Created CloudFront Distribution to serve S3 website content 22/09/2025 22/09/2025 AWS Journey Tuesday - Configured CloudFront behaviors and cache policies\n- Tested website access via CloudFront URL\n- Performed Cache Invalidation to update content 23/09/2025 23/09/2025 AWS Journey Wednesday - Introduction to DynamoDB (NoSQL Architecture)\n- Created DynamoDB tables (Users, Products)\n- Practiced CRUD operations via AWS Console 24/09/2025 24/09/2025 AWS Journey Thursday - Connected and queried DynamoDB using AWS CLI\n- Wrote scripts to put-item and scan data programmatically 25/09/2025 25/09/2025 AWS Journey Friday - Explored ElastiCache (Redis \u0026amp; Memcached)\n- Provisioned a basic Redis cluster\n- Tested connection from EC2 to store/read cache keys 26/09/2025 26/09/2025 AWS Journey 3. Technical Implementation Details 3.1 Amazon CloudFront – CDN Integration Created a distribution pointing to the S3 bucket created in Week 2. Configured Origin Access Control (OAC) to restrict S3 access to CloudFront only. Enabled HTTPS using the default CloudFront certificate. Tested performance improvement (reduced latency) compared to direct S3 access. Executed manual invalidation for index.html: aws cloudfront create-invalidation --distribution-id \u0026lt;ID\u0026gt; --paths \u0026#34;/*\u0026#34; 3.2 Amazon DynamoDB – NoSQL Implementation Created a Users table with UserId as the Partition Key. Performed CRUD operations (Create, Read, Update, Delete) via the Management Console. Interacted via AWS CLI to insert data: aws dynamodb put-item \\ --table-name Users \\ --item \u0026#39;{\u0026#34;UserId\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;u-101\u0026#34;}, \u0026#34;Name\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;Alice\u0026#34;}, \u0026#34;Role\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;Admin\u0026#34;}}\u0026#39; Validated data insertion using the scan command. 3.3 Amazon ElastiCache – Redis Setup Launched an ElastiCache for Redis cluster (cache.t2.micro/t3.micro). Configured Security Groups to allow inbound traffic on port 6379 from the EC2 instance. Connected from EC2 using redis-cli (installed via amazon-linux-extras or yum). Tested caching logic: set mykey \u0026#34;Hello AWS\u0026#34; get mykey # Output: \u0026#34;Hello AWS\u0026#34; 4. Achievements By the end of Week 3, the following outcomes were accomplished:\n✔ Functional Successes Successfully accelerated the S3 static website using CloudFront globally. Demonstrated working knowledge of NoSQL data structures. Established a functional Redis cache cluster accessible from private VPC resources. Integrated AWS CLI for database management, moving beyond Console-only operations. ✔ Skill Development Understood the role of edge locations and caching strategies. Mastered the difference between Relational (RDS) and NoSQL (DynamoDB) models. Learned to perform Cache Invalidation when updating static content. Gained experience in securing internal cache layers (ElastiCache) via Security Groups. 5. Challenges Encountered \u0026amp; Resolutions Challenge 1: CloudFront Content Not Updating\nIssue: Updates to index.html on S3 were not reflecting immediately on the website. Fix: Learned about TTL (Time To Live) and performed a CloudFront Invalidation to force a refresh. Challenge 2: DynamoDB CLI Syntax Complexity\nIssue: Difficulty formatting JSON payloads correctly for CLI commands. Fix: Used a JSON generator tool and referred to AWS CLI documentation for correct AttributeValue syntax (S, N, etc.). Challenge 3: Connecting to Redis from Local Machine\nIssue: Attempted to connect to ElastiCache from outside the VPC (failed). Fix: Understood that ElastiCache is VPC-internal only; utilized the EC2 bastion host established in Week 2 as a jump box. "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/1-worklog/1.4-week4/",
	"title": "Week 4 Worklog",
	"tags": [],
	"description": "",
	"content": "📘 Week 4 Worklog – AWS Journey 1. Weekly Objectives During Week 4, the primary focus shifted towards Migration strategies and Business Continuity. The goal was to understand how to move on-premise workloads to the cloud and ensure system resilience against failures. Key objectives included:\nMigration Processes – Understanding the \u0026ldquo;6 Rs\u0026rdquo; of migration (Rehost, Replatform, Refactor, etc.). AWS Database Migration Service (DMS) – Moving data from a source database to Amazon RDS with minimal downtime. Elastic Disaster Recovery (EDR) – Implementing replication and recovery strategies to minimize data loss. Disaster Recovery (DR) Planning – Defining RTO (Recovery Time Objective) and RPO (Recovery Point Objective). This week establishes the critical skills needed for enterprise-grade infrastructure reliability and modernization.\n2. Detailed Work Summary 🗂 Table of Activities Day Tasks Start Date Completion Date Reference Monday - Learn Migration concepts (Lift \u0026amp; Shift, Replatform, Refactor)\n- Introduction to AWS Database Migration Service (DMS) 29/09/2025 29/09/2025 AWS Journey Tuesday - Practice creating Replication Instance in DMS\n- Configure source data (simulated on-prem) and target (RDS)\n- Perform test data migration 30/09/2025 30/09/2025 AWS Journey Wednesday - Introduction to Elastic Disaster Recovery (EDR)\n- Learn how to set up replication server and recovery instance 01/10/2025 01/10/2025 AWS Journey Thursday - Practice simulating failures: shut down main EC2 and start recovery instance from EDR\n- Evaluate recovery time (RTO/RPO) 02/10/2025 02/10/2025 AWS Journey Friday - Create basic Disaster Recovery plan (backup, restore, failover)\n- Write documentation summarizing Migration + DR processes 03/10/2025 03/10/2025 AWS Journey 3. Technical Implementation Details 3.1 AWS Database Migration Service (DMS) Replication Instance: Provisioned a dms.t2.micro instance in the VPC to handle the migration workload. Endpoints Configuration: Source: Configured an EC2-hosted MySQL database (simulating on-premise) with public access. Target: Connected to the RDS MySQL instance created in Week 2. Migration Task: Created a \u0026ldquo;Full Load\u0026rdquo; task to migrate existing tables. Mapping Rules: Configured schema selection rules to include specific tables (e.g., Users, Products). 3.2 Elastic Disaster Recovery (EDR) Setup Initialized the EDR service in the specific AWS Region. Agent Installation: Downloaded and installed the AWS Replication Agent on the source EC2 instance (Linux). Staging Area: Verified that the replication server was automatically launched in the staging subnet. Data Replication: Monitored the initial sync progress until the status reached \u0026ldquo;Healthy\u0026rdquo; and \u0026ldquo;Data replicated\u0026rdquo;. 3.3 Failover Simulation (Drill) Scenario: Simulating a critical failure by stopping the Source EC2 instance. Recovery Action: Initiated a \u0026ldquo;Recovery Drill\u0026rdquo; in the EDR console. Launch Settings: Configured the launch template (instance type, security groups) for the recovery instance. Validation: Successfully SSH\u0026rsquo;d into the launched Recovery Instance and verified application data integrity. 3.4 DR Planning \u0026amp; Documentation Drafted a basic DR plan outlining: Backup Strategy: Automated snapshots vs. Continuous Replication. Failover Steps: The sequence of actions to switch to the recovery site. RTO/RPO Analysis: Measured how long the recovery took (RTO) and how much data was potential lag (RPO). 4. Achievements By the end of Week 4, the following outcomes were accomplished:\n✔ Functional Successes Successfully migrated data between two database endpoints using AWS DMS. Configured continuous block-level replication using Elastic Disaster Recovery. Executed a successful failover drill, bringing up a recovery server within minutes. Verified data consistency between Source and Target systems. ✔ Skill Development Understood the complete Migration lifecycle (Assess → Mobilize → Migrate \u0026amp; Modernize). Gained practical experience with Hybrid Cloud networking concepts (Source → AWS). Deepened knowledge of Business Continuity Planning (BCP). Learned to differentiate between Backup strategies and Disaster Recovery solutions. 5. Challenges Encountered \u0026amp; Resolutions Challenge 1: DMS Connection Errors\nIssue: The Replication Instance could not connect to the Source EC2 database. Fix: Updated the Source Security Group to allow inbound traffic on port 3306 specifically from the Private IP of the DMS Replication Instance. Challenge 2: EDR Agent Installation Failure\nIssue: The replication agent failed to install due to missing IAM permissions. Fix: Created an IAM user with the specific programmatic access keys required for the AWS Replication Agent and re-ran the installer. Challenge 3: High RTO during Drill\nIssue: The recovery instance took longer than expected to become available. Fix: Optimized the Launch Template to use a pre-warmed AMI or appropriate instance type to speed up the boot process. "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/1-worklog/1.5-week5/",
	"title": "Week 5 Worklog",
	"tags": [],
	"description": "",
	"content": "📘 Week 5 Worklog – AWS Journey 1. Weekly Objectives During Week 5, the focus shifted from manual operations (\u0026ldquo;ClickOps\u0026rdquo;) to Infrastructure as Code (IaC) and Systems Operations. The goal was to automate resource provisioning and management to ensure consistency and speed. Key objectives included:\nInfrastructure as Code (IaC) – Learning AWS CloudFormation and AWS Cloud Development Kit (CDK). Automation – Writing templates and code to deploy S3 buckets and EC2 instances programmatically. AWS Systems Manager (SSM) – Centralizing operational data and managing instances without SSH keys. Operational Excellence – Implementing automated start/stop workflows for cost saving. This week marks the transition towards DevOps practices, preparing for scalable infrastructure management.\n2. Detailed Work Summary 🗂 Table of Activities Day Tasks Start Date Completion Date Reference Monday - Introduction to IaC concepts and benefits compared to manual deployment\n- Get familiar with AWS CloudFormation: template, stack, parameter 06/10/2025 06/10/2025 AWS Journey Tuesday - Write CloudFormation template to deploy S3 bucket and EC2 instance\n- Create, update, and delete stack via AWS Console 07/10/2025 07/10/2025 AWS Journey Wednesday - Introduction to AWS CDK (Cloud Development Kit)\n- Install AWS CDK, create CDK project using Python or TypeScript\n- Write CDK code to deploy EC2 instance 08/10/2025 08/10/2025 AWS Journey Thursday - Introduction to AWS Systems Manager (SSM) and key features\n- Create Parameter Store to store configuration variables 09/10/2025 09/10/2025 AWS Journey Friday - Practice creating Automation Document in SSM to automatically start/stop EC2\n- Test Session Manager (access EC2 without SSH key)\n- Week summary: IaC + SSM demo 10/10/2025 10/10/2025 AWS Journey 3. Technical Implementation Details 3.1 AWS CloudFormation Template Design: Created a YAML template defining an AWS::S3::Bucket and AWS::EC2::Instance. Parameters: Used Parameters to allow inputting InstanceType (e.g., t2.micro) at deployment time. Stack Operations: Create Stack: Uploaded the template to CloudFormation Designer. Update Stack: Modified the template (added tags) and applied a changeset. Drift Detection: Checked if resources were modified manually outside the stack. 3.2 AWS CDK (Cloud Development Kit) Setup: Installed Node.js and the CDK CLI (npm install -g aws-cdk). Initialization: Created a new project: cdk init app --language python. Coding: Defined resources using high-level constructs (L2 constructs) in Python. Deployment: cdk synth: Generated the CloudFormation template. cdk deploy: Provisioned resources to the AWS account. 3.3 AWS Systems Manager (SSM) Parameter Store: Created hierarchical parameters (e.g., /dev/db/password) as SecureString to store sensitive config. Session Manager: Attached the AmazonSSMManagedInstanceCore IAM role to the EC2 instance. Successfully connected to the instance shell via the AWS Console (browser) without opening port 22 (SSH). Automation: Executed an SSM Document (AWS-StopEC2Instance) to test automated operational tasks. 4. Achievements By the end of Week 5, the following outcomes were accomplished:\n✔ Functional Successes Successfully replaced manual resource creation with repeatable CloudFormation templates. Deployed a functional infrastructure stack using imperative code (CDK). Eliminated the need for SSH keys management using SSM Session Manager. Centralized configuration management using SSM Parameter Store. ✔ Skill Development Understood the difference between Declarative (CloudFormation) and Imperative (CDK) IaC approaches. Learned the importance of Idempotency in infrastructure deployment. Gained experience in Agent-based management (SSM Agent). Improved security posture by removing the need for public SSH access. 5. Challenges Encountered \u0026amp; Resolutions Challenge 1: CloudFormation YAML Indentation\nIssue: Stack creation failed due to parsing errors in the YAML file. Fix: Used a YAML Linter and VS Code extension \u0026ldquo;CloudFormation Linter\u0026rdquo; to validate syntax before uploading. Challenge 2: CDK Bootstrapping\nIssue: cdk deploy failed with an error about missing toolkit stack. Fix: Learned that the environment must be bootstrapped once per region using cdk bootstrap aws://\u0026lt;account-id\u0026gt;/\u0026lt;region\u0026gt;. Challenge 3: SSM Agent Not Connecting\nIssue: EC2 instance did not appear in the Systems Manager Fleet Manager. Fix: Discovered that the EC2 instance was missing the necessary IAM Role (AmazonSSMManagedInstanceCore). Attached the role and rebooted the instance. "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/1-worklog/1.6-week6/",
	"title": "Week 6 Worklog",
	"tags": [],
	"description": "",
	"content": "📘 Week 6 Worklog – AWS Journey 1. Weekly Objectives During Week 6, the focus shifted to the critical pillars of the Well-Architected Framework: Security and Cost Optimization. Key objectives included:\nIdentity \u0026amp; Access Management (IAM) – Mastering advanced policy structures to enforce the Principle of Least Privilege. Data Security – Implementing encryption using AWS KMS and managing credentials with Secrets Manager. Cost Management – Analyzing spending patterns via Cost Explorer and setting up automated alerts with AWS Budgets. This week ensures that the infrastructure built in previous weeks is not only functional but also secure and financially efficient.\n2. Detailed Work Summary 🗂 Table of Activities Day Tasks Start Date Completion Date Reference Monday - Review basic IAM knowledge\n- Learn advanced IAM Policy (JSON structure, Conditions)\n- Create custom policies and attach to users/groups 13/10/2025 13/10/2025 AWS Journey Tuesday - Introduction to AWS Key Management Service (KMS)\n- Create a Customer Managed Key (CMK)\n- Apply KMS to encrypt an S3 bucket or an EBS volume 14/10/2025 14/10/2025 AWS Journey Wednesday - Get familiar with AWS Secrets Manager\n- Create a secret to store Database connection info\n- Write a small Lambda script to read the secret 15/10/2025 15/10/2025 AWS Journey Thursday - Explore AWS Billing Dashboard and Cost Explorer\n- View costs by service, region, and usage type\n- Set up Cost Anomaly Detection 16/10/2025 16/10/2025 AWS Journey Friday - Create an AWS Budget and configure email alerts\n- Write a weekly cost summary report with optimization proposals (stop EC2, cleanup EBS)\n- Wrap up Week 6 learnings 17/10/2025 17/10/2025 AWS Journey 3. Technical Implementation Details 3.1 Advanced IAM Policies Analyzed the JSON structure: Version, Statement, Effect, Action, Resource. Created a Condition-based policy to restrict access based on source IP or tags: { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:*\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::my-secure-bucket/*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;IpAddress\u0026#34;: {\u0026#34;aws:SourceIp\u0026#34;: \u0026#34;203.0.113.0/24\u0026#34;} } } Attached inline policies to specific IAM Groups to enforce separation of duties. 3.2 Data Encryption with AWS KMS Created a Customer Managed Key (CMK) (Symmetric). Configured Key Policy to define Key Administrators and Key Users. Enabled default encryption on an S3 bucket using the new CMK. Tested manual encryption via CLI: aws kms encrypt --key-id \u0026lt;key-id\u0026gt; --plaintext fileb://data.txt --output text --query CiphertextBlob 3.3 AWS Secrets Manager Integration Stored RDS credentials (username/password) securely in Secrets Manager. Configured automatic rotation settings (explored concept). Developed a Python (Boto3) script for Lambda to retrieve the secret programmatically, avoiding hardcoded credentials in code. 3.4 Cost Management \u0026amp; Optimization Cost Explorer: Activated tags (e.g., Project: WebApp) to filter costs by specific workloads. AWS Budgets: Set up a monthly budget of $10.00 with an alert triggering at 80% usage ($8.00). Anomaly Detection: Enabled AWS Cost Anomaly Detection to identify spikes in service usage (e.g., unintended Lambda loops). 4. Achievements By the end of Week 6, the following outcomes were accomplished:\n✔ Functional Successes Mastered creating and applying granular IAM Policies to strictly control resource access. Successfully implemented encryption at rest for S3 and EBS using KMS. Replaced hardcoded database credentials with dynamic retrieval from Secrets Manager. Established a cost governance framework using Budgets and Alerts. ✔ Skill Development Deepened understanding of Shared Responsibility Model regarding security. Learned to balance security stringency with operational usability. Gained \u0026ldquo;FinOps\u0026rdquo; awareness: how to analyze costs, propose optimization measures (e.g., stopping idle EC2, deleting unattached EBS), and maintain efficiency. 5. Challenges Encountered \u0026amp; Resolutions Challenge 1: Access Denied due to KMS Key Policy\nIssue: An IAM user with Admin permissions could not decrypt a file. Fix: Learned that KMS Key Policies are separate from IAM policies. Added the user ARN to the \u0026ldquo;Key Users\u0026rdquo; section of the KMS policy. Challenge 2: Secrets Manager Caching\nIssue: Frequent calls to Secrets Manager increased costs/latency. Fix: Implemented caching in the Lambda function code to store the secret temporarily during the execution context. Challenge 3: Interpreting Cost Explorer Data\nIssue: Costs for \u0026ldquo;EC2-Other\u0026rdquo; were high and unclear. Fix: Drilled down by \u0026ldquo;Usage Type\u0026rdquo; to identify that the costs were from NAT Gateway data transfer, leading to an architectural review. "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/1-worklog/1.7-week7/",
	"title": "Week 7 Worklog",
	"tags": [],
	"description": "",
	"content": "📘 Week 7 Worklog – AWS Journey 1. Weekly Objectives During Week 7, the focus was on building High Availability (HA) and Scalability into the architecture. Key objectives included:\nAuto Scaling \u0026amp; Load Balancing – Configuring systems to handle varying traffic loads automatically using ASG and ALB. Decoupling Architecture – Using SQS and SNS to enable asynchronous communication between microservices. Network Monitoring – Enhancing observability by capturing and analyzing network traffic with VPC Flow Logs. This week transforms a static infrastructure into a dynamic, resilient system capable of self-healing and scaling based on demand.\n2. Detailed Work Summary 🗂 Table of Activities Day Tasks Start Date Completion Date Reference Monday - Learn about High Availability, Fault Tolerance, and Elasticity concepts\n- Introduction to Auto Scaling Group (ASG) and Elastic Load Balancer (ELB) 20/10/2025 20/10/2025 AWS Journey Tuesday - Practice creating Auto Scaling Group for EC2 instance\n- Set up launch template, scaling policy, and target tracking 21/10/2025 21/10/2025 AWS Journey Wednesday - Create and configure Application Load Balancer (ALB)\n- Connect ALB with ASG for load distribution\n- Test website access through ALB DNS 22/10/2025 22/10/2025 AWS Journey Thursday - Get familiar with Amazon SQS and SNS services\n- Create SQS queue, SNS topic, and subscription\n- Send and receive notifications between components 23/10/2025 23/10/2025 AWS Journey Friday - Enable VPC Flow Logs to monitor network traffic\n- Analyze logs in CloudWatch Logs\n- Summarize knowledge about reliability \u0026amp; scaling 24/10/2025 24/10/2025 AWS Journey 3. Technical Implementation Details 3.1 Auto Scaling Group (ASG) Launch Template: Created a template defining the AMI, Instance Type (t2.micro), and Security Groups. Scaling Policies: Implemented Target Tracking Scaling Policy to maintain average CPU utilization at 50%. Capacity: Configured Min: 2, Max: 4, Desired: 2 to ensure high availability across multiple Availability Zones. 3.2 Application Load Balancer (ALB) Target Group: Created a target group for HTTP (Port 80) traffic. Listener Rules: Configured the ALB to listen on HTTP and forward traffic to the ASG Target Group. Health Checks: Configured /index.html as the health check path to ensure only healthy instances receive traffic. DNS: Validated access using the auto-generated ALB DNS name (my-loadbalancer-123.region.elb.amazonaws.com). 3.3 Decoupling with SQS \u0026amp; SNS SNS (Simple Notification Service): Created a Topic (OrderAlerts) and subscribed an email address for notifications. SQS (Simple Queue Service): Created a Standard Queue (OrderQueue). Fan-out Pattern: Subscribed the SQS queue to the SNS topic. Published a message to SNS and verified it appeared in both the Email inbox and the SQS queue polling. 3.4 VPC Flow Logs \u0026amp; Monitoring Setup: Enabled Flow Logs for the VPC, sending data to CloudWatch Logs. Analysis: Used CloudWatch Log Insights to query traffic: Identified rejected traffic (Security Group blocks). Monitored SSH connection attempts. Analyzed internal traffic flow between subnets. 4. Achievements By the end of Week 7, the following outcomes were accomplished:\n✔ Functional Successes Understood the High Availability model and how to maintain system uptime during failures. Successfully implemented Auto Scaling Group + Load Balancer to automatically scale EC2 capacity. Configured SQS/SNS for reliable message queuing and fan-out notifications. Enabled and interpreted VPC Flow Logs to audit network security and connectivity. ✔ Skill Development Mastered the relationship between Load Balancers and Auto Scaling Groups. Learned to simulate load (using stress tool) to trigger scaling events. Gained experience in \u0026ldquo;Loose Coupling\u0026rdquo; architecture design. Improved network troubleshooting skills using log analysis. 5. Challenges Encountered \u0026amp; Resolutions Challenge 1: Unhealthy Targets in ALB\nIssue: EC2 instances in the Target Group showed status \u0026ldquo;Unhealthy\u0026rdquo;. Fix: The Security Group on the EC2 instances did not allow traffic from the Load Balancer\u0026rsquo;s Security Group. Updated the Inbound Rules to allow HTTP from the ALB SG. Challenge 2: ASG Not Scaling Down\nIssue: After load testing, instances remained running longer than expected. Fix: Understood the Cooldown Period concept. The ASG was waiting for the default cooldown (300 seconds) before terminating instances to prevent \u0026ldquo;thrashing\u0026rdquo;. Challenge 3: SQS Message Visibility\nIssue: Messages were processed but reappeared in the queue. Fix: Adjusted the Visibility Timeout to match the processing time of the consumer application. "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/1-worklog/1.8-week8/",
	"title": "Week 8 Worklog",
	"tags": [],
	"description": "",
	"content": "📘 Week 8 Worklog – AWS Journey 1. Weekly Objectives Week 8 served as a comprehensive review and consolidation period. The primary goal was to synthesize all knowledge gained over the past 7 weeks through the lens of the AWS Well-Architected Framework. Key objectives included:\nWell-Architected Framework: Deep understanding of the 5 pillars (Operational Excellence, Security, Reliability, Performance Efficiency, Cost Optimization). Architecture Consolidation: Reviewing best practices for Security (IAM, KMS), Resilience (Multi-AZ, DR), and Optimization. Holistic Design: Designing a complete infrastructure integrating core services (EC2, S3, RDS, VPC, Lambda, CloudFront) and evaluating it against AWS standards. This week transitions from learning individual services to designing robust, production-ready systems.\n2. Detailed Work Summary 🗂 Table of Activities Day Tasks Start Date Completion Date Reference Monday - Overview of AWS Well-Architected Framework \u0026amp; 5 Pillars\n- Identify the role and importance of each pillar in system design 27/10/2025 27/10/2025 AWS Journey Tuesday - Review Secure Architecture Design\n- Deep dive: IAM, MFA, SCP, KMS, WAF, Shield, GuardDuty, Security Groups vs NACLs 28/10/2025 28/10/2025 AWS Journey Wednesday - Review Resilient Architecture Design\n- Topics: Multi-AZ, Multi-Region, DR Strategies, Route 53, Backup \u0026amp; Restore 29/10/2025 29/10/2025 AWS Journey Thursday - Review Performance and Cost Optimization\n- Topics: Auto Scaling, Global Accelerator, S3 Tiering, Savings Plans 30/10/2025 30/10/2025 AWS Journey Friday - Comprehensive Practice: Build a sample full-stack architecture\n- Evaluate according to 5 Well-Architected Framework criteria\n- Write weekly summary report 31/10/2025 31/10/2025 AWS Journey 3. Technical Implementation Details 3.1 Security Architecture Review (The Security Pillar) Defense in Depth: Designed a multi-layer security model: Edge: AWS WAF \u0026amp; Shield (DDoS protection). VPC: Public/Private subnets, NACLs (Stateless), Security Groups (Stateful). Identity: IAM Users with MFA, Roles for service access, Principle of Least Privilege. Data: KMS for encryption at rest (EBS/S3/RDS), TLS/ACM for encryption in transit. 3.2 Reliability \u0026amp; Resilience (The Reliability Pillar) High Availability (HA): Architected a Multi-AZ deployment for EC2 (via ASG) and RDS (Primary/Standby). Disaster Recovery (DR): Reviewed the 4 DR strategies: Backup \u0026amp; Restore (Cheapest, highest RTO). Pilot Light. Warm Standby. Multi-Site Active/Active (Most expensive, lowest RTO). 3.3 Performance \u0026amp; Cost (Efficiency \u0026amp; Optimization Pillars) Performance: Implemented CloudFront for edge caching and Global Accelerator for optimized routing to minimize latency. Cost: Analyzed S3 Lifecycle Policies (Standard -\u0026gt; IA -\u0026gt; Glacier) to automate storage savings. Evaluated Compute Savings Plans vs. Reserved Instances for long-term workloads. Used AWS Cost Explorer to identify \u0026ldquo;zombie\u0026rdquo; resources (unattached EIPs, idle ELBs). 3.4 Capstone Architecture Practice Designed a 3-Tier Web Application: Presentation Tier: CloudFront + S3 (Static assets) / ALB (Dynamic requests). Logic Tier: EC2 Auto Scaling Group in Private Subnets. Data Tier: RDS Multi-AZ + DynamoDB. Conducted a self-assessment using the AWS Well-Architected Tool to identify risks (High/Medium Risk Issues). 4. Achievements By the end of Week 8, the following outcomes were accomplished:\n✔ Conceptual Mastery Gained deep understanding and systematized knowledge of the AWS Well-Architected Framework. Able to articulate trade-offs between Cost, Performance, and Reliability. ✔ Technical Consolidation Consolidated 4 core architecture domains: Security, Resilience, Performance, and Cost Optimization. Mastered the interaction between core services (e.g., how CloudWatch triggers Lambda for remediation). ✔ Practical Application Practiced designing a complete, industry-standard infrastructure from scratch. Learned to perform self-evaluation and architectural reviews. 5. Challenges Encountered \u0026amp; Resolutions Challenge 1: Trade-off Analysis\nIssue: Difficulty choosing between \u0026ldquo;Maximum Performance\u0026rdquo; and \u0026ldquo;Lowest Cost\u0026rdquo; (e.g., DynamoDB Provisioned vs. On-Demand). Resolution: Used the Well-Architected Framework to prioritize business requirements (e.g., if traffic is unpredictable, On-Demand is better despite potentially higher unit cost). Challenge 2: Complexity of DR Strategies\nIssue: Confusing the nuances between \u0026ldquo;Pilot Light\u0026rdquo; and \u0026ldquo;Warm Standby\u0026rdquo;. Resolution: Created a comparison matrix focusing on RTO/RPO targets and active resource count to distinguish them clearly. Challenge 3: Security Group vs. NACL Conflicts\nIssue: Troubleshooting connectivity issues when NACLs blocked return traffic (ephemeral ports). Resolution: Reinforced the understanding that NACLs are stateless and require explicit allow rules for both inbound and outbound traffic. "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/1-worklog/1.9-week9/",
	"title": "Week 9 Worklog",
	"tags": [],
	"description": "",
	"content": "📘 Week 9 Worklog – AWS Journey 1. Weekly Objectives During Week 9, the focus expanded into the Data \u0026amp; Analytics domain. The primary goal was to understand the end-to-end data pipeline on AWS, from ingestion to visualization. Key objectives included:\nData Lake Architecture – Building a scalable storage repository using Amazon S3. ETL \u0026amp; Data Cataloging – Using AWS Glue to discover and catalog metadata. Serverless Querying – Analyzing data directly in S3 using Amazon Athena (SQL). Business Intelligence (BI) – Visualizing insights using Amazon QuickSight. This week demonstrates how to turn raw data into actionable business intelligence using serverless analytics tools.\n2. Detailed Work Summary 🗂 Table of Activities Day Tasks Start Date Completion Date Reference Monday - Introduction to Data \u0026amp; Analytics ecosystem on AWS\n- Understand Data Lake concepts, ETL pipeline, and how to connect data sources 03/11/2025 03/11/2025 AWS Journey Tuesday - Create Data Lake on Amazon S3\n- Configure directory structure, access permissions\n- Set up AWS Glue Crawler to identify data schema 04/11/2025 04/11/2025 AWS Journey Wednesday - Practice AWS Athena to query data in Data Lake\n- Write basic SQL queries and export results to S3 05/11/2025 05/11/2025 AWS Journey Thursday - Introduction and practice with Amazon QuickSight\n- Connect QuickSight with Athena\n- Create simple dashboard with charts and tables 06/11/2025 06/11/2025 AWS Journey Friday - Review \u0026amp; consolidate weekly knowledge (Data collection → processing → analysis)\n- Compare Glue/Athena vs traditional tools\n- Write summary report 07/11/2025 07/11/2025 AWS Journey 3. Technical Implementation Details 3.1 Data Lake Setup (Amazon S3) Storage Strategy: Created an S3 bucket with a logical folder structure (e.g., raw-data/, processed-data/). Data Ingestion: Uploaded sample datasets (CSV/JSON files representing sales logs) to the raw-data folder. Security: Applied \u0026ldquo;Block Public Access\u0026rdquo; and ensured IAM roles were ready for Glue and Athena access. 3.2 Metadata Discovery (AWS Glue) Glue Crawler: Configured a Crawler to scan the S3 bucket. IAM Role: Created a service role granting Glue permissions to read S3 and write to the Glue Data Catalog. Data Catalog: Successfully ran the crawler, which automatically detected the schema (columns, data types) and created a table definition in the Glue Database. 3.3 Serverless Querying (Amazon Athena) Configuration: Set up a query result location in S3 (s3://my-bucket/athena-results/). SQL Operations: Executed standard SQL queries against the Glue table: SELECT product_category, SUM(amount) as total_sales FROM sales_data GROUP BY product_category; Verification: Verified that Athena could query the CSV data directly without loading it into a database server. 3.4 Data Visualization (Amazon QuickSight) Dataset Creation: Selected Athena as the data source and imported the table created in the previous steps. SPICE: Imported data into SPICE (Super-fast, Parallel, In-memory Calculation Engine) for faster rendering. Dashboarding: Built a dashboard containing: Bar Chart: Sales by Region. Pie Chart: Customer Demographics. KPI: Total Revenue. 4. Achievements By the end of Week 9, the following outcomes were accomplished:\n✔ Functional Successes Successfully built a Serverless Data Lake using S3. Automated schema discovery using AWS Glue Crawlers. Performed ad-hoc SQL analysis using Athena without provisioning servers. Created a visual BI Dashboard in QuickSight to present data insights. ✔ Skill Development Understood the separation of Compute (Athena) and Storage (S3). Gained experience with Schema-on-Read concepts vs. traditional Schema-on-Write. Learned IAM permission management between Analytics services (QuickSight \u0026lt;-\u0026gt; Athena \u0026lt;-\u0026gt; S3). 5. Challenges Encountered \u0026amp; Resolutions Challenge 1: Athena Output Location Error\nIssue: Query failed with \u0026ldquo;No output location provided\u0026rdquo;. Fix: Configured the \u0026ldquo;Query Result Location\u0026rdquo; in Athena settings to point to a valid S3 bucket folder. Challenge 2: QuickSight Permissions\nIssue: QuickSight could not access the S3 bucket data. Fix: Went to QuickSight \u0026ldquo;Manage QuickSight\u0026rdquo; \u0026gt; \u0026ldquo;Security \u0026amp; Permissions\u0026rdquo; and explicitly ticked the checkbox for the S3 bucket containing the data. Challenge 3: Glue Crawler Classification\nIssue: Crawler classified CSV data incorrectly due to header issues. Fix: Edited the custom classifier in Glue to properly recognize the CSV header row. "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/1-worklog/",
	"title": "Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1: Getting Familiar with AWS and Core Services\nWeek 2: Building a Static Website with S3 and Connecting to RDS\nWeek 3: Optimizing Performance with CloudFront, DynamoDB, and ElastiCache\nWeek 4: Migration and Disaster Recovery with DMS and EDR\nWeek 5: Infrastructure as Code with CloudFormation, CDK, and Systems Manager\nWeek 6: Security and Cost Management with IAM Policies, KMS, Secrets Manager, Billing Dashboard, and AWS Budgets\nWeek 7: Enhancing System Scalability with Auto Scaling, Load Balancer, SQS/SNS, and VPC Flow Logs\nWeek 8: Reviewing the AWS Well-Architected Framework and Consolidating Core Service Knowledge\nWeek 9: Hands-on with Data \u0026amp; Analytics Services: Data Lake with S3, Glue, Athena, and QuickSight\nWeek 10: Hands-on with AI/ML Services: SageMaker, Rekognition, Comprehend, and Kendra\nWeek 11: Proposal Writing and Capstone Project Completion\nWeek 12: Capstone Project – Knowledge Wrap-up and Building a Complete AWS Solution\n"
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/5-workshop/5.4-backed/5.4.2-other-lambdas/",
	"title": "Create Auxiliary Lambdas",
	"tags": [],
	"description": "",
	"content": "In addition to the search function, we need two more functions to complete the virtual assistant\u0026rsquo;s features. Follow similar creation steps as the ragsearch function, only changing the configuration parameters.\n1. Contract Generation Function (generate_contract) Create Function: Name: generate_contract Runtime: Python 3.12 Code: Copy from the generate_contract.py file in the source code -\u0026gt; Deploy. import json import os import uuid import datetime import logging from typing import Dict, Any, List import boto3 from botocore.exceptions import ClientError logger = logging.getLogger() logger.setLevel(logging.INFO) # ------------------------------------------------------------------- # Config \u0026amp; clients # ------------------------------------------------------------------- AWS_REGION = os.getenv(\u0026#34;AWS_REGION\u0026#34;, \u0026#34;ap-southeast-1\u0026#34;) TEMPLATE_BUCKET = os.getenv(\u0026#34;TEMPLATE_BUCKET\u0026#34;) TEMPLATE_METADATA_KEY = os.getenv(\u0026#34;TEMPLATE_METADATA_KEY\u0026#34;, \u0026#34;index/template_metadata.jsonl\u0026#34;) MODEL_ID = os.getenv(\u0026#34;MODEL_ID\u0026#34;, \u0026#34;anthropic.claude-3-haiku-20240307-v1:0\u0026#34;) # Lambda RAG-search (deployed in stage 2.3) RAG_FUNCTION_NAME = os.getenv(\u0026#34;RAG_FUNCTION_NAME\u0026#34;, \u0026#34;ragsearch\u0026#34;) if not TEMPLATE_BUCKET: raise RuntimeError(\u0026#34;TEMPLATE_BUCKET env var is required\u0026#34;) s3 = boto3.client(\u0026#34;s3\u0026#34;, region_name=AWS_REGION) bedrock = boto3.client(\u0026#34;bedrock-runtime\u0026#34;, region_name=AWS_REGION) lambda_client = boto3.client(\u0026#34;lambda\u0026#34;, region_name=AWS_REGION) # ------------------------------------------------------------------- # Global cache template metadata # ------------------------------------------------------------------- TEMPLATE_CACHE = { \u0026#34;loaded\u0026#34;: False, \u0026#34;by_id\u0026#34;: {} # dict[doc_id] = metadata dict } def load_template_metadata_if_needed(): if TEMPLATE_CACHE[\u0026#34;loaded\u0026#34;]: return logger.info(\u0026#34;Loading template metadata from s3://%s/%s ...\u0026#34;, TEMPLATE_BUCKET, TEMPLATE_METADATA_KEY) try: obj = s3.get_object(Bucket=TEMPLATE_BUCKET, Key=TEMPLATE_METADATA_KEY) except ClientError as e: logger.error(\u0026#34;Failed to load template metadata: %s\u0026#34;, e) raise by_id = {} for line in obj[\u0026#34;Body\u0026#34;].iter_lines(): if not line: continue try: rec = json.loads(line.decode(\u0026#34;utf-8\u0026#34;)) except json.JSONDecodeError: logger.warning(\u0026#34;Invalid JSON line in template metadata, skipped\u0026#34;) continue doc_id = rec.get(\u0026#34;doc_id\u0026#34;) if not doc_id: continue by_id[doc_id] = rec TEMPLATE_CACHE[\u0026#34;by_id\u0026#34;] = by_id TEMPLATE_CACHE[\u0026#34;loaded\u0026#34;] = True logger.info(\u0026#34;Loaded %d template metadata records\u0026#34;, len(by_id)) def parse_event_body(event: Dict[str, Any]) -\u0026gt; Dict[str, Any]: if \u0026#34;body\u0026#34; not in event: return event body = event[\u0026#34;body\u0026#34;] if event.get(\u0026#34;isBase64Encoded\u0026#34;): import base64 body = base64.b64decode(body).decode(\u0026#34;utf-8\u0026#34;) try: data = json.loads(body) except json.JSONDecodeError: raise ValueError(\u0026#34;Request body must be valid JSON\u0026#34;) return data def load_template_raw_text(metadata: Dict[str, Any]) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34; MVP: Try reading template file content from S3 as text (many .doc/.docx files won\u0026#39;t be readable, but that\u0026#39;s fine, used only for structure reference if decode works). Later if you want standard DOCX processing, add python-docx. \u0026#34;\u0026#34;\u0026#34; source_raw_path = metadata.get(\u0026#34;source_raw_path\u0026#34;) if not source_raw_path: return \u0026#34;\u0026#34; try: obj = s3.get_object(Bucket=TEMPLATE_BUCKET, Key=source_raw_path) except ClientError as e: logger.warning(\u0026#34;Failed to load template file %s: %s\u0026#34;, source_raw_path, e) return \u0026#34;\u0026#34; try: content_bytes = obj[\u0026#34;Body\u0026#34;].read() text = content_bytes.decode(\u0026#34;utf-8\u0026#34;, errors=\u0026#34;ignore\u0026#34;) return text except Exception: return \u0026#34;\u0026#34; # ------------------------------------------------------------------- # RAG integration # ------------------------------------------------------------------- def build_rag_query(template_metadata: Dict[str, Any], contract_info: Dict[str, Any]) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34; Build query sent to RAG-search: - State contract type (template_type, title). - Attach contract_info JSON. Goal: find legal articles related to contract type and main content. \u0026#34;\u0026#34;\u0026#34; template_type = template_metadata.get(\u0026#34;template_type\u0026#34;) or \u0026#34;\u0026#34; title = template_metadata.get(\u0026#34;title\u0026#34;) or \u0026#34;\u0026#34; header = f\u0026#34;Loại hợp đồng: {template_type}. Tiêu đề: {title}.\u0026#34; body = json.dumps(contract_info, ensure_ascii=False, indent=2) query = ( header + \u0026#34;\\n\\nDưới đây là thông tin chi tiết về hợp đồng (JSON):\\n\u0026#34; + body + \u0026#34;\\n\\nHãy tìm các văn bản pháp luật Việt Nam liên quan trực tiếp tới loại hợp đồng này, \u0026#34; \u0026#34;đặc biệt là về: thời hạn thuê/mua, nghĩa vụ các bên, chấm dứt hợp đồng, phạt vi phạm, \u0026#34; \u0026#34;bồi thường thiệt hại, quyền sử dụng tài sản.\u0026#34; ) return query def call_rag_lambda(query: str, language: str = \u0026#34;vi\u0026#34;) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34; Call Lambda rag_search directly (invokeFunction). Lambda rag_search currently returns format: { \u0026#34;statusCode\u0026#34;: 200, \u0026#34;body\u0026#34;: \u0026#34;{\\\u0026#34;query\\\u0026#34;:..., \\\u0026#34;results\\\u0026#34;: [...]}\u0026#34; } \u0026#34;\u0026#34;\u0026#34; if not RAG_FUNCTION_NAME: return {} payload = { \u0026#34;query\u0026#34;: query, \u0026#34;language\u0026#34;: language, \u0026#34;top_k\u0026#34;: 6, \u0026#34;filters\u0026#34;: { \u0026#34;source_type\u0026#34;: [\u0026#34;legal\u0026#34;] # Can add \u0026#34;field\u0026#34;: [\u0026#34;Xây dựng - Đô thị\u0026#34;] if focusing on Real Estate } } try: response = lambda_client.invoke( FunctionName=RAG_FUNCTION_NAME, InvocationType=\u0026#34;RequestResponse\u0026#34;, Payload=json.dumps(payload).encode(\u0026#34;utf-8\u0026#34;), ) except Exception as e: logger.warning(\u0026#34;RAG Lambda invoke failed: %s\u0026#34;, e) return {} try: raw_payload = response[\u0026#34;Payload\u0026#34;].read() resp_payload = json.loads(raw_payload) except Exception as e: logger.warning(\u0026#34;Failed to parse RAG Lambda raw payload: %s\u0026#34;, e) return {} # Case where rag_search is still using make_response(statusCode, body) if isinstance(resp_payload, dict) and \u0026#34;statusCode\u0026#34; in resp_payload: status = resp_payload.get(\u0026#34;statusCode\u0026#34;, 500) if status != 200: logger.warning(\u0026#34;RAG Lambda returned status %s: %s\u0026#34;, status, resp_payload.get(\u0026#34;body\u0026#34;)) return {} body = resp_payload.get(\u0026#34;body\u0026#34;) or \u0026#34;{}\u0026#34; try: return json.loads(body) except json.JSONDecodeError: logger.warning(\u0026#34;RAG Lambda body is not valid JSON\u0026#34;) return {} # If later rag_search is modified to return raw dict, fall here if isinstance(resp_payload, dict): return resp_payload return {} def build_legal_context_text(rag_result: Dict[str, Any]) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34; Receive result from RAG (rag_search) and build into a text block to stuff into prompt. \u0026#34;\u0026#34;\u0026#34; chunks = rag_result.get(\u0026#34;results\u0026#34;) or [] if not chunks: return \u0026#34;\u0026#34; lines: List[str] = [] for i, c in enumerate(chunks, start=1): title = c.get(\u0026#34;title\u0026#34;) or \u0026#34;\u0026#34; article_no = c.get(\u0026#34;article_no\u0026#34;) or \u0026#34;\u0026#34; article_title = c.get(\u0026#34;article_title\u0026#34;) or \u0026#34;\u0026#34; text = c.get(\u0026#34;text\u0026#34;) or \u0026#34;\u0026#34; header = f\u0026#34;[Trích dẫn pháp luật {i} – {title}\u0026#34; if article_no: header += f\u0026#34;, {article_no}\u0026#34; if article_title: header += f\u0026#34;: {article_title}\u0026#34; header += \u0026#34;]\u0026#34; lines.append(header) lines.append(text) lines.append(\u0026#34;\u0026#34;) return \u0026#34;\\n\u0026#34;.join(lines) def retrieve_legal_context_for_template( template_metadata: Dict[str, Any], contract_info: Dict[str, Any], language: str ) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34; Master function: build query -\u0026gt; call RAG -\u0026gt; build text. If error or no result, return \u0026#34;\u0026#34; to avoid blocking main flow. \u0026#34;\u0026#34;\u0026#34; try: query = build_rag_query(template_metadata, contract_info) rag_result = call_rag_lambda(query=query, language=language) context_text = build_legal_context_text(rag_result) return context_text except Exception as e: logger.warning(\u0026#34;retrieve_legal_context_for_template failed: %s\u0026#34;, e) return \u0026#34;\u0026#34; # ------------------------------------------------------------------- # LLM: generate contract # ------------------------------------------------------------------- def build_system_prompt() -\u0026gt; str: return ( \u0026#34;Bạn là một luật sư hợp đồng tại Việt Nam, chuyên soạn thảo hợp đồng thuê/mua bán/chuyển nhượng bất động sản.\\n\u0026#34; \u0026#34;- Ngôn ngữ: tiếng Việt, văn phong rõ ràng, chặt chẽ, nhưng dễ hiểu.\\n\u0026#34; \u0026#34;- Hãy soạn thảo hợp đồng dựa trên thông tin đầu vào (contract_info), loại hợp đồng (template_type) \u0026#34; \u0026#34;và các trích dẫn pháp luật được cung cấp.\\n\u0026#34; \u0026#34;- Nếu thông tin đầu vào chưa đầy đủ, hãy điền các điều khoản theo thông lệ phổ biến, \u0026#34; \u0026#34;nhưng không bịa số liệu quá cụ thể.\\n\u0026#34; \u0026#34;- Luôn đảm bảo quyền và nghĩa vụ của các bên cân bằng, ưu tiên tuân thủ pháp luật Việt Nam.\\n\u0026#34; \u0026#34;- Output CHỈ là nội dung hợp đồng hoàn chỉnh dạng text thuần, không thêm giải thích.\\n\u0026#34; ) def build_user_prompt( template_metadata: Dict[str, Any], contract_info: Dict[str, Any], template_raw_text: str, legal_context: str ) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34; Prompt for model: provide - Template info (type, title, ...). - contract_info (JSON). - Legal context from RAG. - (Optional) snippet template_raw_text if readable. \u0026#34;\u0026#34;\u0026#34; title = template_metadata.get(\u0026#34;title\u0026#34;) or \u0026#34;\u0026#34; template_type = template_metadata.get(\u0026#34;template_type\u0026#34;) or \u0026#34;\u0026#34; template_desc = f\u0026#34;Loại hợp đồng: {template_type}. Tiêu đề mẫu: {title}.\u0026#34; user_parts: List[str] = [] user_parts.append(template_desc) # Legal info from RAG if legal_context: user_parts.append( \u0026#34;\\nDưới đây là một số trích dẫn pháp luật và điều khoản liên quan do hệ thống truy xuất được \u0026#34; \u0026#34;(hãy dùng làm căn cứ khi soạn thảo, nhưng không cần trích nguyên văn toàn bộ):\\n\u0026#34; ) user_parts.append(legal_context) # contract_info user_parts.append(\u0026#34;\\nDưới đây là thông tin đầu vào (contract_info) ở dạng JSON:\\n\u0026#34;) user_parts.append(json.dumps(contract_info, ensure_ascii=False, indent=2)) # snippet from original template (if any) if template_raw_text: max_chars = 3000 snippet = template_raw_text[:max_chars] user_parts.append( \u0026#34;\\nDưới đây là một phần nội dung gốc của mẫu hợp đồng (nếu đọc được, chỉ dùng tham khảo cấu trúc, \u0026#34; \u0026#34;không cần copy y nguyên):\\n\u0026#34; ) user_parts.append(snippet) user_parts.append( \u0026#34;\\nYÊU CẦU:\\n\u0026#34; \u0026#34;- Hãy soạn thảo TOÀN BỘ hợp đồng hoàn chỉnh, có đầy đủ phần mở đầu, điều khoản chi tiết, \u0026#34; \u0026#34;điều khoản chung, điều khoản về giải quyết tranh chấp, chữ ký.\\n\u0026#34; \u0026#34;- Trả về nội dung hợp đồng ở dạng text thuần (plain text), mỗi điều khoản nên cách nhau ít nhất một dòng trống.\\n\u0026#34; \u0026#34;- Không thêm bất kỳ giải thích nào ngoài nội dung hợp đồng.\u0026#34; ) return \u0026#34;\\n\u0026#34;.join(user_parts) def call_bedrock_generate_contract(system_prompt: str, user_prompt: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34; Call Bedrock (Claude Haiku) to generate contract, return plain text. \u0026#34;\u0026#34;\u0026#34; logger.info(\u0026#34;Calling Bedrock model %s for contract generation ...\u0026#34;, MODEL_ID) try: response = bedrock.converse( modelId=MODEL_ID, system=[{\u0026#34;text\u0026#34;: system_prompt}], messages=[ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: [{\u0026#34;text\u0026#34;: user_prompt}], } ], inferenceConfig={ \u0026#34;maxTokens\u0026#34;: 4096, \u0026#34;temperature\u0026#34;: 0.2, \u0026#34;topP\u0026#34;: 0.9, }, ) except ClientError as e: logger.error(\u0026#34;Bedrock invocation failed: %s\u0026#34;, e) raise try: output_message = response[\u0026#34;output\u0026#34;][\u0026#34;message\u0026#34;] content_list = output_message.get(\u0026#34;content\u0026#34;, []) if not content_list: raise ValueError(\u0026#34;Empty content from model\u0026#34;) model_text = content_list[0].get(\u0026#34;text\u0026#34;, \u0026#34;\u0026#34;) except Exception as e: logger.error(\u0026#34;Failed to extract text from Bedrock response: %s\u0026#34;, e) raise return model_text def to_html_from_text(contract_text: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34; Simple: each line -\u0026gt; \u0026lt;p\u0026gt;, empty line -\u0026gt; \u0026lt;br\u0026gt;. \u0026#34;\u0026#34;\u0026#34; lines = contract_text.splitlines() html_lines: List[str] = [] html_lines.append(\u0026#34;\u0026lt;html\u0026gt;\u0026lt;body\u0026gt;\u0026#34;) for line in lines: line = line.strip() if not line: html_lines.append(\u0026#34;\u0026lt;br\u0026gt;\u0026#34;) else: esc = ( line.replace(\u0026#34;\u0026amp;\u0026#34;, \u0026#34;\u0026amp;amp;\u0026#34;) .replace(\u0026#34;\u0026lt;\u0026#34;, \u0026#34;\u0026amp;lt;\u0026#34;) .replace(\u0026#34;\u0026gt;\u0026#34;, \u0026#34;\u0026amp;gt;\u0026#34;) ) html_lines.append(f\u0026#34;\u0026lt;p\u0026gt;{esc}\u0026lt;/p\u0026gt;\u0026#34;) html_lines.append(\u0026#34;\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt;\u0026#34;) return \u0026#34;\\n\u0026#34;.join(html_lines) def save_generated_to_s3(contract_text: str, contract_html: str) -\u0026gt; Dict[str, str]: \u0026#34;\u0026#34;\u0026#34; Save contract_text and contract_html to S3, return paths. \u0026#34;\u0026#34;\u0026#34; now = datetime.datetime.utcnow() y = now.year m = now.month contract_id = str(uuid.uuid4()) base_prefix = f\u0026#34;generated/contracts/{y}/{m:02d}/{contract_id}\u0026#34; text_key = f\u0026#34;{base_prefix}.txt\u0026#34; html_key = f\u0026#34;{base_prefix}.html\u0026#34; try: s3.put_object( Bucket=TEMPLATE_BUCKET, Key=text_key, Body=contract_text.encode(\u0026#34;utf-8\u0026#34;), ContentType=\u0026#34;text/plain; charset=utf-8\u0026#34;, ) s3.put_object( Bucket=TEMPLATE_BUCKET, Key=html_key, Body=contract_html.encode(\u0026#34;utf-8\u0026#34;), ContentType=\u0026#34;text/html; charset=utf-8\u0026#34;, ) except ClientError as e: logger.warning(\u0026#34;Failed to upload generated contract to S3: %s\u0026#34;, e) return {} return { \u0026#34;contract_text_path\u0026#34;: text_key, \u0026#34;contract_html_path\u0026#34;: html_key, } def make_response(status_code: int, body: Dict[str, Any]) -\u0026gt; Dict[str, Any]: return { \u0026#34;statusCode\u0026#34;: status_code, \u0026#34;headers\u0026#34;: { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;Access-Control-Allow-Origin\u0026#34;: \u0026#34;*\u0026#34;, # demo }, \u0026#34;body\u0026#34;: json.dumps(body, ensure_ascii=False), } # ------------------------------------------------------------------- # Lambda handler # ------------------------------------------------------------------- def lambda_handler(event, context): logger.info(\u0026#34;Received event: %s\u0026#34;, json.dumps(event)[:1000]) try: data = parse_event_body(event) template_id = (data.get(\u0026#34;template_id\u0026#34;) or \u0026#34;\u0026#34;).strip() if not template_id: return make_response(400, {\u0026#34;error\u0026#34;: \u0026#34;template_id is required\u0026#34;}) contract_info = data.get(\u0026#34;contract_info\u0026#34;) or {} if not isinstance(contract_info, dict): return make_response(400, {\u0026#34;error\u0026#34;: \u0026#34;contract_info must be an object\u0026#34;}) language = (data.get(\u0026#34;language\u0026#34;) or \u0026#34;vi\u0026#34;).lower() # 1. Load template metadata load_template_metadata_if_needed() metadata = TEMPLATE_CACHE[\u0026#34;by_id\u0026#34;].get(template_id) if not metadata: return make_response(404, {\u0026#34;error\u0026#34;: f\u0026#34;Template not found for template_id={template_id}\u0026#34;}) # 2. (Optional) load raw text of template file from S3 template_raw_text = load_template_raw_text(metadata) # 3. Get legal context from RAG legal_context = retrieve_legal_context_for_template(metadata, contract_info, language) # 4. Build prompts system_prompt = build_system_prompt() user_prompt = build_user_prompt(metadata, contract_info, template_raw_text, legal_context) # 5. Call Bedrock to generate contract contract_text = call_bedrock_generate_contract(system_prompt, user_prompt) # 6. Convert to HTML contract_html = to_html_from_text(contract_text) # 7. Save to S3 #s3_paths = save_generated_to_s3(contract_text, contract_html) s3_paths = {} # 8. Build response resp_body = { \u0026#34;template_id\u0026#34;: template_id, \u0026#34;template_title\u0026#34;: metadata.get(\u0026#34;title\u0026#34;), \u0026#34;template_type\u0026#34;: metadata.get(\u0026#34;template_type\u0026#34;), \u0026#34;language\u0026#34;: language, \u0026#34;contract_text\u0026#34;: contract_text, \u0026#34;contract_html\u0026#34;: contract_html, \u0026#34;s3_paths\u0026#34;: s3_paths, \u0026#34;debug\u0026#34;: { \u0026#34;used_template_file\u0026#34;: metadata.get(\u0026#34;source_raw_path\u0026#34;), \u0026#34;source_type\u0026#34;: metadata.get(\u0026#34;source_type\u0026#34;), \u0026#34;rag_used\u0026#34;: bool(legal_context), }, } return make_response(200, resp_body) except ValueError as ve: logger.warning(\u0026#34;Bad request: %s\u0026#34;, ve) return make_response(400, {\u0026#34;error\u0026#34;: str(ve)}) except ClientError as ce: logger.error(\u0026#34;AWS client error: %s\u0026#34;, ce) return make_response(502, {\u0026#34;error\u0026#34;: \u0026#34;Upstream AWS error\u0026#34;, \u0026#34;details\u0026#34;: str(ce)}) except Exception as e: logger.error(\u0026#34;Unexpected error: %s\u0026#34;, e) return make_response(500, {\u0026#34;error\u0026#34;: \u0026#34;Internal server error\u0026#34;, \u0026#34;details\u0026#34;: str(e)}) Configuration: Memory: 512 MB Timeout: 30 sec Environment variables: MODEL_ID: anthropic.claude-3-haiku-20240307-v1:0 RAG_FUNCTION_NAME: ragsearch TEMPLATE_BUCKET: \u0026lt;Your-S3-Bucket-Name\u0026gt; TEMPLATE_METADATA_KEY: index/template_metadata.jsonl Permissions: Add AmazonBedrockFullAccess and AmazonS3FullAccess. 2. General Chat Function (CallLLM) Create Function: Name: CallLLM Runtime: Python 3.12 Code: Copy from the CallLLM.py file in the source code -\u0026gt; Deploy. import json import os import base64 import logging from dataclasses import dataclass from typing import Optional, Tuple, Dict, Any import boto3 from botocore.exceptions import ClientError # ----------------------------------------------------------------------------- # 1. Logging \u0026amp; Config # ----------------------------------------------------------------------------- logger = logging.getLogger() logger.setLevel(logging.INFO) bedrock = boto3.client(\u0026#34;bedrock-runtime\u0026#34;) lambda_client = boto3.client(\u0026#34;lambda\u0026#34;) # ENV: RAG-search Lambda name, e.g., \u0026#39;rag_search\u0026#39; RAG_FUNCTION_NAME = os.getenv(\u0026#34;RAG_FUNCTION_NAME\u0026#34;) class Config: MODEL_ID: str = os.getenv( \u0026#34;MODEL_ID\u0026#34;, \u0026#34;anthropic.claude-3-haiku-20240307-v1:0\u0026#34; ) ALLOWED_FORMATS = {\u0026#34;pdf\u0026#34;, \u0026#34;doc\u0026#34;, \u0026#34;docx\u0026#34;, \u0026#34;txt\u0026#34;, \u0026#34;md\u0026#34;, \u0026#34;html\u0026#34;} MAX_FILE_SIZE_BYTES: int = int(10 * 1024 * 1024) # ~10MB SYSTEM_PROMPT = \u0026#34;\u0026#34;\u0026#34; Bạn là một trợ lý pháp lý tự động, hỗ trợ người dùng phổ thông phân tích rủi ro trong hợp đồng tiếng Việt. NGUYÊN TẮC: - Chỉ phân tích dựa trên nội dung hợp đồng được cung cấp. - Luôn nhấn mạnh rằng kết quả chỉ mang tính chất tham khảo, không thay thế luật sư. - Nếu không chắc về điều luật cụ thể, hãy để trống các trường liên quan đến điều/khoản, KHÔNG tự bịa số điều luật. - Ưu tiên ngôn ngữ dễ hiểu, súc tích, dùng tiếng Việt. NHIỆM VỤ: - Tóm tắt ngắn gọn nội dung chính của hợp đồng. - Xác định các rủi ro chính (nếu có), phân loại rủi ro và đánh giá mức độ nghiêm trọng. - Đề xuất cách chỉnh sửa/bo sung điều khoản để giảm rủi ro. - Nếu có thể, chỉ ra các căn cứ pháp luật liên quan (tên luật, điều, khoản); nếu không chắc, hãy để trống. YÊU CẦU QUAN TRỌNG VỀ OUTPUT: - CHỈ trả về một JSON hợp lệ (valid JSON), không chứa bất kỳ giải thích, bình luận hay text nào ở bên ngoài. - KHÔNG dùng markdown, KHÔNG dùng ```json. - JSON phải có cấu trúc CHÍNH XÁC như sau: { \u0026#34;summary\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;overall_risk_level\u0026#34;: \u0026#34;LOW | MEDIUM | HIGH | CRITICAL\u0026#34;, \u0026#34;risk_items\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;string, ví dụ R1, R2,...\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;string, tiêu đề ngắn cho rủi ro\u0026#34;, \u0026#34;clause_excerpt\u0026#34;: \u0026#34;string, trích đoạn điều khoản liên quan (nếu xác định được, nếu không thì chuỗi rỗng)\u0026#34;, \u0026#34;risk_types\u0026#34;: [ \u0026#34;LegalCompliance | Financial | FraudScam | UnclearTerm | ImbalancedObligation\u0026#34; ], \u0026#34;severity\u0026#34;: \u0026#34;LOW | MEDIUM | HIGH | CRITICAL\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;string, mô tả chi tiết rủi ro bằng tiếng Việt\u0026#34;, \u0026#34;recommendation\u0026#34;: \u0026#34;string, gợi ý chỉnh sửa cụ thể bằng tiếng Việt\u0026#34;, \u0026#34;law_references\u0026#34;: [ { \u0026#34;law_name\u0026#34;: \u0026#34;string, có thể để chuỗi rỗng nếu không biết chắc\u0026#34;, \u0026#34;article\u0026#34;: \u0026#34;string, có thể để chuỗi rỗng nếu không biết chắc\u0026#34;, \u0026#34;clause\u0026#34;: \u0026#34;string, có thể để chuỗi rỗng nếu không biết chắc\u0026#34;, \u0026#34;note\u0026#34;: \u0026#34;string, ghi chú thêm nếu cần, có thể để chuỗi rỗng\u0026#34; } ] } ], \u0026#34;disclaimer\u0026#34;: \u0026#34;string, lời cảnh báo rằng đây không phải tư vấn pháp lý chính thức\u0026#34; } - Nếu không tìm thấy rủi ro nào đáng kể, hãy để risk_items là một mảng rỗng [] và overall_risk_level là \u0026#34;LOW\u0026#34;. - Đảm bảo JSON trả về là hợp lệ, không có dấu phẩy thừa, không có comment. \u0026#34;\u0026#34;\u0026#34;.strip() # ----------------------------------------------------------------------------- # 2. Data structures # ----------------------------------------------------------------------------- @dataclass class ContractInput: language: str contract_text: Optional[str] = None file_name: Optional[str] = None file_format: Optional[str] = None file_bytes: Optional[bytes] = None # context RAG sent by BE (or None) rag_context: Optional[str] = None @property def has_file(self) -\u0026gt; bool: return self.file_bytes is not None and self.file_format is not None @property def has_text(self) -\u0026gt; bool: return self.contract_text is not None and self.contract_text.strip() != \u0026#34;\u0026#34; # ----------------------------------------------------------------------------- # 3. HTTP event parsing # ----------------------------------------------------------------------------- def parse_event_body(event: Dict[str, Any]) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34; Support both calls via API Gateway HTTP API (body is string) and direct invoke (event is dict). \u0026#34;\u0026#34;\u0026#34; if \u0026#34;body\u0026#34; not in event: # direct invoke return event body = event[\u0026#34;body\u0026#34;] if event.get(\u0026#34;isBase64Encoded\u0026#34;): body = base64.b64decode(body).decode(\u0026#34;utf-8\u0026#34;) try: data = json.loads(body) except json.JSONDecodeError: logger.warning(\u0026#34;Body is not valid JSON\u0026#34;) raise ValueError(\u0026#34;Request body must be valid JSON\u0026#34;) return data # ----------------------------------------------------------------------------- # 4. Request -\u0026gt; ContractInput (validation) # ----------------------------------------------------------------------------- def parse_contract_input(data: Dict[str, Any]) -\u0026gt; ContractInput: language = (data.get(\u0026#34;language\u0026#34;) or \u0026#34;vi\u0026#34;).lower() # Get context_rag (if sent by BE) rag_context = (data.get(\u0026#34;context_rag\u0026#34;) or \u0026#34;\u0026#34;).strip() or None # File branch file_b64 = data.get(\u0026#34;file_bytes_base64\u0026#34;) file_format = (data.get(\u0026#34;file_format\u0026#34;) or \u0026#34;\u0026#34;).lower() file_name = data.get(\u0026#34;file_name\u0026#34;) or None if file_b64 and file_format: if file_format not in Config.ALLOWED_FORMATS: raise ValueError( f\u0026#34;Unsupported file_format \u0026#39;{file_format}\u0026#39;. \u0026#34; f\u0026#34;Allowed: {sorted(Config.ALLOWED_FORMATS)}\u0026#34; ) try: file_bytes = base64.b64decode(file_b64) except Exception as e: logger.warning(\u0026#34;Invalid base64 for file_bytes_base64: %s\u0026#34;, e) raise ValueError(\u0026#34;file_bytes_base64 is not valid base64\u0026#34;) if len(file_bytes) \u0026gt; Config.MAX_FILE_SIZE_BYTES: raise ValueError( \u0026#34;File is too large for Bedrock document input \u0026#34; \u0026#34;(limit approx 4.5 MB).\u0026#34; ) return ContractInput( language=language, file_name=file_name, file_format=file_format, file_bytes=file_bytes, rag_context=rag_context, ) # Text branch contract_text = (data.get(\u0026#34;contract_text\u0026#34;) or \u0026#34;\u0026#34;).strip() if not contract_text: raise ValueError( \u0026#34;Either contract_text or (file_bytes_base64 + file_format) is required\u0026#34; ) return ContractInput( language=language, contract_text=contract_text, rag_context=rag_context, ) # ----------------------------------------------------------------------------- # 5. RAG hook: use Lambda invoke instead of API Gateway # ----------------------------------------------------------------------------- def retrieve_legal_context(contract_text: str, language: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34; Call Lambda rag_search directly (invokeFunction), return context text to stuff into LLM prompt. If RAG fails or not configured, return empty string to avoid blocking main flow. Requires: - ENV: RAG_FUNCTION_NAME = name of rag_search Lambda - IAM: lambda:InvokeFunction on that function \u0026#34;\u0026#34;\u0026#34; if not RAG_FUNCTION_NAME: return \u0026#34;\u0026#34; # Payload sent to rag_search payload = { \u0026#34;query\u0026#34;: contract_text, \u0026#34;language\u0026#34;: language, \u0026#34;top_k\u0026#34;: 8, \u0026#34;filters\u0026#34;: { \u0026#34;source_type\u0026#34;: [\u0026#34;legal\u0026#34;], } } try: response = lambda_client.invoke( FunctionName=RAG_FUNCTION_NAME, InvocationType=\u0026#34;RequestResponse\u0026#34;, Payload=json.dumps(payload).encode(\u0026#34;utf-8\u0026#34;), ) except Exception as e: print(f\u0026#34;[WARN] RAG Lambda invoke failed: {e}\u0026#34;) return \u0026#34;\u0026#34; try: raw_payload = response[\u0026#34;Payload\u0026#34;].read() resp_payload = json.loads(raw_payload) except Exception as e: print(f\u0026#34;[WARN] Failed to parse RAG Lambda raw payload: {e}\u0026#34;) return \u0026#34;\u0026#34; # Case where rag_search returns API format (statusCode + body) if isinstance(resp_payload, dict) and \u0026#34;statusCode\u0026#34; in resp_payload: status = resp_payload.get(\u0026#34;statusCode\u0026#34;, 500) if status != 200: print(f\u0026#34;[WARN] RAG Lambda returned status {status}: {resp_payload.get(\u0026#39;body\u0026#39;)}\u0026#34;) return \u0026#34;\u0026#34; body = resp_payload.get(\u0026#34;body\u0026#34;) or \u0026#34;{}\u0026#34; try: result = json.loads(body) except json.JSONDecodeError: print(\u0026#34;[WARN] RAG Lambda body is not valid JSON\u0026#34;) return \u0026#34;\u0026#34; else: # If later rag_search returns raw dict, use it directly if isinstance(resp_payload, dict): result = resp_payload else: return \u0026#34;\u0026#34; chunks = result.get(\u0026#34;results\u0026#34;) or [] if not chunks: return \u0026#34;\u0026#34; lines = [] for i, c in enumerate(chunks, start=1): title = c.get(\u0026#34;title\u0026#34;) or \u0026#34;\u0026#34; article_no = c.get(\u0026#34;article_no\u0026#34;) or \u0026#34;\u0026#34; article_title = c.get(\u0026#34;article_title\u0026#34;) or \u0026#34;\u0026#34; text = c.get(\u0026#34;text\u0026#34;) or \u0026#34;\u0026#34; header = f\u0026#34;[Trích dẫn {i} – {title}\u0026#34; if article_no: header += f\u0026#34;, {article_no}\u0026#34; if article_title: header += f\u0026#34;: {article_title}\u0026#34; header += \u0026#34;]\u0026#34; lines.append(header) lines.append(text) lines.append(\u0026#34;\u0026#34;) context_str = \u0026#34;\\n\u0026#34;.join(lines) return context_str # ----------------------------------------------------------------------------- # 6. Prompt builder \u0026amp; Bedrock client # ----------------------------------------------------------------------------- def build_user_prompt_text(contract_text: str, context: str | None = None) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34; Prompt used for TEXT mode. Adds context (RAG) if not empty. \u0026#34;\u0026#34;\u0026#34; base = ( \u0026#34;Dưới đây là nội dung hợp đồng cần phân tích rủi ro. \u0026#34; \u0026#34;Hãy đọc kỹ và TRẢ VỀ DUY NHẤT một JSON hợp lệ theo đúng cấu trúc đã được mô tả trong hướng dẫn hệ thống.\\n\\n\u0026#34; \u0026#34;=== NỘI DUNG HỢP ĐỒNG ===\\n\u0026#34; f\u0026#34;{contract_text}\\n\u0026#34; \u0026#34;=== HẾT NỘI DUNG HỢP ĐỒNG ===\u0026#34; ) if context: return ( base + \u0026#34;\\n\\n=== THÔNG TIN THAM KHẢO TỪ THƯ VIỆN PHÁP LUẬT / MẪU HỢP ĐỒNG (RAG) ===\\n\u0026#34; + context + \u0026#34;\\n=== HẾT THÔNG TIN THAM KHẢO ===\u0026#34; ) return base def call_bedrock_text(contract_text: str, language: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34; Call Bedrock Converse API with TEXT and return raw text from model. Has hook for RAG context. \u0026#34;\u0026#34;\u0026#34; context = retrieve_legal_context(contract_text, language=language) user_prompt = build_user_prompt_text(contract_text, context=context) logger.info(\u0026#34;Calling Bedrock model %s with TEXT ...\u0026#34;, Config.MODEL_ID) try: response = bedrock.converse( modelId=Config.MODEL_ID, system=[{\u0026#34;text\u0026#34;: SYSTEM_PROMPT}], messages=[ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: [{\u0026#34;text\u0026#34;: user_prompt}], } ], inferenceConfig={ \u0026#34;maxTokens\u0026#34;: 4096, \u0026#34;temperature\u0026#34;: 0.2, \u0026#34;topP\u0026#34;: 0.9, }, ) logger.info(\u0026#34;Received response from Bedrock (text mode)\u0026#34;) except ClientError as e: logger.error(\u0026#34;Bedrock invocation failed (text mode): %s\u0026#34;, e) raise try: output_message = response[\u0026#34;output\u0026#34;][\u0026#34;message\u0026#34;] content_list = output_message.get(\u0026#34;content\u0026#34;, []) if not content_list: raise ValueError(\u0026#34;Empty content from model (text mode)\u0026#34;) model_text = content_list[0].get(\u0026#34;text\u0026#34;, \u0026#34;\u0026#34;) except Exception as e: logger.error(\u0026#34;Failed to extract text from Bedrock response (text mode): %s\u0026#34;, e) raise return model_text def call_bedrock_document( file_bytes: bytes, file_format: str, file_name: Optional[str], language: str, context: Optional[str] = None, ) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34; Call Bedrock Converse API with DOCUMENT (pdf/docx/...) and return raw text from model. file_format: pdf, doc, docx, txt, md, html context: RAG string (law, template) built by BE (context_rag) \u0026#34;\u0026#34;\u0026#34; if not file_name: file_name = f\u0026#34;uploaded_contract.{file_format}\u0026#34; # Stuff RAG context (if any) into user_text user_text = ( \u0026#34;Tài liệu đính kèm là một hợp đồng cần phân tích rủi ro. \u0026#34; \u0026#34;Hãy đọc toàn bộ tài liệu và TRẢ VỀ DUY NHẤT một JSON hợp lệ theo đúng schema đã được mô tả trong system prompt.\u0026#34; ) if context: user_text += ( \u0026#34;\\n\\n=== THÔNG TIN THAM KHẢO TỪ THƯ VIỆN PHÁP LUẬT / MẪU HỢP ĐỒNG (RAG) ===\\n\u0026#34; f\u0026#34;{context}\\n\u0026#34; \u0026#34;=== HẾT THÔNG TIN THAM KHẢO ===\u0026#34; ) logger.info( \u0026#34;Calling Bedrock model %s with DOCUMENT (format=%s, name=%s, size=%d bytes, has_context=%s) ...\u0026#34;, Config.MODEL_ID, file_format, file_name, len(file_bytes), bool(context) ) try: response = bedrock.converse( modelId=Config.MODEL_ID, system=[{\u0026#34;text\u0026#34;: SYSTEM_PROMPT}], messages=[ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: [ {\u0026#34;text\u0026#34;: user_text}, { \u0026#34;document\u0026#34;: { \u0026#34;format\u0026#34;: file_format, \u0026#34;name\u0026#34;: file_name, \u0026#34;source\u0026#34;: {\u0026#34;bytes\u0026#34;: file_bytes}, } }, ], } ], inferenceConfig={ \u0026#34;maxTokens\u0026#34;: 4096, \u0026#34;temperature\u0026#34;: 0.2, \u0026#34;topP\u0026#34;: 0.9, }, ) logger.info(\u0026#34;Received response from Bedrock (document mode)\u0026#34;) except ClientError as e: logger.error(\u0026#34;Bedrock invocation failed (document mode): %s\u0026#34;, e) raise try: output_message = response[\u0026#34;output\u0026#34;][\u0026#34;message\u0026#34;] content_list = output_message.get(\u0026#34;content\u0026#34;, []) if not content_list: raise ValueError(\u0026#34;Empty content from model (document mode)\u0026#34;) model_text = content_list[0].get(\u0026#34;text\u0026#34;, \u0026#34;\u0026#34;) except Exception as e: logger.error( \u0026#34;Failed to extract text from Bedrock document response: %s\u0026#34;, e ) raise return model_text # ----------------------------------------------------------------------------- # 7. Model JSON parsing # ----------------------------------------------------------------------------- def parse_model_json(model_output_text: str) -\u0026gt; Tuple[Dict[str, Any], Optional[str]]: \u0026#34;\u0026#34;\u0026#34; Robust version: attempts to extract the first JSON object in the output. \u0026#34;\u0026#34;\u0026#34; try: text = model_output_text.strip() # 1. Find the first \u0026#39;{\u0026#39; position start_idx = text.find(\u0026#34;{\u0026#34;) # 2. Find the last \u0026#39;}\u0026#39; position end_idx = text.rfind(\u0026#34;}\u0026#34;) if start_idx != -1 and end_idx != -1: text = text[start_idx : end_idx + 1] else: raise ValueError(\u0026#34;No JSON object found in output\u0026#34;) analysis = json.loads(text) return analysis, None except Exception as e: logger.warning(\u0026#34;Model output parsing failed: %s\u0026#34;, e) logger.info(\u0026#34;Raw output causing error: %s\u0026#34;, model_output_text) analysis = { \u0026#34;summary\u0026#34;: f\u0026#34;AI đã trả về kết quả nhưng không đúng định dạng JSON. (Lỗi: {str(e)})\u0026#34;, \u0026#34;overall_risk_level\u0026#34;: \u0026#34;UNKNOWN\u0026#34;, \u0026#34;risk_items\u0026#34;: [], \u0026#34;disclaimer\u0026#34;: \u0026#34;Lỗi kỹ thuật từ phía AI Parser.\u0026#34; } return analysis, model_output_text # ----------------------------------------------------------------------------- # 8. Core use case: analyze_contract # ----------------------------------------------------------------------------- def analyze_contract(contract_input: ContractInput) -\u0026gt; Tuple[Dict[str, Any], Optional[str]]: \u0026#34;\u0026#34;\u0026#34; Core logic: - Select TEXT or DOCUMENT mode. - Call corresponding Bedrock. - Parse JSON from output. \u0026#34;\u0026#34;\u0026#34; if contract_input.has_file: model_output_text = call_bedrock_document( file_bytes=contract_input.file_bytes, file_format=contract_input.file_format, file_name=contract_input.file_name, language=contract_input.language, context=contract_input.rag_context, # RAG context passed by BE (if any) ) elif contract_input.has_text: model_output_text = call_bedrock_text( contract_text=contract_input.contract_text, language=contract_input.language, ) else: raise ValueError(\u0026#34;No valid input provided\u0026#34;) analysis, raw = parse_model_json(model_output_text) return analysis, raw # ----------------------------------------------------------------------------- # 9. HTTP response helper # ----------------------------------------------------------------------------- def make_response(status_code: int, body: Dict[str, Any]) -\u0026gt; Dict[str, Any]: return { \u0026#34;statusCode\u0026#34;: status_code, \u0026#34;headers\u0026#34;: { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;Access-Control-Allow-Origin\u0026#34;: \u0026#34;*\u0026#34;, }, \u0026#34;body\u0026#34;: json.dumps(body, ensure_ascii=False), } # ----------------------------------------------------------------------------- # 10. Lambda handler # ----------------------------------------------------------------------------- def lambda_handler(event, context): logger.info(\u0026#34;Received event: %s\u0026#34;, json.dumps(event)[:1000]) try: data = parse_event_body(event) contract_input = parse_contract_input(data) analysis, raw_model_output = analyze_contract(contract_input) response_body = { \u0026#34;analysis\u0026#34;: analysis, \u0026#34;model\u0026#34;: Config.MODEL_ID, \u0026#34;raw_model_output\u0026#34;: raw_model_output, \u0026#34;language\u0026#34;: contract_input.language, } return make_response(200, response_body) except ValueError as ve: logger.warning(\u0026#34;Bad request: %s\u0026#34;, ve) return make_response(400, {\u0026#34;error\u0026#34;: str(ve)}) except ClientError as ce: logger.error(\u0026#34;Bedrock client error: %s\u0026#34;, ce) return make_response( 502, {\u0026#34;error\u0026#34;: \u0026#34;Bedrock invocation failed\u0026#34;, \u0026#34;details\u0026#34;: str(ce)}, ) except Exception as e: logger.error(\u0026#34;Unexpected error: %s\u0026#34;, e) return make_response( 500, {\u0026#34;error\u0026#34;: \u0026#34;Internal server error\u0026#34;, \u0026#34;details\u0026#34;: str(e)}, ) Configuration: Memory: 1024 MB Timeout: 50 sec Environment variables: MODEL_ID: anthropic.claude-3-haiku-20240307-v1:0 RAG_FUNCTION_NAME: ragsearch S3_BUCKET_RAW: \u0026lt;Your-S3-Bucket-Name\u0026gt; Permissions: Add AmazonBedrockFullAccess and AmazonS3FullAccess. "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/5-workshop/5.3-infrastructure/5.3.2-dynamodb/",
	"title": "Create DynamoDB Tables",
	"tags": [],
	"description": "",
	"content": "We need to create 3 tables in Amazon DynamoDB to store user information, chat sessions, and message content.\nAccess the DynamoDB service -\u0026gt; Select Tables on the left menu -\u0026gt; Click Create table.\n1. Create Users Table This table is used to store logged-in user information.\nTable name: Enter Users Partition key: Enter user_id (Data type: String) Table settings: Select Customize settings. Table class: Select DynamoDB Standard. Read/write capacity settings: Select On-demand. Click Create table. 2. Create ChatSessions Table This table stores the list of conversations.\nFollow the same steps as for the Users table. Table name: Enter ChatSessions Partition key: Enter session_id (Data type: String) Table settings: Select Customize settings -\u0026gt; On-demand. Click Create table. 3. Create ChatMessages Table This table stores details of individual messages within a conversation.\nTable name: Enter ChatMessages Partition key: Enter session_id (Data type: String) Sort key: Enter timestamp (Data type: String) Note: This table requires a Sort key to order messages chronologically. Table settings: Select Customize settings -\u0026gt; On-demand. Click Create table. Result After completion, double-check the Tables list. You should see 3 tables with Active status as shown below: "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/5-workshop/5.5-fullstack/5.5.2-frontend-amplify/",
	"title": "Deploy Frontend (Amplify)",
	"tags": [],
	"description": "",
	"content": "1. Prepare GitLab Repository You need to push the frontend source code to your personal GitLab so Amplify can retrieve the code for building. There are 2 ways to do this:\nMethod 1: Create new Repo and Push code (Recommended)\nLog in to your personal GitLab and create a New project/repository (blank). Open Terminal on your computer, navigate to the frontend folder: cd frontend Remove old git configuration (if any): Windows (PowerShell): Remove-Item -Recurse -Force .git Mac/Linux: rm -rf .git Initialize new git and push the code: git init git add . git commit -m \u0026#34;Initial deploy\u0026#34; git remote add origin \u0026lt;Your-GitLab-Repo-URL\u0026gt; git branch -M main git push -uf origin main Method 2: Fork from Sample Repo\nAccess: https://gitlab.com/manh-25/contract-demo Click the Fork button to copy it to your account. 2. Create App on AWS Amplify Access the AWS Amplify service on the Console. Scroll to the bottom of the page, select Create new app. On the \u0026ldquo;Start building with Amplify\u0026rdquo; screen, select GitLab -\u0026gt; Click Next. Connect your GitLab account and select the Repository you just pushed (main branch). 3. Configure Build Settings Enter the App name.\nIn the Build settings section, click the Edit YML file button. Delete all default content and Overwrite (Paste) the following code (to use bun for faster build speed):\nversion: 1 frontend: phases: preBuild: commands: - curl -fsSL https://bun.sh/install | bash - export BUN_INSTALL=\u0026#34;$HOME/.bun\u0026#34; - export PATH=\u0026#34;$BUN_INSTALL/bin:$PATH\u0026#34; - bun install build: commands: - export BUN_INSTALL=\u0026#34;$HOME/.bun\u0026#34; - export PATH=\u0026#34;$BUN_INSTALL/bin:$PATH\u0026#34; - bun run build artifacts: baseDirectory: dist files: - \u0026#34;**/*\u0026#34; cache: paths: - node_modules/**/* 4. Configure Environment Variables Click on Advanced settings -\u0026gt; Scroll down to the Environment variables section.\nClick Add new variable to add the following 4 variables one by one.\nWhere to get the Values? -\u0026gt; Open another browser tab to retrieve the information:\nGo to Amazon Cognito service: Select the User Pool named ai-contract-backend-user-pool-dev. VITE_COGNITO_REGION = ap-southeast-1 VITE_COGNITO_USER_POOL_ID = Copy User pool ID. VITE_COGNITO_CLIENT_ID = Go to the App integration tab, scroll to the bottom, and copy the Client ID. Go to Lambda service: Select the function ai-contract-backend-dev-api. VITE_API_URL = Copy the API Endpoint (in the Configuration -\u0026gt; Triggers or API Gateway section). After filling in all 4 variables, click Next.\n5. Deploy and Verify At the Review screen, double-check the information and click Save and deploy. Wait about 3-5 minutes for Amplify to perform the steps: Provision -\u0026gt; Build -\u0026gt; Deploy. When all 3 steps turn green, the application access link will appear in the Domain section (e.g., https://main.d123...amplifyapp.com). 👉 Click the link to experience your Smart Contract Assistant application!\n"
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/4-eventparticipated/4.2-event2/",
	"title": "Event 2",
	"tags": [],
	"description": "",
	"content": "Workshop: Data Science on AWS Time: 9:30 – 11:45, October 16, 2025 Location: Hall A - FPT University HCMC (FPTU HCMC) Role: Attendee\nObjectives Understand the complete Landscape of AWS AI/ML services. Learn how to build, train, and deploy real-world Machine Learning models. Network with experts from the AWS User Group community. Speakers Van Hoang Kha - Cloud Solutions Architect, AWS User Group Leader Bach Doan Vuong - Cloud DevOps Engineer, AWS Community Builder Key Highlights 1. The AWS AI/ML Stack The speakers clearly introduced the 3-layer structure of the AWS AI/ML Stack:\nAI Services (Top layer): Ready-to-use services requiring no deep ML expertise (Vision, Speech, Chatbots, etc.). ML Services (Middle layer): Amazon SageMaker platform for building, training, and deploying models. Frameworks \u0026amp; Infrastructure (Bottom layer): For experts needing deep control (PyTorch, TensorFlow, EC2 GPU\u0026hellip;). 2. Featured AI Services \u0026amp; Demos I found this section incredibly interesting due to its high applicability for student projects:\nVision: Amazon Rekognition for face recognition, video analysis, PPE Detection, and Content Moderation. Speech \u0026amp; Audio: The combination of Amazon Polly (Text-to-Speech with lifelike voices) and Amazon Transcribe (Speech-to-Text, supporting real-time calls). Natural Language Processing (NLP): Amazon Translate: Fast, multi-language translation. Amazon Textract: Quickly extract data from scanned documents/invoices. Amazon Lex: Build intelligent conversational Chatbots. Personalization: Amazon Personalize creates real-time product recommendations without needing deep ML expertise. 3. Machine Learning Workflow with SageMaker The speakers demonstrated the process from Feature Engineering (raw data processing) to Training and Tuning parameters. Specifically, they showed how to bring existing Python code (Scikit-learn, TensorFlow) to run on AWS\u0026rsquo;s powerful infrastructure.\nPersonal Experience This workshop was very practical and relevant for students like me.\nI was particularly impressed by the sharing on Amazon Personalize. I used to think building a Recommendation System was extremely difficult and required complex algorithms, but AWS makes it much simpler. A huge plus was the speaker\u0026rsquo;s practical advice on Cost Management. The \u0026ldquo;Monitoring Cost Daily\u0026rdquo; slide made me realize that working with Cloud isn\u0026rsquo;t just about making code run, but also about optimizing costs to avoid \u0026ldquo;wallet shock\u0026rdquo; at the end of the month. The atmosphere at FPTU was energetic. Hearing experiences from industry seniors gave me more motivation to study for my upcoming AWS certifications. Event Photos Summary: This event truly \u0026ldquo;enlightened\u0026rdquo; me about the AI Services toolkit. Instead of building models from scratch, I now know how to leverage AWS\u0026rsquo;s available APIs to solve problems faster and more effectively.\n"
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/5-workshop/5.2-prerequiste/",
	"title": "Preparation Steps",
	"tags": [],
	"description": "",
	"content": "1. System Requirements and Tools Before starting, please ensure your computer has the following tools installed:\nAWS Account: An active AWS account. AWS CLI: Installed and configured (Installation Guide). Node.js \u0026amp; NPM: To run backend and frontend deployment commands. Git: To download the source code. Text Editor: VS Code (recommended). 2. Download Source Code Open the terminal on your computer and clone the repository containing the project source code:\ngit clone https://gitlab.com/manh-25/contract-demo.git cd contract-demo 3. Region Setup ⚠️ Important: Throughout this workshop, please ensure you always select the Region as Singapore (ap-southeast-1).\nThis is crucial because services like Amazon Bedrock (Claude 3 models) and configurations in the sample code are set up for this Region.\n4. Create IAM User and Access Keys For the application and AWS CLI to interact with AWS, we need to create an IAM User with administrative permissions.\nStep 1: Create User\nGo to IAM Console -\u0026gt; Select Users -\u0026gt; Create user. User name: Enter contract-app-demo. Click Next. Step 2: Set permissions\nSelect Add user to group. Click Create group. User group name: Name it ADMIN. In the Permission policies list, search for and check AdministratorAccess. Click Create user group. After the group is created, select the ADMIN group and click Next. Click Create user. Step 3: Create Access Key\nClick on the newly created User contract-app-demo. Switch to the Security credentials tab. Scroll down to the Access keys section, click Create access key. Select Use case: Local code. Check the confirmation box \u0026ldquo;I understand\u0026hellip;\u0026rdquo;. Click Next. Click Create access key. Click Done. ⚠️ Note: Click Download .csv file to save the keys to your computer. You will not be able to view the Secret access key again after leaving this page.\n5. Configure AWS CLI Open the Terminal on your computer and run the following command to connect to your AWS account:\naws configure Enter the information based on the .csv file you just downloaded:\nAWS Access Key ID: (Get from csv file) AWS Secret Access Key: (Get from csv file) Default region name: ap-southeast-1 Default output format: json (or leave blank) "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/2-proposal/",
	"title": "Proposal",
	"tags": [],
	"description": "",
	"content": "Smart Contract Assistant - AGREEME A Serverless AWS Solution for Contract Review TEEJ - AGREEME 1. Executive Summary The Smart Contract Assistant - AGREEME is a web-based service for individuals and small user groups (freelancers, small business owners, administrative/legal staff) who work with contracts daily but lack deep legal expertise. The solution uses Amazon Bedrock and a fully serverless AWS architecture to analyze contracts, highlight risks, suggest clause edits, and generate summaries and new contract templates.\nBuilt on AWS Amplify, Lambda, API Gateway, DynamoDB, S3, Cognito, EventBridge, and CloudWatch, the platform delivers low-latency, low-cost, and secure AI-assisted contract review, optimized for single users or small teams without complex enterprise features.\n2. Problem Statement The Problem Contracts are often long, complex, and difficult to understand for non-lawyers. Hiring legal consultants for every contract is expensive and not scalable for individuals. A simple, self-service tool focused on fast, accurate contract review for personal or small-business use is not available. Users do not want complex multi-tenant systems or heavy document management; they just want quick risk checks and clear guidance. The Solution The platform provides an AI-powered web app where users can upload contract files (PDF/DOCX) and receive:\nPlain-language explanations of complex clauses. Legal context with highlighted favorable/unfavorable terms. Risk detection and alerts (unbalanced clauses, hidden obligations, potential legal issues). Clause-level suggestions and alternative wording for negotiation. Automatic executive summaries for busy users. Simple contract generation from templates (lease, sale, service, etc.), with AI-guided adjustments for real-world scenarios. All of this runs on a serverless AWS architecture:\nFrontend on AWS Amplify with integrated Hosting, CDN, and WAF. APIs \u0026amp; compute via Amazon API Gateway and AWS Lambda. AI via Amazon Bedrock (GenAI/LLM + embeddings/RAG). Storage \u0026amp; metadata via Amazon S3 and DynamoDB, encrypted by KMS. Identity \u0026amp; security via Amazon Cognito, AWS WAF, IAM, and KMS. Monitoring \u0026amp; events via Amazon CloudWatch and EventBridge. Benefits and Return on Investment Business impact\nReduce contract reading/understanding time by ≥ 70%. Reduce legal advisory costs by ≥ 50% by replacing initial lawyer review with AI. Increase user confidence when signing and negotiating contracts. Technical performance\nContract analysis accuracy ≥ 85% (internal tests + user feedback). Response time ≤ 8 seconds after upload. System uptime ≥ 99.9% for individual users. Cost efficiency\nEstimated AWS infra cost: $27.91/month → $334.92/12 months. Implementation effort: 592 hours total, ≈ $637.12 in team cost (Solution Architect + Software Engineer + AI Engineer). 3. Solution Architecture The platform is implemented as a fully serverless, secure, and scalable architecture optimized for GenAI-driven document processing and RAG-based contract intelligence. High-Level Architecture Entry \u0026amp; Web Layer\nAmazon Route 53 for DNS and friendly domain names. AWS Amplify Hosting for the React/Amplify-based frontend with integrated CDN and WAF. Identity \u0026amp; Access\nAmazon Cognito for user pools and authentication (JWT). API Gateway validates Cognito tokens before invoking Lambda. IAM roles enforce least-privilege access to S3, DynamoDB, Bedrock, KMS, and EventBridge. Backend Compute (Lambda Microservices)\nCore API Lambda for orchestrating requests from API Gateway.\nSpecialized Lambdas for:\nContract generation (ContractGen). General LLM calls (summaries, classification, transformations). RAG search (embedding-based retrieval and knowledge lookup). Metadata updates (DynamoDB). Template management. AI \u0026amp; LLM Layer\nAmazon Bedrock for:\nContract analysis (summary, risk, clause classification). Embeddings and RAG over legal corpus and templates. Contract generation and clause rewrite suggestions. Data \u0026amp; Storage\nAmazon S3 for user-uploaded contracts, generated documents, and templates. Amazon DynamoDB for metadata, templates, and RAG indices. AWS KMS for encryption at rest across S3, DynamoDB, and secrets. Events \u0026amp; Automation\nAmazon EventBridge for asynchronous workflows (background processing, metadata updates, template sync). Monitoring \u0026amp; Operations\nAmazon CloudWatch for logs, metrics, and alarms across Lambda, API Gateway, Amplify, and Bedrock interactions. AWS Services Used Application Stack\nAWS Amplify (frontend hosting, CI/CD, WAF integration). React / Amplify Framework (UI). Amazon API Gateway (API management). AWS Lambda (backend microservices). Amazon Bedrock (LLM inference, embeddings, RAG). Amazon DynamoDB (metadata and knowledge store). Amazon S3 (document storage). AWS KMS (encryption). Amazon EventBridge (event routing). Monitoring \u0026amp; DevOps\nAmazon CloudWatch (observability, alarms). GitLab + Amplify CI/CD (source control and automated deployment). Security\nAWS WAF (via Amplify). AWS IAM (access control). Amazon Cognito (authentication). Component Design Web Interface: Amplify-hosted React app for upload, analysis view, history, and contract generation. API Layer: API Gateway + Lambda for upload handling, Bedrock orchestration, and result retrieval. AI Logic: Bedrock-powered flows for clause analysis, risk scoring, summarization, and template-based generation. Data Layer: S3 for raw/generated contracts; DynamoDB for metadata, templates, and RAG indices. Security \u0026amp; Compliance: Cognito-based auth, KMS encryption, WAF protection, and strict IAM policies. 4. Technical Implementation Implementation Phases Phase 1 – Assessment (Week 1–2)\nGather business and user requirements. Define AI use cases (analysis, risk detection, suggestions). Design high-level architecture and security baseline. Phase 2 – Setup Base Infrastructure (Week 3–4)\nConfigure Amplify project, IAM roles, S3, DynamoDB, KMS. Enable CloudWatch logging and monitoring. Phase 3 – Frontend \u0026amp; Authentication (Week 5–6)\nDeploy frontend via Amplify. Integrate Cognito and Amplify-managed WAF. Implement contract upload and basic dashboard UI. Phase 4 – Backend Core \u0026amp; AI Integration (Week 7–8)\nImplement Lambda APIs and Bedrock integration. Parse contracts, store results in DynamoDB, and return analysis to UI. Phase 5 – Advanced AI Logic \u0026amp; Optimization (Week 9–10)\nImplement risk detection, negotiation suggestions, and RAG search. Improve UX, optimize latency and cost. Phase 6 – Testing, Go-Live \u0026amp; Handover (Week 11–12)\nUnit/integration tests, security and performance validation. Production deployment and knowledge transfer. Technical Requirements Proficiency with AWS Amplify, Lambda, API Gateway, Cognito, S3, DynamoDB, EventBridge, CloudWatch. Access to Amazon Bedrock (Claude, Llama, Titan, etc.) for legal-text analysis. File processing libraries for PDF/DOCX extraction. Clear data-handling and privacy policies for sensitive contract data. 5. Timeline \u0026amp; Milestones Total duration: 12 weeks (6 two-week sprints).\nSprints:\nSprint 1–2: Assessment \u0026amp; base infrastructure. Sprint 3–4: Frontend + authentication. Sprint 5–6: Backend core, AI integration, advanced AI logic, and optimization. Continuous Agile delivery with sprint planning, review, and retrospective.\nKnowledge transfer in final sprints (architecture, operations, CloudWatch, prompt best practices).\n6. Budget Estimation Infrastructure Costs (per month) Infrastructure Costs Service Monthly Cost (USD) 12-Month Cost (USD) Amazon S3 $1.80 $21.60 Amazon API Gateway $0.05 $0.60 Amazon DynamoDB $4.02 $48.24 AWS Secrets Manager $1.08 $12.96 Amazon Route 53 $2.04 $24.48 Amazon Cognito $1.00 $12.00 AWS Amplify $16.25 $195.00 Amazon CloudWatch $0.53 $6.36 Amazon Bedrock $1.13 $13.56 Amazon Lambda $0.01 $0.12 Total $27.91/month $334.92/12 months AWS Pricing Calculator\nImplementation Team Cost Role Hourly Rate (USD) Solution Architect $2.30/hour Software Engineer $0.70/hour AI Engineer $0.70/hour Total estimated project effort: 592 hours → ≈ $637.12 USD.\n7. Risk Assessment Key Risks AI accuracy risk: Misinterpretation of certain legal clauses. File quality risk: Low-quality scans or complex PDFs may break OCR/parsing. Sensitive data risk: Users may upload highly confidential contracts. Cloud dependency risk: Outages in Bedrock or Amplify affect availability. Usage \u0026amp; cost risk: High volume of AI calls increases operating cost. Mitigation Strategies Thorough internal testing and clear disclaimers on AI limitations. Robust file-processing pipeline with validation and user guidance. Strong encryption (KMS), limited retention, and strict access controls. Monitoring and alerts through CloudWatch, with incident runbooks. Prompt optimization, request limits, and budget alerts to control AI cost. 8. Expected Outcomes Technical Outcomes Production-ready, serverless AI contract assistant for individuals/small teams. Stable performance with ≤ 8 seconds analysis time and ≥ 99.9% uptime. Secure handling of sensitive documents with encryption and least-privilege access. Business Outcomes Faster contract understanding and reduced legal risk for non-expert users. Significant reduction in legal advisory costs and manual review time. A scalable SaaS foundation that can be extended to support more features or additional user segments in future phases. "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/1-worklog/1.10-week10/",
	"title": "Week 10 Worklog",
	"tags": [],
	"description": "",
	"content": "📘 Week 10 Worklog – AWS Journey 1. Weekly Objectives During Week 10, the exploration extended into the Artificial Intelligence (AI) and Machine Learning (ML) domain of AWS. The primary goal was to distinguish between building custom models and using pre-trained AI services. Key objectives included:\nAWS AI Ecosystem – Understanding the landscape of ML services (SageMaker) vs. AI Services (Rekognition, Comprehend, Kendra). Amazon SageMaker – Experiencing the full ML lifecycle: Build, Train, and Deploy. Computer Vision \u0026amp; NLP – Implementing image analysis and natural language processing without deep data science expertise using AWS APIs. Intelligent Search – Setting up enterprise search capabilities with Amazon Kendra. This week highlights how AWS democratizes AI, allowing developers to add intelligence to applications via API calls or build custom models with managed infrastructure.\n2. Detailed Work Summary 🗂 Table of Activities Day Tasks Start Date Completion Date Reference Monday - Overview of AI/ML on AWS\n- Learn about ML support services: SageMaker, Rekognition, Comprehend, Kendra, Translate, Polly 10/11/2025 10/11/2025 AWS Journey Tuesday - Practice with Amazon SageMaker:\n- Create Notebook Instance\n- Train simple models (Linear Regression/Image Classification)\n- Deploy endpoint and test predictions 11/11/2025 11/11/2025 AWS Journey Wednesday - Get familiar with Amazon Rekognition\n- Demo face and object recognition in images/videos\n- Integrate Rekognition API into a small web application 12/11/2025 12/11/2025 AWS Journey Thursday - Practice Amazon Comprehend (NLP)\n- Experiment with Amazon Kendra (contextual intelligent search)\n- Compare advantages and limitations of each service 13/11/2025 13/11/2025 AWS Journey Friday - Summarize Week 10 knowledge (AI/ML development process)\n- Real-world applications of AI/ML in business\n- Write practice results report and expansion directions 14/11/2025 14/11/2025 AWS Journey 3. Technical Implementation Details 3.1 Amazon SageMaker (Custom ML) Notebook Instance: Launched a ml.t2.medium Jupyter Notebook instance. Training: Used the built-in XGBoost algorithm to train a model on a sample dataset (e.g., predicting house prices or MNIST). Deployment: Deployed the trained model to a real-time HTTPS Endpoint. Inference: Invoked the endpoint using Python (Boto3) to generate predictions from new data. 3.2 Amazon Rekognition (Computer Vision) Image Analysis: Uploaded photos to S3 and used the DetectLabels API to identify objects (e.g., \u0026ldquo;Car\u0026rdquo;, \u0026ldquo;Tree\u0026rdquo;, \u0026ldquo;Person\u0026rdquo;) with confidence scores. Facial Analysis: Used DetectFaces to estimate age range, emotion, and gender. Integration: Wrote a simple script that triggers a Lambda function when an image is uploaded to S3 to tag it automatically using Rekognition. 3.3 Amazon Comprehend (NLP) Sentiment Analysis: Processed customer review text to determine sentiment (Positive, Negative, Neutral). Entity Recognition: Extracted specific entities (Dates, Locations, Names) from unstructured text documents. 3.4 Amazon Kendra (Intelligent Search) Index Creation: Created a Kendra Index (Developer Edition). Data Source: Connected an S3 bucket containing PDF manuals as the knowledge base. Querying: Tested natural language queries (e.g., \u0026ldquo;How do I reset my device?\u0026rdquo;) in the search console and received precise answers extracted from the documents. 4. Achievements By the end of Week 10, the following outcomes were accomplished:\n✔ Functional Successes Successfully trained and hosted a Machine Learning model using Amazon SageMaker. Implemented Computer Vision features (Face/Object detection) via API calls. extracted insights from text using Amazon Comprehend. Set up a functional document search engine using Amazon Kendra. ✔ Skill Development Understood the distinction between AI Services (High-level APIs for developers) and ML Services (SageMaker for Data Scientists). Gained experience in Model Inference costs and endpoint management. Learned how to integrate AI capabilities into existing applications using AWS SDKs. 5. Challenges Encountered \u0026amp; Resolutions Challenge 1: SageMaker Cost Management\nIssue: SageMaker Notebooks and Endpoints charge per hour even when idle. Fix: Created a \u0026ldquo;Cleanup Script\u0026rdquo; to delete Endpoints and Stop Notebook instances immediately after lab completion to avoid unexpected bills. Challenge 2: IAM Permissions for Rekognition\nIssue: AccessDeniedException when Rekognition tried to read images from the S3 bucket. Fix: Updated the Bucket Policy and IAM Role to explicitly grant s3:GetObject permission to the user/role calling the Rekognition API. Challenge 3: Kendra Indexing Time\nIssue: Kendra took significant time (30+ minutes) to create an index and sync data. Fix: Learned that Kendra is an enterprise-grade service with provisioning time; planned tasks to allow for \u0026ldquo;waiting time\u0026rdquo; or worked on Comprehend while Kendra was initializing. "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/1-worklog/1.11-week11/",
	"title": "Week 11 Worklog",
	"tags": [],
	"description": "",
	"content": "📘 Week 11 Worklog – AWS Journey 1. Weekly Objectives Week 11 focused on Application Modernization via Serverless Architecture. The primary goal was to shift from managing infrastructure (EC2) to focusing on code and business logic using event-driven services. Key objectives included:\nServerless Paradigm – Understanding the shift from Monolithic to Microservices and the benefits of \u0026ldquo;No-Ops\u0026rdquo;. Core Serverless Stack – Mastering AWS Lambda (Compute), API Gateway (Interface), and DynamoDB (NoSQL Data). Security \u0026amp; Auth – Implementing user identity management with Amazon Cognito. Infrastructure as Code – Using the AWS Serverless Application Model (SAM) to define and deploy serverless resources. This week provides the skillset to build highly scalable, cost-effective applications that scale to zero when not in use.\n2. Detailed Work Summary 🗂 Table of Activities Day Tasks Start Date Completion Date Reference Monday - Introduction to Modernization and Serverless concepts\n- Compare Monolithic vs. Microservices architectures\n- Analyze benefits: Cost, Scalability, Operational overhead 17/11/2025 17/11/2025 AWS Journey Tuesday - Practice AWS Lambda: create functions, configure triggers (S3/APIGW)\n- Deploy basic API processing logic\n- Monitor logs via CloudWatch Logs 18/11/2025 18/11/2025 AWS Journey Wednesday - Integrate API Gateway with Lambda (REST API)\n- Connect logic to DynamoDB (CRUD operations)\n- Test API endpoints using Postman 19/11/2025 19/11/2025 AWS Journey Thursday - Configure Cognito for user authentication (User Pool)\n- Integrate Cognito Authorizer into API Gateway\n- Manage access permissions via IAM Roles 20/11/2025 20/11/2025 AWS Journey Friday - Practice deploying a complete Serverless App using AWS SAM\n- Testing, logging, and performance optimization\n- Summarize knowledge and weekly report 21/11/2025 21/11/2025 AWS Journey 3. Technical Implementation Details 3.1 AWS Lambda \u0026amp; Logic Runtime: Created Python 3.9 Lambda functions. Handler: Implemented the lambda_handler(event, context) function to parse JSON input. Triggers: Configured API Gateway as the event source. Logging: Used print() statements to send structured logs to CloudWatch for debugging execution errors. 3.2 API Gateway \u0026amp; DynamoDB Integration API Type: Built a REST API. Integration: Used Lambda Proxy Integration to pass the full HTTP request object to the function. Database: Created a DynamoDB table (Items) with ItemId as the Partition Key. Used boto3 in Lambda to perform put_item, get_item, and scan operations based on HTTP methods (POST, GET). 3.3 Security with Amazon Cognito User Pool: Created a User Pool to handle sign-up and sign-in. App Client: Generated a client ID for the application. Authorizer: Configured a Cognito User Pool Authorizer in API Gateway. Validation: Verified that API requests without a valid Authorization (JWT) token were rejected with 401 Unauthorized. 3.4 AWS SAM (Serverless Application Model) Template: Defined resources (Function, API, Table) in template.yaml. Build: Ran sam build to compile dependencies. Deploy: executed sam deploy --guided to package code to S3 and create the CloudFormation stack automatically. Local Testing: Used sam local invoke to test functions before deployment. 4. Achievements By the end of Week 11, the following outcomes were accomplished:\n✔ Functional Successes Transitioned from managing servers to deploying functions. Built a fully functional Serverless CRUD API. Secured API endpoints using JWT Tokens from Cognito. Automated deployment using AWS SAM, replacing manual console actions. ✔ Skill Development Thoroughly understood the Event-Driven Architecture model. Learned to handle Stateless compute limitations. Mastered the process of decomposing a problem into microservices (Auth service, Data service, Logic service). 5. Challenges Encountered \u0026amp; Resolutions Challenge 1: CORS Errors\nIssue: Calling the API from a browser frontend resulted in Cross-Origin Resource Sharing errors. Fix: Enabled CORS in API Gateway settings and ensured the Lambda function returned the Access-Control-Allow-Origin: * header in the response object. Challenge 2: Lambda Permissions\nIssue: AccessDeniedException when Lambda tried to write to DynamoDB. Fix: Updated the Lambda Execution Role (IAM) to include dynamodb:PutItem and dynamodb:GetItem permissions for the specific table ARN. Challenge 3: Cold Starts\nIssue: The first API call after a period of inactivity took 2-3 seconds. Fix: Acknowledged this as a trade-off of Serverless. Optimized imports in the Python code to reduce initialization time. 6. Plan for Next Week (Preview of Week 12) Comprehensive Review: Consolidate all 11 weeks of knowledge. Real-world Project: Design and implementation of a final Capstone Project. Final Report: Documentation and presentation of the AWS Journey. Certification Prep: Roadmap for AWS Certified Solutions Architect Associate (SAA). 5. Challenges Encountered \u0026amp; Resolutions Challenge 1: SageMaker Cost Management\nIssue: SageMaker Notebooks and Endpoints charge per hour even when idle. Fix: Created a \u0026ldquo;Cleanup Script\u0026rdquo; to delete Endpoints and Stop Notebook instances immediately after lab completion to avoid unexpected bills. Challenge 2: IAM Permissions for Rekognition\nIssue: AccessDeniedException when Rekognition tried to read images from the S3 bucket. Fix: Updated the Bucket Policy and IAM Role to explicitly grant s3:GetObject permission to the user/role calling the Rekognition API. Challenge 3: Kendra Indexing Time\nIssue: Kendra took significant time (30+ minutes) to create an index and sync data. Fix: Learned that Kendra is an enterprise-grade service with provisioning time; planned tasks to allow for \u0026ldquo;waiting time\u0026rdquo; or worked on Comprehend while Kendra was initializing. "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/1-worklog/1.12-week12/",
	"title": "Week 12 Worklog",
	"tags": [],
	"description": "",
	"content": "📘 Week 12 Worklog – AWS Journey 1. Weekly Objectives Week 12 marked the conclusion of the AWS learning roadmap. The primary objective was to synthesize and apply all skills acquired over the past 11 weeks into a comprehensive Capstone Project. Key objectives included:\nHolistic Review – Revisiting core services (EC2, VPC, S3, RDS, Lambda) to ensure deep understanding. Capstone Project – Designing, building, and deploying a production-ready \u0026ldquo;E-Commerce Order Processing System\u0026rdquo; from scratch. Operational Excellence – Implementing CI/CD, Monitoring, and Security best practices. Self-Assessment – Evaluating the architecture against the Well-Architected Framework and preparing for certification. This week transitions the status from \u0026ldquo;Learner\u0026rdquo; to \u0026ldquo;Cloud Practitioner/Architect\u0026rdquo; ready for real-world challenges.\n2. Detailed Work Summary 🗂 Table of Activities Day Tasks Start Date Completion Date Reference Monday - Review all core services: EC2, S3, RDS, DynamoDB, IAM, VPC, Lambda, CloudFront\n- Define requirements and architecture for the final project 24/11/2025 24/11/2025 AWS Journey Tuesday - Start project deployment:\n- Design VPC (Public/Private subnets), Security Groups\n- Configure S3 for static assets, CloudFront for CDN 25/11/2025 25/11/2025 AWS Journey Wednesday - Backend implementation:\n- Build Lambda functions and API Gateway resources\n- Connect to DynamoDB/RDS and implement business logic\n- Integrate CloudWatch Logs \u0026amp; Alarms 26/11/2025 26/11/2025 AWS Journey Thursday - Complete the project:\n- Add Cognito authentication (SignUp/SignIn)\n- Finalize CI/CD pipeline (CodePipeline/CodeBuild)\n- Perform End-to-end system testing 27/11/2025 27/11/2025 AWS Journey Friday - Write final report and documentation\n- Prepare presentation (Architecture diagram, Cost analysis, Security review)\n- Summarize the entire journey and self-evaluate 28/11/2025 28/11/2025 AWS Journey 3. Technical Implementation Details (Capstone Project) 3.1 Project Scope: \u0026ldquo;Serverless E-Commerce Backend\u0026rdquo; Architecture Overview: A 3-tier serverless web application. Frontend: Hosted on S3 + CloudFront. Backend: API Gateway + Lambda. Database: DynamoDB (Product Catalog) + RDS MySQL (Order History). Auth: Amazon Cognito. 3.2 Infrastructure Setup VPC Design: Created a custom VPC with 2 Public Subnets (NAT Gateway, Load Balancer) and 2 Private Subnets (Lambda, RDS). Security Groups: Strictly defined rules: RDS only accepts traffic from Lambda SG on port 3306. Lambda only accepts traffic from internal VPC endpoints. 3.3 Application Logic \u0026amp; Data Lambda Functions: Developed Python functions for CreateOrder, GetProducts, and ProcessPayment. Database Integration: Used Boto3 to scan DynamoDB for product availability. Used PyMySQL to transact SQL commands to RDS for order recording. Monitoring: Created a CloudWatch Dashboard to visualize API Latency and Error Rates (4xx/5xx). 3.4 Automation \u0026amp; Operations CI/CD: Built a pipeline using AWS CodePipeline: Source: GitHub. Build: AWS CodeBuild (Runs unit tests). Deploy: CloudFormation/SAM deploy. Cost Optimization: Implemented S3 Lifecycle policies for logs and set up an AWS Budget alert for the project. 4. Achievements By the end of Week 12 (and the entire course), the following outcomes were accomplished:\n✔ Project Success Successfully delivered a production-grade cloud application combining 10+ AWS services. Demonstrated ability to integrate Relational (RDS) and Non-Relational (DynamoDB) data stores in a single system. Achieved a Secure (Cognito/IAM), Reliable (Multi-AZ), and Performant (CloudFront/Caching) architecture. ✔ Professional Growth Mastered the process of building Cloud applications from Requirement Analysis → Design → Implementation → Operation. Developed strong troubleshooting skills for complex distributed systems. Completed the learning roadmap and fully prepared for the AWS Certified Solutions Architect – Associate exam. 5. Challenges Encountered \u0026amp; Resolutions Challenge 1: Lambda in VPC Connectivity\nIssue: Lambda function in Private Subnet lost internet access (could not reach public APIs or DynamoDB). Fix: Configured a NAT Gateway in the Public Subnet and updated Route Tables. Also used a VPC Endpoint for DynamoDB to keep traffic internal and reduce NAT costs. Challenge 2: CodePipeline Build Errors\nIssue: buildspec.yml failed due to missing Python dependencies. Fix: Updated the install phase in the buildspec file to execute pip install -r requirements.txt. Challenge 3: CORS with Cognito\nIssue: API Gateway rejected requests with valid tokens due to CORS headers missing on 401 responses. Fix: Configured \u0026ldquo;Gateway Responses\u0026rdquo; in API Gateway to include CORS headers even for Unauthorized errors. "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/5-workshop/5.4-backed/5.4.3-api-secrets/",
	"title": "API Gateway &amp; Secrets",
	"tags": [],
	"description": "",
	"content": "The final step of the Backend section is to expose connection endpoints (API Gateway) and configure security.\n1. Create API Gateway Trigger Re-open the ragsearch Lambda Function.\nIn the Function overview section, click Add trigger. Select source: API Gateway.\nIntent: Create a new API. API type: HTTP API. Security: IAM. Click Add.\n👉 Important: After creation, copy the API Endpoint URL (format https://...amazonaws.com) and save it to Notepad.\n2. Create Secrets Manager Search for the Secrets Manager service -\u0026gt; Select Store a new secret. Secret type: Select Other type of secret. Key/value pairs: Key: JWT_SECRET Value: (Enter any random password of your choice) Click Next -\u0026gt; Name the Secret (arbitrary) -\u0026gt; Next -\u0026gt; Store. 3. Update ARN for Lambda The Lambda functions need to know each other\u0026rsquo;s addresses (ARNs) to call one another.\nOpen 3 browser tabs corresponding to the 3 Lambda functions (ragsearch, generate_contract, CallLLM). Copy the Function ARN of each function (Located at the top right corner of the Function overview section). Return to the Environment variables configuration tab of the ragsearch function. Click Edit and add the following variables: LAMBDA_RETRIEVAL_ARN: Paste the ARN of the ragsearch function. LAMBDA_REVIEW_ARN: Paste the ARN of the CallLLM function. LAMBDA_GENERATE_ARN: Paste the ARN of the generate_contract function. Click Save. "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/4-eventparticipated/4.3-event3/",
	"title": "Event 3",
	"tags": [],
	"description": "",
	"content": "AWS Cloud Mastery Series #1: AI/ML/GenAI on AWS Time: Saturday, November 15, 2025, 8:30 – 12:00 Location: AWS Vietnam Office, Bitexco Financial Tower, District 1, Ho Chi Minh City Role: Attendee (Learner)\nEvent Objectives Gain a comprehensive overview of the AI/ML landscape in Vietnam and the holistic AWS AI service ecosystem. Deep dive into Amazon SageMaker to understand the end-to-end MLOps process, from data preparation to model deployment. Master core Generative AI technologies on Amazon Bedrock, specifically Prompt Engineering and RAG. Understand the strategic shift from passive Chatbots to autonomous Agentic AI. Speakers Danh Hoang Hieu Nghi: Expert in Amazon Bedrock AgentCore and the Agent ecosystem. Dinh Le Hoang Anh: Demonstrated Pre-trained AI Services and real-world applications (AMZ Photo). Lam Tuan Kiet: Specialist in Generative AI, Prompt Engineering techniques, and RAG architecture. Key Highlights The event focused on three main pillars of the AWS AI Stack:\nAI Services (Pre-trained):\nIntroduction to ready-to-use services like Amazon Rekognition (image analysis), Transcribe (speech-to-text), and Polly (text-to-speech). Demo of AMZ Photo: A smart photo management app featuring face search and a multimodal chatbot built with the Pipecat framework. Generative AI with Amazon Bedrock:\nComparison of Foundation Models (Claude, Llama, Titan). Optimization techniques for Prompt Engineering: Zero-shot, Few-shot, and Chain-of-Thought (CoT). RAG (Retrieval-Augmented Generation) Architecture: Combining language models with enterprise data via Vector Databases. Agentic AI \u0026amp; AgentCore:\nThe evolution from Generative AI Assistants to Agentic AI (capable of autonomous execution). Introduction to AgentCore: Components such as Memory, Identity, Gateway, and Code Interpreter that enable Agents to operate at scale. Key Takeaways Prompt Engineering Mindset: Prompting is not just simple questioning but a natural language programming technique. Using Chain-of-Thought significantly improves the model\u0026rsquo;s ability to solve complex logic problems compared to standard querying. Standard RAG Process: Understanding the data flow (Data Ingestion -\u0026gt; Embeddings -\u0026gt; Vector Store -\u0026gt; Semantic Search) is crucial to solving AI hallucination issues. Power of Pre-trained Services: Building models from scratch is not always necessary. Leveraging existing APIs (like Rekognition, Textract) drastically reduces Time-to-market. Agent Architecture: grasping the distinction between a passive Chatbot and an active Agent capable of using Tools and accessing the Browser Viewer. Applying to Work Optimize Internal Chatbots: Immediately apply RAG architecture and Titan Text Embeddings V2 to upgrade the company\u0026rsquo;s technical documentation support bot, ensuring answers are accurate and based on actual data sources. Automate Document Processing: Propose using Amazon Textract combined with Comprehend to extract and classify information from invoices/contracts, replacing manual data entry processes. Research Agentic Workflows: Pilot a simple Agent using LangGraph or Amazon Bedrock Agents to automate the daily market news summarization process for the Business team. Personal Experience Attending the AWS Cloud Mastery Series at the AWS Vietnam office was professionally valuable for me:\nMr. Lam Tuan Kiet\u0026rsquo;s session was highly practical, especially the illustrative examples of Prompt Engineering. It helped me realize previous mistakes in interacting with LLMs and how to scientifically improve response quality. Mr. Dinh Le Hoang Anh\u0026rsquo;s demo with the AMZ Photo app was impressive. The integration of Multimodal capabilities and real-time processing demonstrated the immense potential of combining various AWS services. Mr. Danh Hoang Hieu Nghi\u0026rsquo;s topic on Agentic AI was very forward-looking. Although the AgentCore architecture is complex, it opens up a new perspective where AI is not just a support tool but can become a true \u0026ldquo;virtual employee.\u0026rdquo; Event Photos Summary: An in-depth workshop that balanced foundational theory with practical demos. Knowledge regarding RAG and Agentic AI will be the focus of my research in the near future.\n"
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/5-workshop/5.3-infrastructure/",
	"title": "Infrastructure Setup",
	"tags": [],
	"description": "",
	"content": "Introduction In this module, we will start building the foundation for the Smart Contract Assistant application. Before deploying the logic processing code (Lambda) or the user interface (Frontend), we need to prepare a location to store data.\nWe will set up two critical AWS services:\nAmazon S3 (Simple Storage Service):\nActs as a \u0026ldquo;Data Lake\u0026rdquo;. Stores contract templates. Stores encoded legal text data (Embeddings) to serve the intelligent search feature (RAG). Amazon DynamoDB:\nActs as a high-performance NoSQL Database. Stores user profiles and manages login sessions. Stores real-time chat history with low latency. List of Steps We will proceed through the following steps:\nCreate Amazon S3 Bucket: Create a bucket and folder structure, and upload sample data. Create DynamoDB Tables: Initialize the 3 necessary data tables (Users, ChatSessions, ChatMessages). Important Note: Please ensure you have selected the Asia Pacific (Singapore) ap-southeast-1 Region before starting to create any of the resources below.\n"
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/3-blogstranslated/",
	"title": "Translated Blogs",
	"tags": [],
	"description": "",
	"content": "Blog 1 - Model Customization, RAG, or Both: A Case Study with Amazon Nova In the rapidly evolving Generative AI landscape, choosing the optimal strategy to improve the accuracy of large language models (LLMs) for domain tools can be a big challenge. This paper conducts a case study using Amazon Nova models to compare the performance of three multivariate approaches: Model Fine-Tuning (Fine-Tuning), Retrieval Augmentation Generation (RAG), and Hybrid. Through real-world experiments on the AWS dataset, the paper provides an in-depth analysis of performance, cost, and technical complexity, helping developers and enterprises make the most informed decisions for their AI applications.\nBlog 2 - Migrating CDK Version 1 Applications to CDK Version 2 with Amazon Q Developer With AWS CDK v1 officially retired, developers are faced with an urgent need to upgrade to v2 to ensure security and take advantage of the latest features. This article details how to use Amazon Q Developer – an AI-powered programming assistant – to automate and accelerate this migration. From updating dependencies and modifying imports to debugging and documentation, you will discover how Amazon Q makes infrastructure-as-code (IaC) modernization simple, fast, and less error-prone.\nBlog 3 - Melting The Ice - How Natural Intelligence Simplified a Data Lake Migration to Apache Iceberg Migrating a large-scale, operational Data Lake to a modern tabular format like Apache Iceberg often comes with the risk of system disruption and technical complexity. This article is a success story of Natural Intelligence\u0026rsquo;s zero-downtime migration from Apache Hive to Apache Iceberg. The authors share details of an innovative hybrid migration strategy that uses automated schema and change synchronization (CDC) between the old and new systems. This is a valuable reference on architecture and processes for organizations looking to safely and efficiently modernize their big data platforms.\n"
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/5-workshop/5.4-backed/",
	"title": "Building the Backend (Lambda)",
	"tags": [],
	"description": "",
	"content": "Introduction In this section, we will build the core processing component (Backend) for the Smart Contract Assistant application. The system utilizes a Serverless architecture with AWS Lambda to handle user requests and interact with AI (Amazon Bedrock).\nKey Components We will proceed through the following steps:\nCreate RAG Search Lambda: Build an intelligent search function that connects to vector data in S3. Create Auxiliary Lambdas: Build the contract generation function (generate_contract) and the general chat function (CallLLM). Configure API \u0026amp; Security: Set up API Gateway to expose connection endpoints and Secrets Manager to store secret keys. "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/4-eventparticipated/4.4-event4/",
	"title": "Event 4",
	"tags": [],
	"description": "",
	"content": "AWS Cloud Mastery Series #2: DevOps on AWS Time: Monday, November 17, 2025, 8:30 – 17:00 Location: AWS Vietnam Office, Bitexco Financial Tower, District 1, Ho Chi Minh City Role: Attendee (Learner)\nEvent Objectives Shift mindset from traditional system administration to DevOps Mindset and CI/CD culture. Master the AWS Developer Tools suite (CodeCommit, CodeBuild, CodeDeploy, CodePipeline) to automate development workflows. Approach Infrastructure as Code (IaC) techniques with CloudFormation and AWS CDK. Deep dive into Containerization technologies (Docker, ECR, ECS/EKS) and operating Microservices on AWS. Master Monitoring \u0026amp; Observability to supervise full-stack systems. Speakers AWS Technical Team: Solutions Architects (SA) specializing in DevOps and Modern Application Development. Experts in Containers and Serverless computing. Key Highlights The full-day event covered the journey from mindset to detailed practice:\nCI/CD Pipeline on AWS:\nFull workflow demo: Source Control (CodeCommit) -\u0026gt; Build \u0026amp; Test (CodeBuild) -\u0026gt; Deploy (CodeDeploy) -\u0026gt; Orchestration (CodePipeline). Safe deployment strategies like Blue/Green Deployment and Canary Release to minimize downtime risks. Infrastructure as Code (IaC):\nThe difference between manual \u0026ldquo;ClickOps\u0026rdquo; on the Console and defining infrastructure as code. Introduction to AWS CDK (Cloud Development Kit): Allows using familiar programming languages (Python, TS, Java) to provision infrastructure instead of writing lengthy YAML in CloudFormation. Container Services:\nReference architecture for Microservices using Amazon ECS and Fargate (Serverless compute for containers). The process of packaging Docker Images, storing on ECR, and automatic security scanning. Observability:\nBeyond basic logs and metrics, understanding systems through Distributed Tracing with AWS X-Ray. Key Takeaways DevOps Metrics: Understanding DORA metrics (Deployment Frequency, Lead Time for Changes\u0026hellip;) to measure team efficiency. Git Strategies: Knowing how to choose the right branching model (GitFlow vs. Trunk-based development) for different project scales. Container Workflow: Mastering the lifecycle of a container from writing a Dockerfile to running in a Production environment with Auto Scaling. Drift Detection: How to detect and handle discrepancies between actual Cloud configuration and the defined IaC code. Applying to Work Automate Build \u0026amp; Deploy: Propose shifting from manual deployment (copy-pasting files) to using AWS CodePipeline combined with existing Github Actions to reduce human error. Implement IaC: Start using AWS CDK for new projects to easily manage infrastructure versions and reuse code (Constructs). Improve Monitoring: Integrate CloudWatch Alarms and X-Ray into critical services to receive early warnings before customers report errors. Personal Experience Attending a full day on DevOps helped me \u0026ldquo;crack\u0026rdquo; many aspects of professional software processes:\nThe CI/CD Pipeline demo was visually satisfying. Just pushing code to the repo triggered automated tests, docker image builds, and server updates without manual intervention. I really enjoyed the session on AWS CDK. I used to be intimidated by thousand-line CloudFormation templates, but with CDK, defining VPCs, Subnets, and EC2s takes just a few lines of clean, logical Python code. The ECS architecture drawn on the whiteboard (as captured in the photos) helped me clearly visualize the request flow from Internet Gateway -\u0026gt; ALB -\u0026gt; Target Group -\u0026gt; Container, and how to separate Public/Private subnets for security. Event Photos Summary: \u0026ldquo;Automation is King.\u0026rdquo; The event made me realize that any task repeated more than twice should be automated. DevOps is not just tools; it\u0026rsquo;s a culture of efficiency.\n"
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/4-eventparticipated/",
	"title": "Events Participated",
	"tags": [],
	"description": "",
	"content": " In this section, you should list and describe in detail the events you have participated in during your internship or work experience.\nEach event should be presented in the format Event 1, Event 2, Event 3…, along with the following details:\nEvent name Date and time Location (if applicable) Your role in the event (attendee, event support, speaker, etc.) A brief description of the event’s content and main activities Outcomes or value gained (lessons learned, new skills, contribution to the team/project) This listing helps demonstrate your actual participation as well as the soft skills and experience you have gained from each event. During my internship, I participated in two events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: Vietnam Cloud Day 2025 : Ho Chi Minh City Connect Edition for Builders\nDate \u0026amp; Time: Thursday, 18 September 2025, 9:00 – 17:30\nLocation: 36th floor, 2 Hai Trieu Street, Ben Nghe Ward, District 1, Ho Chi Minh City\nRole: Attendee\nDescription: A deep-dive tech event for Builders, focusing on strategic trends such as Agentic AI, Data Foundation solutions, and the AI-Driven Development Lifecycle (AI-DLC). It also covered essential security standards for Generative AI.\nOutcomes: Gained insights into the core mindset and architecture of Agentic AI; deeply understood the AI-DLC process to shift from coding to architecting and reviewing; updated key knowledge on multi-layer security for GenAI applications.\nEvent 2 Event Name: Workshop Data Science on AWS\nDate \u0026amp; Time: 16 October 2025, 9:30 – 11:45\nLocation: Hall A - FPT University HCMC (FPTU HCMC)\nRole: Attendee\nDescription: A practical workshop on Data Science on AWS platform, focusing on the AI/ML ecosystem stack, key AI Services (Vision, Speech, Text), and the model training workflow using Amazon SageMaker.\nOutcomes: Understood the big picture of AWS AI/ML Stack; learned how to quickly integrate AI Services (Rekognition, Polly, Lex) into applications; grasped the Feature Engineering process and cost optimization strategies when running models on the Cloud.\nEvent 3 Event Name: AWS Cloud Mastery Series #1: AI/ML/GenAI on AWS\nDate \u0026amp; Time: Saturday, 15 November 2025, 8:30 – 12:00\nLocation: AWS Vietnam Office, Bitexco Financial Tower, HCMC\nRole: Attendee\nDescription: An intensive series on Generative AI, focusing on Prompt Engineering techniques, RAG (Retrieval-Augmented Generation) architecture, and building AI Agents with Amazon Bedrock.\nOutcomes: Deeply understood the differences between Foundation Models (Claude, Llama, Titan); mastered Chain-of-Thought prompting; learned the Bedrock Agent architecture to build smart chatbots capable of using tools.\nEvent 4 Event Name: AWS Cloud Mastery Series #2: DevOps on AWS\nDate \u0026amp; Time: Monday, 17 November 2025, 8:30 – 17:00\nLocation: AWS Vietnam Office, Bitexco Financial Tower, HCMC\nRole: Attendee\nDescription: A full-day deep dive into DevOps culture and tools on AWS. Content covered building automated CI/CD pipelines (CodePipeline), managing infrastructure as code (IaC) with AWS CDK, Containerization techniques (ECS/EKS), and comprehensive system Observability.\nOutcomes: Shifted mindset from manual administration to \u0026ldquo;Automation is King\u0026rdquo;; mastered the AWS Developer Tools suite to eliminate manual operations; learned how to use AWS CDK to define infrastructure using programming languages; gained a clear understanding of Microservices architecture and safe deployment strategies (Blue/Green).\nEvent 5 Event Name: AWS Cloud Mastery Series #3: AWS Well-Architected Security Pillar\nDate \u0026amp; Time: Saturday, 29 November 2025, 8:30 – 12:00\nLocation: AWS Vietnam Office, Bitexco Financial Tower, HCMC\nRole: Attendee\nDescription: A workshop dedicated to the Security Pillar, covering Zero Trust principles, Modern Identity and Access Management (IAM), Detection-as-Code, and Automated Incident Response.\nOutcomes: Adopted the \u0026ldquo;No ClickOps\u0026rdquo; mindset in security (configuration must be code); deeply understood the Shared Responsibility Model; learned to use EventBridge and Lambda to build systems that automatically remediate vulnerabilities upon detection; gained confidence in designing multi-layered secure network architectures.\n"
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/4-eventparticipated/4.5-event5/",
	"title": "Event 5",
	"tags": [],
	"description": "",
	"content": "AWS Cloud Mastery Series #3: AWS Well-Architected Security Pillar Time: Saturday, November 29, 2025, 8:30 – 12:00 Location: AWS Vietnam Office, Bitexco Financial Tower, District 1, Ho Chi Minh City Role: Attendee (Learner)\nEvent Objectives Deeply understand the Security Pillar of the AWS Well-Architected Framework, shifting mindset from Perimeter Security to Zero Trust. Master Modern IAM architecture for managing identity and access at scale. Learn to build an \u0026ldquo;immune system\u0026rdquo; with Detection-as-Code and Automated Remediation. Practice Infrastructure Protection and Data Protection strategies against common attack vectors. Speakers Tran Duc Anh - Cloud Security Engineer Trainee, AWS Cloud Club Captain SGU. Nguyen Tuan Thanh - Cloud Engineer Trainee. Nguyen Do Thanh Dat - Cloud Engineer Trainee. (Featuring the technical team from AWS Cloud Clubs and First Cloud Journey) Key Highlights The workshop deep-dived into the 5 core pillars of security on AWS:\nIdentity \u0026amp; Access Management (IAM):\n\u0026ldquo;Kill long-lived credentials\u0026rdquo; principle: Replace permanent Access Keys with IAM Roles and Temporary Tokens. Using IAM Access Analyzer to detect overly permissive policies (like Principal: *). Demo on configuring SSO (Single Sign-On) for centralized access management across multiple AWS accounts. Detection \u0026amp; Continuous Monitoring:\nImplementing Detection-as-Code: Using CloudFormation/Terraform to enable GuardDuty and Security Hub organization-wide instead of manual clicking. Runtime Monitoring: GuardDuty Agent installed deep within EC2/EKS to detect strange processes (process injection) or anomalous file access. Infrastructure Protection:\nDefense in Depth strategy: Layering WAF, Shield at the Edge -\u0026gt; Network Firewall in VPC -\u0026gt; Security Groups at the Instance level. Clearly distinguishing the roles of Security Groups (Stateful) and NACLs (Stateless) in defending against DDoS or Lateral Movement attacks. Incident Response (IR):\nAutomating the incident response process: CloudTrail logs -\u0026gt; EventBridge filter -\u0026gt; Lambda function to automatically isolate malware-infected EC2s or rotate exposed credentials. Key Takeaways \u0026ldquo;No ClickOps\u0026rdquo; Mindset: In security, manual configuration is the enemy. Every security rule, from Security Groups to GuardDuty detectors, must be defined by Code (IaC) and version controlled. Shared Responsibility Model: Clearly understanding what AWS handles (Security of the Cloud) and what we must handle (Security in the Cloud), especially regarding Data Encryption (at rest/in transit). Power of EventBridge: It\u0026rsquo;s not just a message bus but the \u0026ldquo;central nervous system\u0026rdquo; connecting Detection to Remediation in real-time. Micro-segmentation: Never trust the internal network. Segmenting VPCs into public/private subnets and tightening them with Security Groups is mandatory. Applying to Work Immediate IAM Review: Audit all IAM Users in company projects, delete unused Access Keys, and enable MFA for 100% of accounts, especially Root accounts. Deploy GuardDuty: Propose enabling GuardDuty at the Organization level to instantly detect basic threats (like crypto mining, port scanning) with minimal configuration effort. Secret Management: Stop hardcoding passwords in code or .env files immediately. Switch to AWS Secrets Manager and configure Automatic Rotation for DB credentials. Personal Experience Honestly, it was a \u0026ldquo;brain-stretching\u0026rdquo; but incredibly high-quality morning. Although the speakers were Trainees/Students, the knowledge was deep and battle-tested:\nI was very impressed with the \u0026ldquo;Prevention - Nobody has time for incidents\u0026rdquo; slide. The quote \u0026ldquo;Public buckets = your data on the evening news\u0026rdquo; was both funny and poignant regarding the importance of blocking S3 public access. The Automated Remediation demo opened my eyes. I used to think Incident Response required someone monitoring screens 24/7, but it turns out Lambda can be written to \u0026ldquo;neutralize\u0026rdquo; threats the moment they appear. The Network Attack Vectors slide visually mapped out a Hacker\u0026rsquo;s path from the Internet through protection layers, helping me clearly visualize a secure network architecture. Event Photos Summary: Security is not a blocker, but a key enabler for businesses to move faster and safer. This event gave me significant confidence to propose security solutions for upcoming projects.\n"
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/5-workshop/5.5-fullstack/",
	"title": "Fullstack Deployment",
	"tags": [],
	"description": "",
	"content": "Introduction We have the discrete components: S3, DynamoDB, Lambda Functions. Now it is time to \u0026ldquo;assemble\u0026rdquo; them into a complete application.\nIn this section, we will:\nDeploy Backend Code: Use the Serverless Framework to automatically configure connections, create a User Pool (Cognito) to manage logins, and update the backend code. Deploy Frontend (Amplify): Push the web interface source code to GitLab and connect to AWS Amplify to build and host the website. After completing this section, you will have a live, functioning website link.\n"
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/5-workshop/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": "Building a Smart Contract Assistant on AWS Introduction The AI Contract Intelligence platform is a web service designed for individuals and small teams (freelancers, small business owners, administrative/legal personnel) who work with contracts daily but lack deep legal expertise. The solution utilizes Amazon Bedrock and a fully serverless AWS architecture to analyze contracts, highlight risks, suggest clause revisions, and generate summaries as well as new contract templates.\nBuilt on AWS Amplify, Lambda, API Gateway, DynamoDB, S3, Cognito, EventBridge, and CloudWatch, the platform provides AI-powered contract review capabilities with low latency, low cost, and high security. It is optimized for single users or small teams without the need for complex enterprise-grade features.\nThe primary goal of the application is to assist users (such as lawyers, compliance officers, or business owners) in performing complex tasks such as:\nInformation Retrieval: Q\u0026amp;A regarding legal clauses based on an available repository of legal texts. Automated Drafting: Requesting the AI to generate contract drafts based on standard templates and provided information. Analysis: Summarizing and auditing contract content. Solution Architecture The solution is built entirely on AWS Serverless architecture, helping optimize operational costs and scalability. A highlight of the architecture is the application of the RAG (Retrieval-Augmented Generation) technique, allowing AI to retrieve accurate information from your own data repository instead of relying solely on pre-trained knowledge.\nThe key components of the system include:\nAI \u0026amp; LLM (Amazon Bedrock):\nThis is the \u0026ldquo;heart\u0026rdquo; of the application. We use Amazon Bedrock to access advanced Large Language Models (LLMs) such as Claude 3 (Haiku/Sonnet) via API. Bedrock is responsible for understanding the context of questions, synthesizing information from documents, and generating natural responses. Backend \u0026amp; Computing (AWS Lambda \u0026amp; API Gateway):\nUses AWS Lambda to run Python code snippets to handle business logic (vector search, calling Bedrock API, processing input data) without managing servers. Amazon API Gateway acts as the gateway, receiving requests from the user side (Frontend) and routing them to the corresponding Lambda functions. Database \u0026amp; Storage (Amazon S3 \u0026amp; DynamoDB):\nAmazon S3: Acts as the storage repository (\u0026ldquo;Data Lake\u0026rdquo;) containing sample contract files (.docx, .pdf), metadata files, and embedded vector data to serve semantic search. Amazon DynamoDB: A high-performance NoSQL database used to store user information, manage work sessions, and chat message history, ensuring the lowest latency for retrieval. Frontend \u0026amp; Authentication (AWS Amplify \u0026amp; Cognito):\nThe user interface is built using React/VueJS and is automatically deployed via AWS Amplify. Amazon Cognito provides sign-up, sign-in, and security mechanisms, ensuring only authorized users can access the application. Learning Objectives After completing this workshop, you will master:\nHow to deploy a Full-stack application on AWS from scratch. Understanding and applying the RAG model to combine enterprise data with the power of LLMs. Skills in working with Infrastructure as Code via the Serverless Framework. Configuring and integrating core AWS services: S3, DynamoDB, Lambda, API Gateway, and Bedrock. Practice Content Workshop Overview Prerequisites Infrastructure Setup (S3, DynamoDB) Building Backend (Lambda, IAM) Fullstack Deployment (Amplify) Clean Up Resources "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/5-workshop/5.6-cleanup/",
	"title": "Clean Up Resources",
	"tags": [],
	"description": "",
	"content": "0. Backup Data (Optional) If you want to keep the data before deleting, perform this step. Otherwise, skip to step 1.\nDynamoDB: Go to each table (ChatMessages, ChatSessions, Users) -\u0026gt; Explore items tab -\u0026gt; Select Scan -\u0026gt; Select all -\u0026gt; Export to CSV. S3: Go to the Bucket -\u0026gt; Select all files -\u0026gt; Actions -\u0026gt; Download. 1. Delete API Gateway Why delete first? API Gateway is currently connected to Lambda. Deleting it first helps sever dependent connections.\nAccess the API Gateway Console.\nFind and delete the following 2 APIs one by one:\nragsearch-API (Manually created API). dev-ai-contract-backend (API created by Serverless). Select the API -\u0026gt; Click Delete -\u0026gt; Confirm deletion.\n2. Delete Lambda Functions Access the Lambda Console. You will see a list of about 6 functions (3 manually created and 3 created by Serverless).\nCheck the checkbox next to all of the following functions:\nragsearch CallLLM generate_contract ai-contract-backend-dev-api ai-contract-backend-dev-postConfirmation ai-contract-backend-dev-custom-resource-existing-cup Click the Actions button -\u0026gt; Select Delete.\nConfirm deletion. 3. Delete Cognito User Pool Access Cognito Console -\u0026gt; User pools. Click on the pool name: ai-contract-backend-user-pool-dev. Click Delete. Select the deletion verification method (usually typing the pool name) and confirm. 4. Delete DynamoDB Tables Access DynamoDB Console -\u0026gt; Tables. Check the 3 tables: ChatMessages ChatSessions Users Click Delete. Note: Uncheck the Create a backup box (to avoid time and backup costs) -\u0026gt; Type confirm to confirm deletion. 5. Delete S3 Buckets Note: AWS requires the Bucket to be Empty before allowing it to be Deleted.\nPerform the following process for both buckets (The main Bucket contract-app-demo... and the deployment bucket ai-contract-backend-dev-serverless...):\nEmpty the Bucket:\nSelect the Bucket -\u0026gt; Click Empty. Type permanently delete to confirm -\u0026gt; Click Empty. Click Exit to return. Delete the Bucket:\nOnce the bucket is empty, select that Bucket -\u0026gt; Click Delete. Type the bucket name to confirm -\u0026gt; Click Delete bucket. 6. Delete Secrets Manager Access Secrets Manager Console. Click on the secret name you created (e.g., JWT_SECRET\u0026hellip;). Click Actions -\u0026gt; Delete secret. Set the Waiting period to the minimum (7 days) -\u0026gt; Confirm deletion. 7. Delete Amplify App Access AWS Amplify. Select the App SmartContractAssistant (or the name you assigned). On the left menu, select App settings -\u0026gt; General settings. Click the Delete app button on the right corner -\u0026gt; Confirm. Conclusion Congratulations! You have completed the workshop and cleaned up your resources.\nList of deleted resources:\n✅ 2 API Gateways ✅ 6 Lambda Functions ✅ 1 Cognito User Pool ✅ 3 DynamoDB Tables ✅ 2 S3 Buckets ✅ 1 Secrets Manager Secret ✅ 1 Amplify App "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/6-self-evaluation/",
	"title": "Self-Assessment",
	"tags": [],
	"description": "",
	"content": "During my internship at AWS First Cloud Journey from September 4, 2025 to December 09, 2025, I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI directly participated in building and deploying the Serverless architecture for the Agreeme project. Through this process, I gained specific experiences:\nTechnical Skills: Mastered the operation and integration of core AWS services (AWS Lambda, API Gateway, DynamoDB), practiced Infrastructure as Code (IaC) for infrastructure management, and built CI/CD pipelines for automated testing and deployment. Mindset: Developed a mindset for designing scalable systems and began addressing Cloud Cost Optimization challenges. Soft Skills: Honed effective Teamwork skills within an Agile/Scrum environment, along with transparent task management and progress reporting. In terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ☐ ✅ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ✅ ☐ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ✅ ☐ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ☐ ✅ ☐ 5 Discipline Adhering to schedules, rules, and work processes ✅ ☐ ☐ 6 Progressive mindset Willingness to receive feedback and improve oneself ✅ ☐ ☐ 7 Communication Presenting ideas and reporting work clearly ✅ ☐ ☐ 8 Teamwork Working effectively with colleagues and participating in teams ✅ ☐ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ☐ ✅ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ✅ ☐ ☐ 12 Overall General evaluation of the entire internship period ✅ ☐ ☐ Needs Improvement To develop into a more professional engineer, I recognize the need to focus on improving the following areas:\n1. Technical Depth Optimization Mindset: Move beyond just \u0026ldquo;making it work\u0026rdquo; to focusing on Performance Optimization and Cost Optimization for the AWS services used in the project. Security Awareness: Apply security principles (IAM roles, Security Groups) more strictly and proactively from the early development stages. 2. Workflow \u0026amp; Process Documentation Discipline: Maintain a habit of writing clear Technical Documentation and comprehensive code comments to facilitate future maintenance. Time Management: Improve the ability to estimate time for complex tasks to ensure more accurate deadlines. 3. Soft Skills Proactive Communication: Report blockers or issues earlier instead of trying to solve them alone for too long. Technical English: Enhance the ability to comprehend in-depth documentation and communicate technical concepts in English. "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/7-feedback/",
	"title": "Sharing and Feedback",
	"tags": [],
	"description": "",
	"content": "Part 1: Professional Growth \u0026amp; Skills 1. Knowledge \u0026amp; Technology\nTech stack: Over the past 3 months, I have truly mastered AWS Core Services (EC2, S3, RDS) and Infrastructure as Code (Terraform) – topics I previously only knew in theory. Additionally, I got hands-on experience with CI/CD pipelines, an area that AI students like me often overlook. Theory vs. Reality: At university, as long as the code runs, it\u0026rsquo;s done (Happy Case). At FCJ, I learned that running code is not enough; it must be cost-optimized, secure, and scalable. Biggest Challenge: Migrating a database from on-premise to the Cloud. Handling data synchronization without causing service downtime was the toughest problem I solved. 2. Soft Skills \u0026amp; Workflow\nSkills: The skill of \u0026ldquo;Asking the right questions\u0026rdquo; is what I honed the most. Instead of just asking \u0026ldquo;Why is this broken?\u0026rdquo;, I learned to frame it as \u0026ldquo;I tried A and B, but got result C. Is my approach correct?\u0026rdquo;. Workflow: I really appreciate the strict Code Review process. The feedback from seniors didn\u0026rsquo;t just catch syntax errors but also suggested ways to write cleaner, more optimized code. Part 2: People \u0026amp; Culture 3. Mentorship Experience\nStyle: My mentor follows a \u0026ldquo;Coaching\u0026rdquo; style – never giving the answer immediately but asking leading questions so I can find the solution myself. This was challenging at first but helped me retain knowledge effectively. Impression: The mentor\u0026rsquo;s calmness during system incidents. Instead of panicking, he guided the team step-by-step to trace logs and fix the issue systematically. 4. Culture \u0026amp; Connection\nAtmosphere: The working environment is very flat and open. I felt comfortable debating technical solutions with Mentors without fear of judgment, as long as I had logical arguments. Events: Beyond the \u0026ldquo;day in the life\u0026rdquo; office experiences, FCJ and the seniors organized many professional sharing sessions, helping everyone gain a deeper and broader perspective on AWS. Part 3: Operations \u0026amp; Support 5. Admin Team \u0026amp; FCJ Team Role\nOnboarding: The internal \u0026ldquo;Cloud Journey\u0026rdquo; documentation is incredibly detailed. It even predicts common errors and provides ready-made solutions for newbies. Support: The Admin team is very enthusiastic. Just be brave enough to ask, and you will definitely get support. 6. Facilities\nWorkspace: Open office layout with comfortable ergonomic chairs. Although the space was occasionally a bit crowded, the atmosphere remained pleasant and never stifling. Part 4: Retrospective \u0026amp; Suggestions Key Takeaways\nThe most memorable (and painful) lesson: Every mistake on the Cloud costs actual money. Creating AWS services and forgetting to delete resources is a sure way to make your wallet \u0026ldquo;evaporate\u0026rdquo; in the blink of an eye.\nAreas for Improvement (Feedback)\nImprovement: The program should add a workshop on \u0026ldquo;Technical Writing,\u0026rdquo; as I noticed many interns (including myself) are still weak in this area. Training: I hope to have more in-depth sharing sessions on specific service techniques in future batches. Final Thoughts\nThank you, FCJ team, for giving me a memorable experience and wonderful colleagues! "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]
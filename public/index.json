[
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/3-blogstranslated/3.1-blog1/",
	"title": "Blog 1",
	"tags": [],
	"description": "",
	"content": "Model Customization, RAG, or Both: A Case Study with Amazon Nova by Flora Wang, Anila Joshi, Baishali Chaudhury, Sungmin Hong, Jae Oh Woo, and Rahul Ghosh on 10 APR 2025 in Advanced (300), Amazon Bedrock, Amazon Machine Learning, Amazon Nova, Amazon SageMaker, Generative AI, Technical How-to\nAs enterprises and developers increasingly seek to optimize their language models for specific tasks, the decision between model customization and Retrieval Augmented Generation (RAG) becomes critical. In this post, we aim to address this growing need by providing clear, actionable guidance and best practices on when to use each approach, helping you make informed decisions tailored to your unique requirements and goals.\nThe introduction of Amazon Nova models represents a significant advancement in the AI landscape, offering new opportunities for Large Language Model (LLM) optimization. In this post, we demonstrate how to effectively implement model customization and RAG using Amazon Nova models as a base. We conducted a comprehensive comparative study between model customization and RAG using the latest Amazon Nova models and share these valuable insights.\nApproach and Foundation Model Overview In this section, we discuss the differences between fine-tuning and RAG, present common use cases for each approach, and provide an overview of the foundation model used for our experiments.\nDemystifying RAG and Model Customization RAG is a technique to enhance the capabilities of pre-trained models by allowing them to access external, domain-specific data sources. It combines two components: external knowledge retrieval and response generation. This allows pre-trained language models to dynamically incorporate external data during the response generation process, enabling more contextually accurate and updated outputs. Unlike fine-tuning, in RAG, the model does not undergo any training, and model weights are not updated to learn domain knowledge. While fine-tuning implicitly uses domain-specific information by embedding necessary knowledge directly into the model, RAG explicitly utilizes domain-specific information through external retrieval.\nModel customization refers to adjusting a pre-trained language model to better suit specific tasks, domains, or datasets. Fine-tuning is one such technique that helps introduce task-specific or domain-specific knowledge to improve model performance. It adjusts model parameters to better align with the nuances of the target task while leveraging its general knowledge.\nCommon Use Cases for Each Approach RAG is optimal for use cases requiring dynamic or frequently updated data (such as customer support FAQs and e-commerce catalogs), domain-specific insights (such as legal or medical Q\u0026amp;A), scalable solutions for broad applications (such as software-as-a-service (SaaS) platforms), multimodal data retrieval (such as document summarization), and strict adherence to secure or sensitive data (such as financial and regulatory systems).\nIn contrast, fine-tuning thrives in scenarios requiring precise customization (such as personalized chatbots or creative writing), high accuracy for narrow tasks (such as code generation or specialized summarization), ultra-low latency (such as real-time customer interactions), stability with static datasets (such as domain-specific glossaries), and cost-effective scaling for high-volume tasks (such as call center automation).\nWhile RAG excels in real-time grounding in external data and fine-tuning specializes in static, structured, and personalized workflows, the choice between them often depends on nuanced factors. This post provides a comprehensive comparison of RAG and fine-tuning, clarifying the strengths, limitations, and contexts where each approach delivers the best performance.\nIntroduction to Amazon Nova Models Amazon Nova is a new generation of foundation models (FMs) providing state-of-the-art intelligence and industry-leading price performance. Amazon Nova Pro and Amazon Nova Lite are multimodal models excelling in accuracy and speed, with Amazon Nova Lite optimized for fast processing and low cost. Amazon Nova Micro focuses on ultra-low latency text tasks. They provide fast inference, support agentic workflows with Amazon Bedrock Knowledge Bases and RAG, and enable fine-tuning on text and multimodal data. Optimized for cost-efficient performance, they are trained on data in over 200 languages.\nSolution Overview To evaluate the effectiveness of RAG versus model customization, we designed a comprehensive experimental framework using a set of AWS-specific questions. Our study utilized Amazon Nova Micro and Amazon Nova Lite as the base FMs and examined their performance across different configurations.\nWe structured our evaluation as follows:\nBase Model: Used Amazon Nova Micro and Amazon Nova Lite. Generated answers for AWS-specific questions without additional context. Base Model with RAG: Connected base models to Amazon Bedrock Knowledge Bases. Provided access to relevant AWS documentation and blogs. Model Customization: Fine-tuned both Amazon Nova models using 1,000 AWS-specific question-answer pairs generated from the same set of AWS articles. Deployed customized models via provisioned throughput. Generated answers for AWS-specific questions with the fine-tuned models. Hybrid Approach (Model Customization + RAG): Connected fine-tuned models to Amazon Bedrock Knowledge Bases. Provided fine-tuned models access to relevant AWS articles at inference time. In the following sections, we guide you through setting up the second and third approaches (base model with RAG and model customization with fine-tuning) in Amazon Bedrock.\nPrerequisites To follow this post, you need:\nAn AWS account and appropriate permissions. An Amazon Simple Storage Service (Amazon S3) bucket with two folders: one for your training data and one for your model output and training metrics. Implementing RAG with Base Amazon Nova Model In this section, we walk through the steps to implement RAG with a base model. To do so, we create a knowledge base. Complete the following steps:\nOn the Amazon Bedrock console, select Knowledge Bases in the navigation pane. In Knowledge Bases, select Create. On the Configure data source page, provide the following: Specify the Amazon S3 location of the documents. Specify the chunking strategy. Select Next. On the Select embedding model and configure vector store page, provide the following: In the Embedding model section, select the embedding model used to embed the chunks. In the Vector database section, create a new vector store or use an existing one where embeddings will be stored for retrieval. Select Next. On the Review and create page, review the settings and select Create knowledge base. Fine-tuning Amazon Nova Models via Amazon Bedrock API In this section, we provide detailed instructions on how to fine-tune and host custom Amazon Nova models using Amazon Bedrock. The following diagram illustrates the solution architecture.\nCreating a Fine-Tuning Job Fine-tuning Amazon Nova models through the Amazon Bedrock API is a streamlined process:\nOn the Amazon Bedrock console, select us-east-1 as your AWS Region. At the time of writing, Amazon Nova fine-tuning is only available in us-east-1. Select Custom Models under Foundation Models in the navigation pane. Under Customize models, select Create fine-tuning job. For Source model, select Select model. Select Amazon as the provider and choose your preferred Amazon Nova model. Select Apply. For Fine-tuned model name, enter a unique name for the fine-tuned model. For Job name, enter a name for the fine-tuning job. In Input data, enter the S3 bucket locations for source (training data) and destination (model output and training metrics) and optionally your validation dataset location. Configuring Hyperparameters For Amazon Nova models, you can customize the following hyperparameters:\nParameter Range/Constraints Epochs 1–5 Batch Size Fixed at 1 Learning Rate 0.000001–0.0001 Learning Rate Warmup Steps 0–100 Preparing Dataset for Compatibility with Amazon Nova Models Similar to other LLMs, Amazon Nova requires prompt-completion pairs, also known as Q\u0026amp;A pairs, for Supervised Fine-Tuning (SFT). This dataset must contain the ideal results you want the language model to generate for specific tasks or prompts. Refer to the Amazon Nova Data Preparation Guide for best practices and example formatting.\nChecking Fine-Tuning Job Status After creating the fine-tuning job, select Custom Models in the navigation pane. You will find the current fine-tuning job listed under Jobs. You can use this page to track the status.\nWhen the status changes to Completed, you can select the job name to navigate to the Training job overview page to find specifications, S3 locations, and hyperparameters used.\nHosting the Fine-Tuned Model with Provisioned Throughput Once the job is successful:\nGo to Custom Models \u0026gt; Models and select your model. Select Purchase Provisioned Throughput. Choose a commitment term (no commitment, 1 month, or 6 months). Evaluation Framework and Results Multi-LLM Judge to Reduce Bias We used a scoring system (0-10) with two judges: Claude 3.5 Sonnet and Llama 3.1 70B.\nResponse Quality Comparison Fine-tuning and RAG both improved response quality by ~30% for Nova Lite. The hybrid approach (Fine-tuning + RAG) improved quality by 83%.\nNotably, Nova Micro (smaller model) with the hybrid approach performed nearly as well as larger models.\nLatency and Token Usage Fine-tuning reduced base model latency by ~50%, while RAG reduced it by ~30%.\nFine-tuning reduced average total tokens by over 60%, whereas RAG doubled token usage due to context switching.\nConclusion We recommend combining Model Customization and RAG for Q\u0026amp;A tasks to maximize performance. Fine-tuning is superior for tone/style adjustment and latency-sensitive tasks, while RAG is essential for dynamic data.\nAbout the Authors\nẢnh đại diện Giới thiệu về các tác giả Mengdie (Flora) Wang is a Data Scientist in the AWS General AI Innovation Center, where she works with customers to architect and deploy scalable general AI solutions to solve their unique business challenges. She specializes in custom modeling techniques and agent-based AI systems, helping organizations unlock the full potential of general AI technology. Prior to joining AWS, Flora earned her Master\u0026rsquo;s in Computer Science from the University of Minnesota, where she developed expertise in machine learning and artificial intelligence. Sungmin Hong is a Senior Applied Scientist at Amazon\u0026rsquo;s General AI Innovation Center, where he helps accelerate the diversity of use cases for AWS customers. Prior to joining Amazon, Sungmin was a postdoctoral fellow at Harvard Medical School. He holds a PhD in Computer Science from New York University. Outside of work, he prides himself on keeping his houseplants alive for 3+ years. Jae Oh Woo is a Senior Applied Scientist at the AWS General AI Innovation Center, where he specializes in developing custom solutions and custom models for a variety of use cases. He has a strong passion for interdisciplinary research that connects theoretical foundations with practical applications in the rapidly evolving field of general AI. Prior to joining Amazon, Jae Oh was a Simons Postdoctoral Fellow at the University of Texas at Austin, where he conducted research across the departments of Mathematics and Electrical and Computer Engineering. He holds a PhD in Applied Mathematics from Yale University. Rahul Ghoshis an Applied Scientist at Amazon\u0026rsquo;s General AI Innovation Center, where he works with AWS customers across various verticals to accelerate the use of general AI. Rahul holds a PhD in Computer Science from the University of Minnesota. Baishali Chaudhury is an Applied Scientist in the General AI Innovation Center at AWS, where she focuses on advancing general AI solutions for real-world applications. She has a strong background in computer vision, machine learning, and AI for healthcare. Baishali holds a PhD in Computer Science from the University of South Florida and a PostDoc from Moffitt Cancer Center. Anila Joshi has over a decade of experience building AI solutions. As the AWSI Geographic Leader at the AWS General AI Innovation Center, Anila pioneers innovative applications of AI that push the boundaries of what is possible and accelerate customer adoption of AWS services by helping customers conceptualize, define, and deploy secure general AI solutions. "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/3-blogstranslated/3.2-blog2/",
	"title": "Blog 2",
	"tags": [],
	"description": "",
	"content": "Migrating CDK Version 1 Applications to CDK Version 2 with Amazon Q Developer by Dr. Rahul Sharad Gaikwad, Tamilselvan P, and Vinodkumar Mandalapu on April 30, 2025 on Amazon Q.\nIntroduction: AWS Cloud Development Kit (AWS CDK) is an open-source software development framework to define cloud infrastructure in code and provision it through AWS CloudFormation. As of June 1, 2023, AWS CDK version 1 is no longer supported. To avoid potential issues using an outdated version and to take advantage of the latest features and improvements, we recommend upgrading to AWS CDK version 2.\nAmazon Q Developer, a generative AI-powered assistant for software development, enhances the efficiency of software development teams. It facilitates the creation of deployment-ready Infrastructure as Code (IaC) for AWS CloudFormation, AWS CDK, and Terraform. By using Amazon Q, developers can accelerate IaC development, enhance code quality, and reduce the likelihood of configuration errors.\nThis post demonstrates how Amazon Q Developer supports upgrading an existing AWS CDK v1 application to AWS CDK v2.\nPrerequisites AWS Builder ID or AWS IAM Identity Center credentials controlled by your organization\nSupported IDE, such as Visual Studio Code\nAWS Toolkit IDE extension\nAuthentication and connection\nNode.js\nAWS CDK version 1\nAWS CDK version 2\nPlan In this blog post, I will explore a code example where I created a VPC, Subnet, and ECS Fargate cluster using AWS CDK version 1. Then, I will explain how you can use Amazon Q to convert the code from CDK v1 to CDK v2.\n1. To start this process, I began by asking Amazon Q Developer to provide the necessary steps to migrate from CDK version 1 to version 2, as outlined below.\nCan you provide the steps to migrate from cdk version 1 to version 2?\n2. In the screenshot above, Amazon Q Developer outlined several steps we can take to make the necessary changes. The first step is to update dependencies. If I need guidance on how to update dependencies, I can ask Amazon Q Developer for help again by requesting steps related to updating dependencies as shown below.\nCan you provide the steps to update dependencies?\n3. After updating the dependencies, the next step is to update the import statements. For guidance on how to update import statements, I can ask the Amazon Q Developer assistant for help again by asking for steps related to how to import statements as shown below.\n@workspace Can you provide the steps to update import statements?\nIn the screenshot above, if you notice, I prefixed the prompt with @workspace, which automatically includes the most relevant parts of my workspace code as context.\n4. If any errors occur while updating the code according to Amazon Q Developer\u0026rsquo;s recommendations, I can use Amazon Q Developer to debug the issue and provide the necessary input to resolve it.\n5. After completing the required steps, I can deploy the application using AWS CDK version 2 by running the cdk deploy command.\n6. In addition to other capabilities, Amazon Q also offers code review functionality. To start a code review, simply select Amazon Q and use the /review command. Then, I will have the option to review active files or the entire open workspace. Select your preference and Amazon Q will analyze your project and provide comprehensive review results.\n7. Amazon Q Developer can also generate documentation, including README files. To generate documentation, select Amazon Q and enter the /doc command. Amazon Q will automatically generate a README file for your project. I can then review the generated documentation, accept the changes, or provide specific instructions for further modifications.\nConclusion In this blog, I demonstrated how Amazon Q Developer can simplify and accelerate the process of upgrading from AWS CDK version 1 to version 2, ensuring your cloud infrastructure remains secure, efficient, and aligned with the latest AWS innovations. AWS CDK version 2 offers a unified, streamlined library with improved performance and ongoing support, making infrastructure management easier and more reliable.\nBy leveraging Amazon Q Developer, a generative AI-powered assistant, teams can automate Infrastructure as Code development, enhance code quality, and minimize configuration errors. Together, these tools empower development teams to confidently modernize and scale their AWS environments, turning the upgrade process into a seamless opportunity for innovation and growth.\nResources To learn more about Amazon Q Developer, check out the following resources:\nAmazon Q Developer Workshop\nAmazon Q Developer User Guide\nTo learn more about AWS CDK, check out the following resources:\nAWS CDK Workshop\nHow to use Amazon Q Developer to deploy a serverless web application with AWS CDK\nAbout the authors:\nProfile Photo About the authors Dr. Rahul Sharad Gaikwad is a Solutions Architect at AWS, driving cloud innovation through customer workload migration and modernization. As a Generative AI and DevOps enthusiast, he architects cutting-edge solutions and is recognized as an APJC HashiCorp Ambassador. He holds a PhD in AIOps and has received the Man of Excellence Award, Indian Achiever Award, Best PhD Thesis Award, Research Scholar of the Year Award, and Young Researcher Award. Vinodkumar Mandalapu is a DevOps Consultant at AWS, specializing in designing and deploying cloud-based infrastructure and deployment pipelines on AWS. With extensive experience in automating and streamlining software delivery, he has helped organizations of all sizes leverage the power of the cloud to drive innovation, improve scalability, and enhance operational efficiency. In his free time, he enjoys traveling and spending quality time with his son. Tamilselvan P is a DevOps Consultant at AWS, focused on architecting and implementing cloud-native systems as well as continuous delivery within the ecosystem. Leveraging his comprehensive expertise in orchestrating and refining software release processes, he has assisted customers across various industries and scales in harnessing cloud technology to innovate faster, increase scalability, and enhance operational performance. In his free time, he enjoys playing cricket. "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/3-blogstranslated/3.3-blog3/",
	"title": "Blog 3",
	"tags": [],
	"description": "",
	"content": "Melting The Ice - How Natural Intelligence Simplified a Data Lake Migration to Apache Iceberg by Yonatan Dolan and Haya Axelrod Stern, Zion Rubin, Michal Urbanowicz on April 28, 2025 in Advanced (300), Analytics, AWS Glue, Best Practices, Case Study, Technical How-to Permalink\nThis article was co-authored by Haya Axelrod Stern, Zion Rubin, and Michal Urbanowicz from Natural Intelligence.\nMany organizations choose data lakes for their flexibility and scalability in managing structured and unstructured data. However, migrating an existing data lake to a new table format like Apache Iceberg can present numerous technical and organizational challenges.\nNatural Intelligence (NI) is a leader in the multi-category marketplace sector. With prominent brands like Top10.com and BestMoney.com, NI helps millions of people make smart decisions every day. Recently, NI embarked on a journey to transform its traditional data lake from Apache Hive to Apache Iceberg.\nIn this article, NI shares their journey, the innovative solutions they developed, and key lessons that can guide other organizations looking to follow a similar path. The content focuses heavily on practical challenges and how they were resolved during the transition, rather than the complex technical specifications of Apache Iceberg itself.\nWhy Apache Iceberg? The data architecture at NI follows the Medallion Architecture (Bronze – Silver – Gold layers), described as follows:\nBronze layer: Raw, unprocessed data collected from various sources, stored in its native format in Amazon Simple Storage Service (Amazon S3) and ingested via Apache Kafka brokers.\nSilver layer: Contains cleaned and enriched data, processed using Apache Flink.\nGold layer: Stores analytics-ready datasets designed for Business Intelligence (BI) and reporting. Data in this layer is generated through Apache Spark pipelines and consumed by services such as Snowflake, Amazon Athena, Tableau, and Apache Druid. Data is stored in Apache Parquet format, with AWS Glue Catalog responsible for metadata management.\nWhile this architecture met NI\u0026rsquo;s data analytics needs, it lacked the flexibility required for a truly open and adaptable data platform. The Gold layer could only work with query engines that supported Hive and the AWS Glue Data Catalog. Although Amazon Athena could be used, for Snowflake, NI had to maintain a separate catalog to query external tables. This issue made evaluating or adopting alternative tools and engines difficult—unless they wanted to duplicate data, rewrite queries, or perform costly catalog synchronizations. As the business scaled, NI needed a data platform that could support multiple different query engines simultaneously with a single data catalog, while avoiding vendor lock-in.\nThe Power of Apache Iceberg Apache Iceberg emerged as the perfect solution—an open, flexible table format suited to NI\u0026rsquo;s Data Lake First approach. Iceberg offers several key advantages such as ACID transactions, schema evolution, time travel, improved performance, and more. But the primary strategic benefit lies in its ability to support multiple query engines simultaneously. It also has the following advantages:\nDecoupling of storage and compute: The open table format allows you to separate the storage layer from the query engine, enabling easy swapping and simultaneous support for multiple tools without data duplication.\nVendor independence: As an open table format, Apache Iceberg prevents vendor lock-in, giving you the flexibility to adapt to changing analytics needs.\nVendor adoption: Apache Iceberg is widely supported by major platforms and tools, providing seamless integration and long-term compatibility with the ecosystem.\nBy transitioning to Iceberg, NI was able to embrace a truly open data platform, providing long-term flexibility, scalability, and interoperability while maintaining a unified single source of truth for all analytics and reporting needs.\nChallenges Faced Migrating a live production data lake to Iceberg is challenging due to operational complexity and legacy constraints. The data service at NI runs hundreds of Spark and machine learning processes, manages thousands of tables, and supports over 400 dashboards—all operating 24/7. Any migration process needed to be executed without production interruption; coordinating such a migration while operations continued seamlessly was difficult.\nNI needed to accommodate diverse users with varying requirements and timelines, ranging from data engineers to data analysts, data scientists, and BI teams.\nAdding to the challenge were legacy constraints. Some existing tools did not fully support Iceberg, so it was necessary to maintain Hive-backed tables for compatibility. NI realized that not all consumers could adopt Iceberg immediately. A plan was needed to allow for incremental transition without downtime or disruption to ongoing operations.\nKey Components for Migration To ensure a smooth and successful transition, six critical components were identified:\nSupport ongoing operations: Ensure uninterrupted compatibility with existing systems and processes throughout the migration.\nUser transparency: Minimize disruption for users by keeping table names and access methods the same.\nGradual consumer migration: Allow users to switch to Iceberg at their own pace, avoiding the need for a simultaneous mass migration.\nETL flexibility: Allow shifting ETL pipelines to Iceberg without imposing constraints on development or deployment.\nCost effectiveness: Minimize storage and processing data duplication, as well as incremental costs during the transition phase.\nMinimize maintenance: Reduce the operational burden of maintaining two table formats (Hive and Iceberg) in parallel during the transition.\nEvaluating Traditional Migration Methods Apache Iceberg supports two main migration methods: In-place migration and Rewrite-based migration.\nIn-place migration\nHow it works: This method converts existing datasets to Iceberg tables without duplicating data, by creating Iceberg metadata based on existing data files, while keeping the original layout and format.\nPros:\nSaves storage costs, as there is no data duplication. Easy to implement, simple execution process. Keeps current table names and locations, so users don\u0026rsquo;t have to change access. No data movement required, minimal compute requirements → lower cost. Cons:\nRequires downtime: All write operations must pause during conversion, which was unacceptable for NI, as data and analytics processes are critical and operate 24/7. Cannot migrate gradually: All users must switch to Iceberg at the same time, increasing the risk of system disruption. Limited validation: No opportunity to check data correctness before completing the conversion; if there is an error, recovery from backups is needed. Technical limitations: Schema evolution during migration can be difficult; data type conflicts can cause the entire process to fail. Rewrite-based migration How it works: This method creates a new Iceberg table by rewriting and reorganizing existing data files into an optimal Iceberg format and structure, improving performance and data management.\nPros:\nNo downtime required during the migration process. Supports gradual consumer migration, allowing teams to adopt Iceberg at their own pace. Allows for thorough data validation before full switchover. Simple rollback mechanism, can easily revert if errors occur. Cons:\nIncreased resource costs: Requires double the storage capacity and processing power during the migration time. Maintenance complexity: Requires maintaining two parallel data pipelines, increasing operational burden. Consistency challenges: Hard to ensure two systems remain fully synchronized during the transition. Performance impact: Dual writes can increase latency and slow down pipelines. Why neither option was good enough NI found that neither method (In-place nor Rewrite-based) could fully meet the critical requirements:\nIn-place migration was unsuitable because the downtime requirement was unacceptable, and it did not support a gradual transition process. Rewrite-based migration was costly and complex in terms of operational management due to maintaining two parallel pipelines. Based on this analysis, NI developed a Hybrid approach — combining the advantages of both methods while minimizing and overcoming their limitations.\nThe Hybrid Solution The hybrid migration strategy was designed based on 5 core components, leveraging AWS analytics services to orchestrate, process, and manage state.\n1. Hive-to-Iceberg CDC (Change Data Capture): A system that automatically syncs Hive tables to Iceberg using a custom CDC process to support existing users. Unlike traditional row-level CDC, NI applied partition-level CDC—since Hive typically updates data by overwriting entire partitions. This approach helps maintain data consistency between Hive and Iceberg without changing data write logic, ensuring that both tables contain the same data during the migration phase.\n2. Continuous schema synchronization: During migration, schema evolution causes many maintenance challenges. NI implemented an automated schema synchronization process that compares schemas between Hive and Iceberg and adjusts differences while maintaining data type compatibility.\n3. Iceberg-to-Hive reverse CDC: Allows data teams to switch ETL (Extract, Transform, Load) jobs to write directly to Iceberg, while still maintaining compatibility with legacy processes using Hive. Reverse CDC automatically updates data from Iceberg back to Hive, ensuring that downstream pipelines that haven\u0026rsquo;t migrated yet continue to function normally. Thanks to this, the system can transition gradually without disrupting existing processes.\n4. Alias management in Snowflake: Using aliases in Snowflake ensures that Iceberg tables keep their original names, making the transition transparent to users. This approach minimizes reconfiguration on dependent teams and existing workflows.\n5. Table replacement: Once the entire system has switched to Iceberg, NI swaps the production tables, retaining the original table names, and completes the migration process.\nTechnical Deep Dive The migration process from Hive to Iceberg was built from several steps:\n1. Hive-to-Iceberg CDC Diagram: Goal: Keep Hive and Iceberg tables synchronized without duplicate effort.\nThe preceding figure shows how every partition written to the Hive table is automatically and transparently replicated to the Iceberg table using a CDC process. This process ensures that both tables are synchronized, allowing for a seamless and incremental migration without disrupting downstream systems. NI chose partition-level synchronization because legacy Hive ETL jobs wrote updates by overwriting entire partitions and updating partition locations. Adopting a similar approach in the CDC process helped ensure that it remained consistent with how data was originally managed, making the migration smoother and avoiding the need to rework row-level logic.\nImplementation:\nTo keep Hive and Iceberg tables synchronized without duplicate effort, a streamlined process was implemented. Whenever partitions in a Hive table are updated, the AWS Glue Catalog emits events like UpdatePartition. Amazon EventBridge captures these events, filters them for relevant databases and tables according to EventBridge rules, and triggers an AWS Lambda function. This function parses the event metadata and sends partition updates to an Apache Kafka topic.\nA Spark task running on Amazon EMR consumes messages from Kafka, which contain the updated partition details from the Data Catalog events. Using that event metadata, the Spark task queries the relevant Hive table and writes to the Iceberg table in Amazon S3 using the Spark Iceberg API overwritePartitions, as shown in the following example:\n{ \u0026#34;id\u0026#34;: \u0026#34;10397e54-c049-fc7b-76c8-59e148c7cbfc\u0026#34;, \u0026#34;detail-type\u0026#34;: \u0026#34;Glue Data Catalog Table State Change\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;aws.glue\u0026#34;, \u0026#34;time\u0026#34;: \u0026#34;2024-10-27T17:16:21Z\u0026#34;, \u0026#34;region\u0026#34;: \u0026#34;us-east-1\u0026#34;, \u0026#34;detail\u0026#34;: { \u0026#34;databaseName\u0026#34;: \u0026#34;dlk_visitor_funnel_dwh_production\u0026#34;, \u0026#34;changedPartitions\u0026#34;: [ \u0026#34;2024-10-27\u0026#34; ], \u0026#34;typeOfChange\u0026#34;: \u0026#34;UpdatePartition\u0026#34;, \u0026#34;tableName\u0026#34;: \u0026#34;fact_events\u0026#34; } } By targeting only modified partitions, the process (shown in the following figure) significantly reduced the need for expensive full-table rewrites. Iceberg\u0026rsquo;s robust metadata layers, including snapshots and manifest files, were seamlessly updated to capture these changes, providing efficient and accurate synchronization between the Hive and Iceberg tables. 2. Iceberg-to-Hive Reverse CDC Diagram Goal: Support Hive consumers while allowing ETL processes to switch to Iceberg.\nThe preceding figure shows the reverse process, where every partition written to the Iceberg table is automatically and transparently replicated to the Hive table using a CDC mechanism. This process helps ensure synchronization between the two systems, allowing seamless data updates for legacy systems that still rely on Hive while transitioning to Iceberg.\nImplementation:\nSynchronizing data from Iceberg tables back to Hive tables presented a different challenge. Unlike Hive tables, the Data Catalog does not track partition updates for Iceberg tables because partitions in Iceberg are managed internally rather than in the catalog. This meant NI could not rely on Glue Catalog events to detect partition changes.\nTo address this, NI implemented a solution similar to the previous process but adapted to Iceberg\u0026rsquo;s architecture. Apache Spark is used to query Iceberg\u0026rsquo;s metadata tables - specifically the snapshots and entries tables - to identify modified partitions since the last synchronization. The query used is:\nSELECT e.data_file.partition, MAX(s.committed_at) AS last_modified_time FROM $target_table.snapshots JOIN $target_table.entries e ON s.snapshot_id = e.snapshot_id WHERE s.committed_at \u0026gt; \u0026#39;$last_sync_time\u0026#39; GROUP BY e.data_file.partition; This query returns only the partitions that have been updated since the last sync, allowing it to focus entirely on changed data. Using this information, similar to the previous process, a Spark job retrieves the updated partitions from Iceberg and writes them back to the corresponding Hive table, providing seamless synchronization between both tables.\n3. Continuous Schema Synchronization Goal: Automatically update schemas to maintain consistency across Hive and Iceberg.\nThe preceding figure shows the automated schema synchronization process that helps ensure consistency between Hive and Iceberg table schemas by automatically syncing schema changes. In this example, adding a Channel column minimizes manual work and double maintenance during the extended migration period.\nImplementation:\nTo handle schema changes between Hive and Iceberg, a process was implemented to detect and reconcile differences automatically. When a schema change occurs in a Hive table, the Data Catalog emits an UpdateTable event. This event triggers a Lambda function (routed via EventBridge), which retrieves the updated schema from the Data Catalog for the Hive table and compares it with the Iceberg schema. It is important to note that in NI\u0026rsquo;s setup, schema changes originate from Hive because the Iceberg table is hidden behind aliases system-wide. Because Iceberg is primarily used for Snowflake, a one-way sync from Hive to Iceberg was sufficient. Therefore, there was no mechanism to detect or handle schema changes made directly in Iceberg, as they were not needed in the current workflow.\nDuring the schema reconciliation process (shown in the following figure), data types are normalized to help ensure compatibility — for example, converting Hive VARCHAR to Iceberg STRING. Any new fields or type changes are validated and applied to the Iceberg schema using a Spark task running on Amazon EMR. Amazon DynamoDB stores schema synchronization checkpoints, allowing tracking of changes over time and maintaining consistency between Hive and Iceberg schemas.\nBy automating this schema synchronization, maintenance overhead was significantly reduced, freeing developers from manually syncing schemas, making the long migration period significantly more manageable.\nThe preceding figure depicts an automated workflow for maintaining schema consistency between Hive and Iceberg tables. AWS Glue records table state change events from Hive, triggering an EventBridge event. This event invokes a Lambda function that fetches metadata from DynamoDB and compares schemas fetched from AWS Glue for both Hive and Iceberg tables. If a mismatch is detected, the schema in Iceberg is updated to help ensure alignment, minimizing manual intervention and supporting smooth operations during migration.\n4. Alias Management in Snowflake Goal: Allow Snowflake consumers to adopt Iceberg without changing query references.\nThe preceding figure shows Snowflake aliases enabling seamless migration by mapping queries like SELECT platform, COUNT(clickouts) FROM funnel.clickouts to Iceberg tables in the Glue Catalog. Even with suffix additions during the Iceberg migration, existing queries and workflows remain unchanged, minimizing disruption for BI tools and analysts.\nImplementation:\nTo help ensure a seamless experience for BI tools and analysts during migration, Snowflake aliases were used to map external tables to Iceberg metadata stored in the Data Catalog. By assigning aliases that matched the original Hive table names, existing queries and reports were preserved without disruption. For example, an external table was created in Snowflake and aliased to the original table name, as shown in the following query:\nCREATE OR REPLACE ICEBERG TABLE dlk_visitor_funnel_dwh_production.aggregated_cost EXTERNAL_VOLUME = \u0026#39;s3_dlk_visitor_funnel_dwh_production_iceberg_migration\u0026#39; CATALOG = \u0026#39;glue_dlk_visitor_funnel_dwh_production_iceberg_migration\u0026#39; CATALOG_TABLE_NAME = \u0026#39;aggregated_cost\u0026#39;; ALTER ICEBERG TABLE dlk_visitor_funnel_dwh_production.aggregated_cost REFRESH; Once the migration was complete, a simple change back to the alias was made to point to the new location or schema, making the transition seamless and minimizing any disruption to user workflows.\n5. Table Replacement Goal: Once all ETLs and relevant data processes were successfully converted to use Apache Iceberg capabilities and everything was working correctly with the sync flow, it was time to move to the final stage of the migration. The main objective was to maintain the original table names, avoiding the use of any prefixes like those used in the previous intermediate migration steps. This helps ensure that the configuration remains clean and free of unnecessary naming complexities.\nThe preceding figure shows the table replacement to complete the migration, where Hive on Amazon EMR is used to register Parquet files as Iceberg tables while keeping the original table names and avoiding data duplication, helping ensure a seamless and clean migration.\nImplementation:\nOne of the challenges was the inability to rename tables in AWS Glue, which prevented the use of a simple renaming method for existing sync flow tables. Additionally, AWS Glue does not support the Migrate procedure to create Iceberg metadata over existing data files while preserving the original table name. The strategy to overcome this limitation was to use the Hive metastore on an Amazon EMR cluster. By using Hive on Amazon EMR, NI could create final tables with their original names because it operates in a separate metastore environment, providing the flexibility to define any required schema and table name without interference.\nThe add_files procedure is used to systematically register all existing Parquet files, thereby building all necessary metadata in Hive. This is a critical step, as it helps ensure that all data files are cataloged and appropriately linked within the metastore.\nThe preceding figure shows the conversion of the production table to Iceberg using the add_files procedure to register existing Parquet files and generate Iceberg metadata. This helps ensure a smooth migration while preserving original data and avoiding duplication.\nThis setup allowed the use of existing Parquet files without duplicating data, thereby saving resources. Although the sync flow used separate storage buckets for the final architecture, NI chose to maintain the original buckets and clean up intermediate files. This resulted in a different folder structure on Amazon S3. Historical data has subdirectories for each partition in the original table directory, while new Iceberg data organizes subdirectories under a data folder. This difference was acceptable to avoid data duplication and preserve the original Amazon S3 buckets.\nTechnical Summary The AWS Glue Data Catalog serves as the central source of truth for schema and table updates, with Amazon EventBridge capturing Data Catalog events to trigger synchronization processes. AWS Lambda parses event metadata and manages schema synchronization, while Apache Kafka buffers events for real-time processing. Apache Spark on Amazon EMR handles data transformation and incremental updates, while Amazon DynamoDB maintains state, including synchronization checkpoints and table mappings. Finally, Snowflake seamlessly consumes Iceberg tables via aliases without disrupting existing workflows.\nMigration Results The migration process was completed with zero downtime; continuous operations were maintained throughout the process, supporting hundreds of pipelines and dashboards without interruption. The migration was executed with a cost-optimization mindset, with incremental updates and partition-level synchronization minimizing compute and storage resource usage. Ultimately, NI established a modern, vendor-neutral platform that allows for scaling evolving analytics and machine learning needs. It enables seamless integration with multiple compute and query engines, supporting further flexibility and innovation.\nConclusion Transforming Natural Intelligence to Apache Iceberg was a critical step in modernizing the company\u0026rsquo;s data infrastructure. By adopting a hybrid strategy and utilizing the power of event-driven architecture, NI helped ensure a seamless transition that balanced innovation with operational stability. The journey underscores the importance of careful planning, understanding the data ecosystem, and focusing on an organization-first approach.\nAbove all, business operations were centered, and continuity prioritized user experience. By doing so, NI unlocked the flexibility and scalability of their data lake while minimizing disruption, enabling teams to utilize advanced analytics capabilities, positioning the company at the forefront of modern data management and ready for the future.\nIf you are considering an Apache Iceberg migration or facing similar data infrastructure challenges, we encourage you to explore the possibilities. Embrace open formats, utilize automation, and design with your organization\u0026rsquo;s unique needs in mind. The journey can be complex, but the rewards in terms of scalability, flexibility, and innovation are well worth the effort. You can use AWS Prescriptive Guidance to help learn more about how to best use Apache Iceberg for your organization.\nAbout the authors\nProfile Photo About the authors Yonatan Dolan is a Principal Analytics Specialist at Amazon Web Services. Yonatan is an Apache Iceberg evangelist. Haya Stern is a Senior Director of Data at Natural Intelligence. She leads the development of NI\u0026rsquo;s large-scale data platform, focusing on enabling analytics, streamlining data workflows, and improving development efficiency. Over the past year, she led the successful migration from the previous data architecture to a modern data lakehouse based on Apache Iceberg and Snowflake. Zion Rubin is a Data Architect at Natural Intelligence with ten years of experience architecting large-scale big data platforms, currently focused on developing intelligent agent systems that turn complex data into real-time business insights. Michał Urbanowicz is a Cloud Data Engineer at Natural Intelligence with expertise in data warehouse migration and implementing robust retention, cleaning, and monitoring processes to ensure scalability and reliability. He also develops automation features that streamline and support campaign management operations in cloud-based environments. "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/5-workshop/5.3-s3-vpc/5.3.1-create-gwe/",
	"title": "Create a gateway endpoint",
	"tags": [],
	"description": "",
	"content": " Open the Amazon VPC console In the navigation pane, choose Endpoints, then click Create Endpoint: You will see 6 existing VPC endpoints that support AWS Systems Manager (SSM). These endpoints were deployed automatically by the CloudFormation Templates for this workshop.\nIn the Create endpoint console: Specify name of the endpoint: s3-gwe In service category, choose AWS services In Services, type s3 in the search box and choose the service with type gateway For VPC, select VPC Cloud from the drop-down. For Configure route tables, select the route table that is already associated with two subnets (note: this is not the main route table for the VPC, but a second route table created by CloudFormation). For Policy, leave the default option, Full Access, to allow full access to the service. You will deploy a VPC endpoint policy in a later lab module to demonstrate restricting access to S3 buckets based on policies. Do not add a tag to the VPC endpoint at this time. Click Create endpoint, then click x after receiving a successful creation message. "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/4-eventparticipated/4.1-event1/",
	"title": "Event 1",
	"tags": [],
	"description": "",
	"content": "Vietnam Cloud Day 2025: Ho Chi Minh City Connect Edition for Builders Time: Thursday, September 18, 2025, 9:00 – 17:30 Location: Amazon Web Services Vietnam, 36th Floor, 2 Hai Trieu, Ben Nghe Ward, District 1, Ho Chi Minh City Role: Attendee\nEvent Objectives Update on top strategic technology trends: Agentic AI. Learn about Data Foundation solutions to address the \u0026ldquo;Data Silos\u0026rdquo; problem that 52% of CDOs are facing. Approach the new software development process: AI-Driven Development Lifecycle (AI-DLC). Grasp security standards for GenAI (MITRE ATLAS, OWASP, NIST) and risk layers. Speakers Eric Yeo - Country General Manager, AWS Vietnam Dr. Jens Lottner - CEO, Techcombank Ms. Trang Phung - CEO \u0026amp; Co-Founder, U2U Network Jaime Valles - VP, GM Asia Pacific and Japan, AWS Jeff Johnson - Managing Director, ASEAN, AWS Vu Van - Co-founder \u0026amp; CEO, ELSA Corp Nguyen Hoa Binh - Chairman, Nexttech Group Dieter Botha - CEO, TymeX Jun Kai Loke - AI/ML Specialist SA, AWS Kien Nguyen - Solutions Architect, AWS Tamelly Lim - Storage Specialist SA, AWS Binh Tran - Senior Solutions Architect, AWS Taiki Dang - Solutions Architect, AWS Michael Armentano - Principal WW GTM Specialist, AWS Hung Nguyen Gia - Head of Solutions Architect, AWS Son Do - Technical Account Manager, AWS Nguyen Van Hai - Director of Software Engineering, Techcombank Phuc Nguyen - Solutions Architect, AWS Alex Tran - AI Director, OCB Nguyen Minh Ngan - AI Specialist, OCB Nguyen Manh Tuyen - Head of Data Application, LPBank Securities Vinh Nguyen - Co-Founder \u0026amp; CTO, Ninety Eight Hung Hoang - Customer Solutions Manager, AWS Christal Poon - Specialist Solutions Architect, AWS Key Highlights 1. Agentic AI \u0026amp; Data Strategy Trends Current State: 88% of CDOs are moving forward with GenAI, yet 52% state their data foundation is not ready. Challenges: Businesses are held back by 3 types of \u0026ldquo;Silos\u0026rdquo;: Data Silos, People Silos, and Business Silos. Data Strategy: End-to-end model from Producers → Foundations → Consumers. Infrastructure: Amazon S3 (Data Lakes), Amazon Redshift (Data Warehouses), supporting open standard Apache Iceberg. Governance: Amazon DataZone assists with Data \u0026amp; AI Governance. New Tools: Introduction of Unified Studio integrating analytics and AI tools. 2. AI-Driven Development Lifecycle (AI-DLC) Speaker Binh Tran introduced the shift from AI-Assisted to AI-Driven Development. The AI-DLC process consists of 3 main phases:\nInception: Build Context on existing code, clarify intent (User Stories), plan (Units of Work), and Domain Modeling. Construction: AI generates code \u0026amp; Tests, adds architectural components. Operation: Deploy with IaC \u0026amp; tests, incident management. 3. Security for GenAI Speaker Taiki Dang emphasized that security must run in parallel with Generative AI.\nRisk Layers: Top layer (Consumer): Risks regarding IP, legal, hallucinations, safety. Middle layer (Tuner): Risks from Managed services, data retention policies. Bottom layer (Provider): Risks from training data. Frameworks \u0026amp; Standards: Apply MITRE ATLAS, OWASP Top 10 for LLM, NIST AI RMF, ISO 42001. Solutions: Use Amazon Bedrock Guardrails to prevent and mitigate risks (such as toxicity, PII leaks). 4. Analytics \u0026amp; Business Intelligence Speaker Christal Poon presented the transition from Amazon QuickSight to Amazon Q.\nFeatures for creating Dashboards, reports, and Data QA using natural language. Coming soon to Vietnam: Amazon Agentic AI Workbench (Quick Suite) with Quick Researcher and Quick Automate capabilities, keeping humans in the loop for control. Key Takeaways Agent Mindset: Clearly understand the structure of an AI Agent: Goals → Observation → Tools → Context → Action. Agent Core Architecture: Ensure all components are present: Runtime, Gateway, Memory, Observability, and Identity to deploy Agents to production safely and scalably. Multi-layer Security: Security extends beyond the application; it must control risks from training data and fine-tuning processes down to end-users (Consumer risks). Applying to Work Implement AI-DLC: Pilot the 7-step AI-DLC process in a new project, starting with using AI to \u0026ldquo;Build Context\u0026rdquo; and \u0026ldquo;Domain Modeling\u0026rdquo;. Enhance Security: Review current GenAI applications against the OWASP Top 10 for LLM checklist and integrate Bedrock Guardrails to filter harmful content. Modernize Data Stack: Evaluate the feasibility of migrating the current Data Warehouse to a Lakehouse architecture with Apache Iceberg on AWS to break down Data Silos. Personal Experience This event went much deeper technically (deep-dive) than I expected, providing significant practical value:\nI was very impressed by the sharing on AI-DLC by Mr. Binh Tran. It completely changed my perspective on coding: developers are no longer just writing code but becoming \u0026ldquo;architects\u0026rdquo; and \u0026ldquo;reviewers\u0026rdquo; for AI execution. The Scoping Matrix slide and the risk layers in GenAI gave me a more systematic view to justify new AI project deployments to the company\u0026rsquo;s Security department. I am very excited about the news that Amazon Agentic AI Workbench is coming to Vietnam, promising to solve the problem of automating market research processes (Quick Researcher) that the business team currently needs. Event Photos Summary: A \u0026ldquo;must-attend\u0026rdquo; event for Builders. Knowledge about Agentic AI and AI-DLC will be the compass for my technical development roadmap in the coming year.\n"
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/",
	"title": "Internship Report",
	"tags": [],
	"description": "",
	"content": "Internship Report Student Information: Full Name: Le Minh Tuan\nPhone Number: 0981 500 154\nEmail: tuanlmse184475@fpt.edu.vn\nUniversity: FPT University HCMC\nMajor: Artificial Intelligence\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 09/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/5-workshop/5.1-workshop-overview/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "VPC endpoints VPC endpoints are virtual devices. They are horizontally scaled, redundant, and highly available VPC components. They allow communication between your compute resources and AWS services without imposing availability risks. Compute resources running in VPC can access Amazon S3 using a Gateway endpoint. PrivateLink interface endpoints can be used by compute resources running in VPC or on-premises. Workshop overview In this workshop, you will use two VPCs.\n\u0026ldquo;VPC Cloud\u0026rdquo; is for cloud resources such as a Gateway endpoint and an EC2 instance to test with. \u0026ldquo;VPC On-Prem\u0026rdquo; simulates an on-premises environment such as a factory or corporate datacenter. An EC2 instance running strongSwan VPN software has been deployed in \u0026ldquo;VPC On-prem\u0026rdquo; and automatically configured to establish a Site-to-Site VPN tunnel with AWS Transit Gateway. This VPN simulates connectivity from an on-premises location to the AWS cloud. To minimize costs, only one VPN instance is provisioned to support this workshop. When planning VPN connectivity for your production workloads, AWS recommends using multiple VPN devices for high availability. "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/5-workshop/5.4-s3-onprem/5.4.1-prepare/",
	"title": "Prepare the environment",
	"tags": [],
	"description": "",
	"content": "To prepare for this part of the workshop you will need to:\nDeploying a CloudFormation stack Modifying a VPC route table. These components work together to simulate on-premises DNS forwarding and name resolution.\nDeploy the CloudFormation stack The CloudFormation template will create additional services to support an on-premises simulation:\nOne Route 53 Private Hosted Zone that hosts Alias records for the PrivateLink S3 endpoint One Route 53 Inbound Resolver endpoint that enables \u0026ldquo;VPC Cloud\u0026rdquo; to resolve inbound DNS resolution requests to the Private Hosted Zone One Route 53 Outbound Resolver endpoint that enables \u0026ldquo;VPC On-prem\u0026rdquo; to forward DNS requests for S3 to \u0026ldquo;VPC Cloud\u0026rdquo; Click the following link to open the AWS CloudFormation console. The required template will be pre-loaded into the menu. Accept all default and click Create stack. It may take a few minutes for stack deployment to complete. You can continue with the next step without waiting for the deployemnt to finish.\nUpdate on-premise private route table This workshop uses a strongSwan VPN running on an EC2 instance to simulate connectivty between an on-premises datacenter and the AWS cloud. Most of the required components are provisioned before your start. To finalize the VPN configuration, you will modify the \u0026ldquo;VPC On-prem\u0026rdquo; routing table to direct traffic destined for the cloud to the strongSwan VPN instance.\nOpen the Amazon EC2 console\nSelect the instance named infra-vpngw-test. From the Details tab, copy the Instance ID and paste this into your text editor\nNavigate to the VPC menu by using the Search box at the top of the browser window.\nClick on Route Tables, select the RT Private On-prem route table, select the Routes tab, and click Edit Routes.\nClick Add route. Destination: your Cloud VPC cidr range Target: ID of your infra-vpngw-test instance (you saved in your editor at step 1) Click Save changes "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/1-worklog/1.1-week1/",
	"title": "Week 1 Worklog",
	"tags": [],
	"description": "",
	"content": "📘 Week 1 Worklog – AWS Journey 1. Weekly Objectives The primary goals for Week 1 were to establish the foundational environment for the AWS journey and understand the core operational principles. Key objectives included:\nOnboarding: Familiarize with FCJ internship processes, communication channels, and regulations. Account Setup: Complete AWS Free Tier registration, configure AWS CLI, and enable security baselines (MFA, IAM). Core Services: Gain an overview of the AWS ecosystem (Compute, Storage, Networking, Database, Security). Hands-on Practice: proficiently use the AWS Management Console \u0026amp; AWS CLI v2. Infrastructure: Deploy and operate an EC2 t2.micro instance and perform basic EBS operations. Cost Control: Establish AWS Budgets to monitor spending. 2. Detailed Work Summary 🗂 Implementation Plan vs. Actual Category Plan Actual Status Onboarding \u0026amp; Rules Introduction, grasp communication channels Introduced, noted report standards ✅ Done AWS Overview Systematize service groups + Mindmap Completed, categorized notes taken ✅ Done Free Tier \u0026amp; Security Create account, enable MFA, create IAM user Enabled MFA; created user + Viewer group ✅ Done AWS CLI Install CLI, configure profile Profile acj-student set, sts test OK ✅ Done EC2/EBS/SSH Create EC2, SSH, attach EBS EC2 t2.micro + EBS 8GB gp3, SSH successful ✅ Done Cost Management Set Budgets $5/month Received test email alert ✅ Done 📅 Daily Activities Log Day Task Start Date Completion Date Reference Material Monday Onboarding: FCJ orientation, read regulations, learn report standards 08/09 08/09 AWS Journey Tuesday Research: Explore AWS ecosystem (Compute/Storage/Networking/DB/Security), create mindmap 09/09 09/09 AWS Journey Wednesday Account Setup: Create AWS Free Tier, enable MFA for root, create IAM user + Viewer group 10/09 10/09 AWS Journey Thursday CLI Setup: Install AWS CLI v2 (Windows), run aws configure (profile acj-student), check sts identity 11/09 11/09 AWS Journey Friday Theory: Study EC2 (instance types, AMI, EBS, SG, Elastic IP) + Free Tier checklist 12/09 12/09 AWS Journey Saturday Practice: Create EC2 t2.micro (AL2023), create/use key pair (.pem), SSH; attach EBS 8GB, format \u0026amp; mount 13/09 13/09 AWS Journey 3. Results \u0026amp; Evidence 3.1 Resources Created IAM: 01 Daily working user (Group: Viewer), MFA enabled for root account. EC2: t2.micro (Free Tier), AMI: Amazon Linux 2023. Security Group: Inbound rule opening 22/tcp restricted to My IP only. EBS: 8GB gp3 volume, formatted (xfs) and mounted to /data. Budgets: Monthly budget set to $5 USD with email alerts. CLI Region: Defaulted to ap-southeast-1 (Singapore). 3.2 CLI Commands Executed aws sts get-caller-identity --profile acj-student aws ec2 describe-regions --profile acj-student --output table aws ec2 describe-instances --profile acj-student --region ap-southeast-1 aws ec2 create-key-pair --key-name fcj-key --query \u0026#34;KeyMaterial\u0026#34; --output text \u0026gt; fcj-key.pem 3.3 Evidence Captured Screenshots saved: MFA enabled status, IAM user \u0026amp; group configuration, Budgets alert email, EC2 instance details, EBS volume attachment, and mount point verification. 4. Issues \u0026amp; Troubleshooting Issue Cause Resolution Result SSH Connection Failed Security Group did not authorize the correct IP Updated inbound rule -\u0026gt; My IP SSH OK CLI Missing Credentials Wrong profile or region selected Standardized profile acj-student, set default region ap-southeast-1 CLI OK Console Navigation Difficulty Unfamiliar User Interface Used Search bar + Pinned frequent services (EC2, IAM, S3, Budgets) Faster navigation 5. Key Takeaways Security First: Always adhere to the Least-privilege principle; use IAM users/roles instead of the Root account for daily tasks. Operations: Mastered the standard procedure for EC2 creation and EBS attach/format/mount lifecycle. Management: Understood the synchronization between Console ↔ CLI and the importance of Tagging (Project=FCJ, Owner=The Liems, Env=Dev) for resource tracking. Cost: Proactively using Budgets is essential to avoid billing surprises. 6. Cost \u0026amp; Security Summary Budgets: Limit set to $5 USD/month (Test email received). Security: Root MFA Enabled. .pem key stored securely. SSH access restricted to specific IP. Tagging Strategy: Project=FCJ, Owner=The Liems, Env=Dev. 7. Risks \u0026amp; Mitigation Strategies Risk: Exceeding Free Tier limits by forgetting to terminate resources. Mitigation: Set AWS Budgets + Perform weekly tag reviews to clean up resources. Risk: Leaking Key Pairs/Credentials. Mitigation: Store keys in a secure local directory, add to .gitignore, never commit to public repositories. "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/1-worklog/1.2-week2/",
	"title": "Week 2 Worklog",
	"tags": [],
	"description": "",
	"content": "📘 Week 2 Worklog – AWS Journey 1. Weekly Objectives During Week 2, the primary goal was to gain foundational hands-on experience with core AWS infrastructure services, including:\nAmazon S3 – Hosting a static website and managing bucket permissions. Amazon RDS (MySQL) – Provisioning a managed relational database and configuring connectivity. Amazon EC2 – Using an EC2 instance as a secure bastion host to access RDS. Amazon Route53 – Managing domains and mapping DNS records to AWS services. This week focuses on building fundamental cloud architecture components that serve as prerequisites for Week 3 tasks involving CloudFront, DynamoDB, and ElastiCache.\n2. Detailed Work Summary 🗂 Table of Activities Day Tasks Start Date Completion Date Reference Monday - Created an S3 bucket for static website content\n- Uploaded initial HTML/CSS demo files 15/09/2025 15/09/2025 AWS Journey Tuesday - Enabled Static Website Hosting on S3\n- Configured Bucket Policy to allow public read access\n- Tested website accessibility via S3 endpoint 16/09/2025 16/09/2025 AWS Journey Wednesday - Created an RDS MySQL instance (Free Tier)\n- Configured VPC Security Groups for inbound traffic\n- Recorded DB endpoint \u0026amp; credentials 17/09/2025 17/09/2025 AWS Journey Thursday - Launched an EC2 instance and installed MySQL client\n- Connected from EC2 → RDS using command line\n- Executed test queries and created sample tables 18/09/2025 18/09/2025 AWS Journey Friday - Explored Route53 functionality\n- Created a Hosted Zone and DNS records (A/CNAME)\n- Configured routing from custom domain → S3 static site\n- Validated website accessibility using domain name 19/09/2025 19/09/2025 AWS Journey 3. Technical Implementation Details 3.1 AWS S3 – Static Website Setup Created a new S3 bucket following naming conventions and regional placement. Uploaded static assets (HTML/CSS/Images). Enabled the Static Website Hosting feature. Configured index.html and error.html. Added a public-read Bucket Policy to serve content globally. Verified accessibility through the website endpoint: http://\u0026lt;bucket-name\u0026gt;.s3-website-\u0026lt;region\u0026gt;.amazonaws.com 3.2 Amazon RDS – Database Provisioning Launched RDS MySQL 8.0 instance under Free Tier. Applied secure Security Group rules (EC2 → RDS, port 3306). Stored the generated endpoint for later connection. Ensured DB subnet group and VPC configuration were valid for private access. 3.3 Amazon EC2 – Secure DB Connectivity Created a t2.micro EC2 instance in the same VPC as the RDS instance. Installed MySQL Client: sudo yum install mysql -y Successfully connected to RDS: mysql -h \u0026lt;rds-endpoint\u0026gt; -u admin -p Created a sample database and table for validation purposes. 3.4 Amazon Route53 – DNS Configuration Set up a new Hosted Zone. Added DNS records: A Record → S3 static website. CNAME Record for testing aliases. Waited for DNS propagation (typically 1–5 minutes). Successfully accessed the static site using a custom domain. 4. Achievements By the end of Week 2, the following outcomes were accomplished:\n✔ Functional Successes Completed a fully operational S3-hosted static website. Enabled access through both S3 website endpoint and Route53 domain. Successfully deployed and connected an RDS MySQL database. Verified secure communication between EC2 ↔ RDS. ✔ Skill Development Demonstrated understanding of: IAM roles \u0026amp; permissions. S3 access control. VPC networking \u0026amp; Security Groups. DNS routing concepts. Gained foundational experience with AWS core services. Strengthened understanding of end-to-end cloud application flow. Built confidence working with CLI-based operations (EC2 → RDS). Improved troubleshooting skills (DNS propagation, SG configuration, and public access policies). 5. Challenges Encountered \u0026amp; Resolutions Challenge 1: S3 Public Access Block\nIssue: Website not accessible due to default S3 public access block. Fix: Disabled “Block Public Access” settings and added a correct bucket policy. Challenge 2: EC2 -\u0026gt; RDS Connection Timeout\nIssue: Security Group did not allow inbound MySQL traffic. Fix: Modified RDS Security Group to accept traffic specifically from the EC2 Security Group on port 3306. Challenge 3: DNS Not Resolving Immediately\nIssue: Domain took time to propagate in Route53. Fix: Waited for TTL window and revalidated using dig / nslookup. "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/1-worklog/1.3-week3/",
	"title": "Week 3 Worklog",
	"tags": [],
	"description": "",
	"content": "📘 Week 3 Worklog – AWS Journey 1. Weekly Objectives During Week 3, the primary goal was to optimize application performance and expand data management skills beyond relational databases. Key objectives included:\nAmazon CloudFront – Understanding Content Delivery Networks (CDN) and optimizing static site delivery. Amazon DynamoDB – gaining hands-on experience with NoSQL database modeling and operations. Amazon ElastiCache (Redis) – Implementing in-memory caching to improve read performance. AWS CLI Integration – Advanced interaction with AWS services using command-line scripts. This week focuses on shifting from a basic architecture to a high-performance, scalable model using caching layers and managed NoSQL services.\n2. Detailed Work Summary 🗂 Table of Activities Day Tasks Start Date Completion Date Reference Monday - Introduction to CDN concepts and CloudFront benefits\n- Created CloudFront Distribution to serve S3 website content 22/09/2025 22/09/2025 AWS Journey Tuesday - Configured CloudFront behaviors and cache policies\n- Tested website access via CloudFront URL\n- Performed Cache Invalidation to update content 23/09/2025 23/09/2025 AWS Journey Wednesday - Introduction to DynamoDB (NoSQL Architecture)\n- Created DynamoDB tables (Users, Products)\n- Practiced CRUD operations via AWS Console 24/09/2025 24/09/2025 AWS Journey Thursday - Connected and queried DynamoDB using AWS CLI\n- Wrote scripts to put-item and scan data programmatically 25/09/2025 25/09/2025 AWS Journey Friday - Explored ElastiCache (Redis \u0026amp; Memcached)\n- Provisioned a basic Redis cluster\n- Tested connection from EC2 to store/read cache keys 26/09/2025 26/09/2025 AWS Journey 3. Technical Implementation Details 3.1 Amazon CloudFront – CDN Integration Created a distribution pointing to the S3 bucket created in Week 2. Configured Origin Access Control (OAC) to restrict S3 access to CloudFront only. Enabled HTTPS using the default CloudFront certificate. Tested performance improvement (reduced latency) compared to direct S3 access. Executed manual invalidation for index.html: aws cloudfront create-invalidation --distribution-id \u0026lt;ID\u0026gt; --paths \u0026#34;/*\u0026#34; 3.2 Amazon DynamoDB – NoSQL Implementation Created a Users table with UserId as the Partition Key. Performed CRUD operations (Create, Read, Update, Delete) via the Management Console. Interacted via AWS CLI to insert data: aws dynamodb put-item \\ --table-name Users \\ --item \u0026#39;{\u0026#34;UserId\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;u-101\u0026#34;}, \u0026#34;Name\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;Alice\u0026#34;}, \u0026#34;Role\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;Admin\u0026#34;}}\u0026#39; Validated data insertion using the scan command. 3.3 Amazon ElastiCache – Redis Setup Launched an ElastiCache for Redis cluster (cache.t2.micro/t3.micro). Configured Security Groups to allow inbound traffic on port 6379 from the EC2 instance. Connected from EC2 using redis-cli (installed via amazon-linux-extras or yum). Tested caching logic: set mykey \u0026#34;Hello AWS\u0026#34; get mykey # Output: \u0026#34;Hello AWS\u0026#34; 4. Achievements By the end of Week 3, the following outcomes were accomplished:\n✔ Functional Successes Successfully accelerated the S3 static website using CloudFront globally. Demonstrated working knowledge of NoSQL data structures. Established a functional Redis cache cluster accessible from private VPC resources. Integrated AWS CLI for database management, moving beyond Console-only operations. ✔ Skill Development Understood the role of edge locations and caching strategies. Mastered the difference between Relational (RDS) and NoSQL (DynamoDB) models. Learned to perform Cache Invalidation when updating static content. Gained experience in securing internal cache layers (ElastiCache) via Security Groups. 5. Challenges Encountered \u0026amp; Resolutions Challenge 1: CloudFront Content Not Updating\nIssue: Updates to index.html on S3 were not reflecting immediately on the website. Fix: Learned about TTL (Time To Live) and performed a CloudFront Invalidation to force a refresh. Challenge 2: DynamoDB CLI Syntax Complexity\nIssue: Difficulty formatting JSON payloads correctly for CLI commands. Fix: Used a JSON generator tool and referred to AWS CLI documentation for correct AttributeValue syntax (S, N, etc.). Challenge 3: Connecting to Redis from Local Machine\nIssue: Attempted to connect to ElastiCache from outside the VPC (failed). Fix: Understood that ElastiCache is VPC-internal only; utilized the EC2 bastion host established in Week 2 as a jump box. "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/1-worklog/1.4-week4/",
	"title": "Week 4 Worklog",
	"tags": [],
	"description": "",
	"content": "📘 Week 4 Worklog – AWS Journey 1. Weekly Objectives During Week 4, the primary focus shifted towards Migration strategies and Business Continuity. The goal was to understand how to move on-premise workloads to the cloud and ensure system resilience against failures. Key objectives included:\nMigration Processes – Understanding the \u0026ldquo;6 Rs\u0026rdquo; of migration (Rehost, Replatform, Refactor, etc.). AWS Database Migration Service (DMS) – Moving data from a source database to Amazon RDS with minimal downtime. Elastic Disaster Recovery (EDR) – Implementing replication and recovery strategies to minimize data loss. Disaster Recovery (DR) Planning – Defining RTO (Recovery Time Objective) and RPO (Recovery Point Objective). This week establishes the critical skills needed for enterprise-grade infrastructure reliability and modernization.\n2. Detailed Work Summary 🗂 Table of Activities Day Tasks Start Date Completion Date Reference Monday - Learn Migration concepts (Lift \u0026amp; Shift, Replatform, Refactor)\n- Introduction to AWS Database Migration Service (DMS) 29/09/2025 29/09/2025 AWS Journey Tuesday - Practice creating Replication Instance in DMS\n- Configure source data (simulated on-prem) and target (RDS)\n- Perform test data migration 30/09/2025 30/09/2025 AWS Journey Wednesday - Introduction to Elastic Disaster Recovery (EDR)\n- Learn how to set up replication server and recovery instance 01/10/2025 01/10/2025 AWS Journey Thursday - Practice simulating failures: shut down main EC2 and start recovery instance from EDR\n- Evaluate recovery time (RTO/RPO) 02/10/2025 02/10/2025 AWS Journey Friday - Create basic Disaster Recovery plan (backup, restore, failover)\n- Write documentation summarizing Migration + DR processes 03/10/2025 03/10/2025 AWS Journey 3. Technical Implementation Details 3.1 AWS Database Migration Service (DMS) Replication Instance: Provisioned a dms.t2.micro instance in the VPC to handle the migration workload. Endpoints Configuration: Source: Configured an EC2-hosted MySQL database (simulating on-premise) with public access. Target: Connected to the RDS MySQL instance created in Week 2. Migration Task: Created a \u0026ldquo;Full Load\u0026rdquo; task to migrate existing tables. Mapping Rules: Configured schema selection rules to include specific tables (e.g., Users, Products). 3.2 Elastic Disaster Recovery (EDR) Setup Initialized the EDR service in the specific AWS Region. Agent Installation: Downloaded and installed the AWS Replication Agent on the source EC2 instance (Linux). Staging Area: Verified that the replication server was automatically launched in the staging subnet. Data Replication: Monitored the initial sync progress until the status reached \u0026ldquo;Healthy\u0026rdquo; and \u0026ldquo;Data replicated\u0026rdquo;. 3.3 Failover Simulation (Drill) Scenario: Simulating a critical failure by stopping the Source EC2 instance. Recovery Action: Initiated a \u0026ldquo;Recovery Drill\u0026rdquo; in the EDR console. Launch Settings: Configured the launch template (instance type, security groups) for the recovery instance. Validation: Successfully SSH\u0026rsquo;d into the launched Recovery Instance and verified application data integrity. 3.4 DR Planning \u0026amp; Documentation Drafted a basic DR plan outlining: Backup Strategy: Automated snapshots vs. Continuous Replication. Failover Steps: The sequence of actions to switch to the recovery site. RTO/RPO Analysis: Measured how long the recovery took (RTO) and how much data was potential lag (RPO). 4. Achievements By the end of Week 4, the following outcomes were accomplished:\n✔ Functional Successes Successfully migrated data between two database endpoints using AWS DMS. Configured continuous block-level replication using Elastic Disaster Recovery. Executed a successful failover drill, bringing up a recovery server within minutes. Verified data consistency between Source and Target systems. ✔ Skill Development Understood the complete Migration lifecycle (Assess → Mobilize → Migrate \u0026amp; Modernize). Gained practical experience with Hybrid Cloud networking concepts (Source → AWS). Deepened knowledge of Business Continuity Planning (BCP). Learned to differentiate between Backup strategies and Disaster Recovery solutions. 5. Challenges Encountered \u0026amp; Resolutions Challenge 1: DMS Connection Errors\nIssue: The Replication Instance could not connect to the Source EC2 database. Fix: Updated the Source Security Group to allow inbound traffic on port 3306 specifically from the Private IP of the DMS Replication Instance. Challenge 2: EDR Agent Installation Failure\nIssue: The replication agent failed to install due to missing IAM permissions. Fix: Created an IAM user with the specific programmatic access keys required for the AWS Replication Agent and re-ran the installer. Challenge 3: High RTO during Drill\nIssue: The recovery instance took longer than expected to become available. Fix: Optimized the Launch Template to use a pre-warmed AMI or appropriate instance type to speed up the boot process. "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/1-worklog/1.5-week5/",
	"title": "Week 5 Worklog",
	"tags": [],
	"description": "",
	"content": "📘 Week 5 Worklog – AWS Journey 1. Weekly Objectives During Week 5, the focus shifted from manual operations (\u0026ldquo;ClickOps\u0026rdquo;) to Infrastructure as Code (IaC) and Systems Operations. The goal was to automate resource provisioning and management to ensure consistency and speed. Key objectives included:\nInfrastructure as Code (IaC) – Learning AWS CloudFormation and AWS Cloud Development Kit (CDK). Automation – Writing templates and code to deploy S3 buckets and EC2 instances programmatically. AWS Systems Manager (SSM) – Centralizing operational data and managing instances without SSH keys. Operational Excellence – Implementing automated start/stop workflows for cost saving. This week marks the transition towards DevOps practices, preparing for scalable infrastructure management.\n2. Detailed Work Summary 🗂 Table of Activities Day Tasks Start Date Completion Date Reference Monday - Introduction to IaC concepts and benefits compared to manual deployment\n- Get familiar with AWS CloudFormation: template, stack, parameter 06/10/2025 06/10/2025 AWS Journey Tuesday - Write CloudFormation template to deploy S3 bucket and EC2 instance\n- Create, update, and delete stack via AWS Console 07/10/2025 07/10/2025 AWS Journey Wednesday - Introduction to AWS CDK (Cloud Development Kit)\n- Install AWS CDK, create CDK project using Python or TypeScript\n- Write CDK code to deploy EC2 instance 08/10/2025 08/10/2025 AWS Journey Thursday - Introduction to AWS Systems Manager (SSM) and key features\n- Create Parameter Store to store configuration variables 09/10/2025 09/10/2025 AWS Journey Friday - Practice creating Automation Document in SSM to automatically start/stop EC2\n- Test Session Manager (access EC2 without SSH key)\n- Week summary: IaC + SSM demo 10/10/2025 10/10/2025 AWS Journey 3. Technical Implementation Details 3.1 AWS CloudFormation Template Design: Created a YAML template defining an AWS::S3::Bucket and AWS::EC2::Instance. Parameters: Used Parameters to allow inputting InstanceType (e.g., t2.micro) at deployment time. Stack Operations: Create Stack: Uploaded the template to CloudFormation Designer. Update Stack: Modified the template (added tags) and applied a changeset. Drift Detection: Checked if resources were modified manually outside the stack. 3.2 AWS CDK (Cloud Development Kit) Setup: Installed Node.js and the CDK CLI (npm install -g aws-cdk). Initialization: Created a new project: cdk init app --language python. Coding: Defined resources using high-level constructs (L2 constructs) in Python. Deployment: cdk synth: Generated the CloudFormation template. cdk deploy: Provisioned resources to the AWS account. 3.3 AWS Systems Manager (SSM) Parameter Store: Created hierarchical parameters (e.g., /dev/db/password) as SecureString to store sensitive config. Session Manager: Attached the AmazonSSMManagedInstanceCore IAM role to the EC2 instance. Successfully connected to the instance shell via the AWS Console (browser) without opening port 22 (SSH). Automation: Executed an SSM Document (AWS-StopEC2Instance) to test automated operational tasks. 4. Achievements By the end of Week 5, the following outcomes were accomplished:\n✔ Functional Successes Successfully replaced manual resource creation with repeatable CloudFormation templates. Deployed a functional infrastructure stack using imperative code (CDK). Eliminated the need for SSH keys management using SSM Session Manager. Centralized configuration management using SSM Parameter Store. ✔ Skill Development Understood the difference between Declarative (CloudFormation) and Imperative (CDK) IaC approaches. Learned the importance of Idempotency in infrastructure deployment. Gained experience in Agent-based management (SSM Agent). Improved security posture by removing the need for public SSH access. 5. Challenges Encountered \u0026amp; Resolutions Challenge 1: CloudFormation YAML Indentation\nIssue: Stack creation failed due to parsing errors in the YAML file. Fix: Used a YAML Linter and VS Code extension \u0026ldquo;CloudFormation Linter\u0026rdquo; to validate syntax before uploading. Challenge 2: CDK Bootstrapping\nIssue: cdk deploy failed with an error about missing toolkit stack. Fix: Learned that the environment must be bootstrapped once per region using cdk bootstrap aws://\u0026lt;account-id\u0026gt;/\u0026lt;region\u0026gt;. Challenge 3: SSM Agent Not Connecting\nIssue: EC2 instance did not appear in the Systems Manager Fleet Manager. Fix: Discovered that the EC2 instance was missing the necessary IAM Role (AmazonSSMManagedInstanceCore). Attached the role and rebooted the instance. "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/1-worklog/1.6-week6/",
	"title": "Week 6 Worklog",
	"tags": [],
	"description": "",
	"content": "📘 Week 6 Worklog – AWS Journey 1. Weekly Objectives During Week 6, the focus shifted to the critical pillars of the Well-Architected Framework: Security and Cost Optimization. Key objectives included:\nIdentity \u0026amp; Access Management (IAM) – Mastering advanced policy structures to enforce the Principle of Least Privilege. Data Security – Implementing encryption using AWS KMS and managing credentials with Secrets Manager. Cost Management – Analyzing spending patterns via Cost Explorer and setting up automated alerts with AWS Budgets. This week ensures that the infrastructure built in previous weeks is not only functional but also secure and financially efficient.\n2. Detailed Work Summary 🗂 Table of Activities Day Tasks Start Date Completion Date Reference Monday - Review basic IAM knowledge\n- Learn advanced IAM Policy (JSON structure, Conditions)\n- Create custom policies and attach to users/groups 13/10/2025 13/10/2025 AWS Journey Tuesday - Introduction to AWS Key Management Service (KMS)\n- Create a Customer Managed Key (CMK)\n- Apply KMS to encrypt an S3 bucket or an EBS volume 14/10/2025 14/10/2025 AWS Journey Wednesday - Get familiar with AWS Secrets Manager\n- Create a secret to store Database connection info\n- Write a small Lambda script to read the secret 15/10/2025 15/10/2025 AWS Journey Thursday - Explore AWS Billing Dashboard and Cost Explorer\n- View costs by service, region, and usage type\n- Set up Cost Anomaly Detection 16/10/2025 16/10/2025 AWS Journey Friday - Create an AWS Budget and configure email alerts\n- Write a weekly cost summary report with optimization proposals (stop EC2, cleanup EBS)\n- Wrap up Week 6 learnings 17/10/2025 17/10/2025 AWS Journey 3. Technical Implementation Details 3.1 Advanced IAM Policies Analyzed the JSON structure: Version, Statement, Effect, Action, Resource. Created a Condition-based policy to restrict access based on source IP or tags: { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:*\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::my-secure-bucket/*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;IpAddress\u0026#34;: {\u0026#34;aws:SourceIp\u0026#34;: \u0026#34;203.0.113.0/24\u0026#34;} } } Attached inline policies to specific IAM Groups to enforce separation of duties. 3.2 Data Encryption with AWS KMS Created a Customer Managed Key (CMK) (Symmetric). Configured Key Policy to define Key Administrators and Key Users. Enabled default encryption on an S3 bucket using the new CMK. Tested manual encryption via CLI: aws kms encrypt --key-id \u0026lt;key-id\u0026gt; --plaintext fileb://data.txt --output text --query CiphertextBlob 3.3 AWS Secrets Manager Integration Stored RDS credentials (username/password) securely in Secrets Manager. Configured automatic rotation settings (explored concept). Developed a Python (Boto3) script for Lambda to retrieve the secret programmatically, avoiding hardcoded credentials in code. 3.4 Cost Management \u0026amp; Optimization Cost Explorer: Activated tags (e.g., Project: WebApp) to filter costs by specific workloads. AWS Budgets: Set up a monthly budget of $10.00 with an alert triggering at 80% usage ($8.00). Anomaly Detection: Enabled AWS Cost Anomaly Detection to identify spikes in service usage (e.g., unintended Lambda loops). 4. Achievements By the end of Week 6, the following outcomes were accomplished:\n✔ Functional Successes Mastered creating and applying granular IAM Policies to strictly control resource access. Successfully implemented encryption at rest for S3 and EBS using KMS. Replaced hardcoded database credentials with dynamic retrieval from Secrets Manager. Established a cost governance framework using Budgets and Alerts. ✔ Skill Development Deepened understanding of Shared Responsibility Model regarding security. Learned to balance security stringency with operational usability. Gained \u0026ldquo;FinOps\u0026rdquo; awareness: how to analyze costs, propose optimization measures (e.g., stopping idle EC2, deleting unattached EBS), and maintain efficiency. 5. Challenges Encountered \u0026amp; Resolutions Challenge 1: Access Denied due to KMS Key Policy\nIssue: An IAM user with Admin permissions could not decrypt a file. Fix: Learned that KMS Key Policies are separate from IAM policies. Added the user ARN to the \u0026ldquo;Key Users\u0026rdquo; section of the KMS policy. Challenge 2: Secrets Manager Caching\nIssue: Frequent calls to Secrets Manager increased costs/latency. Fix: Implemented caching in the Lambda function code to store the secret temporarily during the execution context. Challenge 3: Interpreting Cost Explorer Data\nIssue: Costs for \u0026ldquo;EC2-Other\u0026rdquo; were high and unclear. Fix: Drilled down by \u0026ldquo;Usage Type\u0026rdquo; to identify that the costs were from NAT Gateway data transfer, leading to an architectural review. "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/1-worklog/1.7-week7/",
	"title": "Week 7 Worklog",
	"tags": [],
	"description": "",
	"content": "📘 Week 7 Worklog – AWS Journey 1. Weekly Objectives During Week 7, the focus was on building High Availability (HA) and Scalability into the architecture. Key objectives included:\nAuto Scaling \u0026amp; Load Balancing – Configuring systems to handle varying traffic loads automatically using ASG and ALB. Decoupling Architecture – Using SQS and SNS to enable asynchronous communication between microservices. Network Monitoring – Enhancing observability by capturing and analyzing network traffic with VPC Flow Logs. This week transforms a static infrastructure into a dynamic, resilient system capable of self-healing and scaling based on demand.\n2. Detailed Work Summary 🗂 Table of Activities Day Tasks Start Date Completion Date Reference Monday - Learn about High Availability, Fault Tolerance, and Elasticity concepts\n- Introduction to Auto Scaling Group (ASG) and Elastic Load Balancer (ELB) 20/10/2025 20/10/2025 AWS Journey Tuesday - Practice creating Auto Scaling Group for EC2 instance\n- Set up launch template, scaling policy, and target tracking 21/10/2025 21/10/2025 AWS Journey Wednesday - Create and configure Application Load Balancer (ALB)\n- Connect ALB with ASG for load distribution\n- Test website access through ALB DNS 22/10/2025 22/10/2025 AWS Journey Thursday - Get familiar with Amazon SQS and SNS services\n- Create SQS queue, SNS topic, and subscription\n- Send and receive notifications between components 23/10/2025 23/10/2025 AWS Journey Friday - Enable VPC Flow Logs to monitor network traffic\n- Analyze logs in CloudWatch Logs\n- Summarize knowledge about reliability \u0026amp; scaling 24/10/2025 24/10/2025 AWS Journey 3. Technical Implementation Details 3.1 Auto Scaling Group (ASG) Launch Template: Created a template defining the AMI, Instance Type (t2.micro), and Security Groups. Scaling Policies: Implemented Target Tracking Scaling Policy to maintain average CPU utilization at 50%. Capacity: Configured Min: 2, Max: 4, Desired: 2 to ensure high availability across multiple Availability Zones. 3.2 Application Load Balancer (ALB) Target Group: Created a target group for HTTP (Port 80) traffic. Listener Rules: Configured the ALB to listen on HTTP and forward traffic to the ASG Target Group. Health Checks: Configured /index.html as the health check path to ensure only healthy instances receive traffic. DNS: Validated access using the auto-generated ALB DNS name (my-loadbalancer-123.region.elb.amazonaws.com). 3.3 Decoupling with SQS \u0026amp; SNS SNS (Simple Notification Service): Created a Topic (OrderAlerts) and subscribed an email address for notifications. SQS (Simple Queue Service): Created a Standard Queue (OrderQueue). Fan-out Pattern: Subscribed the SQS queue to the SNS topic. Published a message to SNS and verified it appeared in both the Email inbox and the SQS queue polling. 3.4 VPC Flow Logs \u0026amp; Monitoring Setup: Enabled Flow Logs for the VPC, sending data to CloudWatch Logs. Analysis: Used CloudWatch Log Insights to query traffic: Identified rejected traffic (Security Group blocks). Monitored SSH connection attempts. Analyzed internal traffic flow between subnets. 4. Achievements By the end of Week 7, the following outcomes were accomplished:\n✔ Functional Successes Understood the High Availability model and how to maintain system uptime during failures. Successfully implemented Auto Scaling Group + Load Balancer to automatically scale EC2 capacity. Configured SQS/SNS for reliable message queuing and fan-out notifications. Enabled and interpreted VPC Flow Logs to audit network security and connectivity. ✔ Skill Development Mastered the relationship between Load Balancers and Auto Scaling Groups. Learned to simulate load (using stress tool) to trigger scaling events. Gained experience in \u0026ldquo;Loose Coupling\u0026rdquo; architecture design. Improved network troubleshooting skills using log analysis. 5. Challenges Encountered \u0026amp; Resolutions Challenge 1: Unhealthy Targets in ALB\nIssue: EC2 instances in the Target Group showed status \u0026ldquo;Unhealthy\u0026rdquo;. Fix: The Security Group on the EC2 instances did not allow traffic from the Load Balancer\u0026rsquo;s Security Group. Updated the Inbound Rules to allow HTTP from the ALB SG. Challenge 2: ASG Not Scaling Down\nIssue: After load testing, instances remained running longer than expected. Fix: Understood the Cooldown Period concept. The ASG was waiting for the default cooldown (300 seconds) before terminating instances to prevent \u0026ldquo;thrashing\u0026rdquo;. Challenge 3: SQS Message Visibility\nIssue: Messages were processed but reappeared in the queue. Fix: Adjusted the Visibility Timeout to match the processing time of the consumer application. "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/1-worklog/1.8-week8/",
	"title": "Week 8 Worklog",
	"tags": [],
	"description": "",
	"content": "📘 Week 8 Worklog – AWS Journey 1. Weekly Objectives Week 8 served as a comprehensive review and consolidation period. The primary goal was to synthesize all knowledge gained over the past 7 weeks through the lens of the AWS Well-Architected Framework. Key objectives included:\nWell-Architected Framework: Deep understanding of the 5 pillars (Operational Excellence, Security, Reliability, Performance Efficiency, Cost Optimization). Architecture Consolidation: Reviewing best practices for Security (IAM, KMS), Resilience (Multi-AZ, DR), and Optimization. Holistic Design: Designing a complete infrastructure integrating core services (EC2, S3, RDS, VPC, Lambda, CloudFront) and evaluating it against AWS standards. This week transitions from learning individual services to designing robust, production-ready systems.\n2. Detailed Work Summary 🗂 Table of Activities Day Tasks Start Date Completion Date Reference Monday - Overview of AWS Well-Architected Framework \u0026amp; 5 Pillars\n- Identify the role and importance of each pillar in system design 27/10/2025 27/10/2025 AWS Journey Tuesday - Review Secure Architecture Design\n- Deep dive: IAM, MFA, SCP, KMS, WAF, Shield, GuardDuty, Security Groups vs NACLs 28/10/2025 28/10/2025 AWS Journey Wednesday - Review Resilient Architecture Design\n- Topics: Multi-AZ, Multi-Region, DR Strategies, Route 53, Backup \u0026amp; Restore 29/10/2025 29/10/2025 AWS Journey Thursday - Review Performance and Cost Optimization\n- Topics: Auto Scaling, Global Accelerator, S3 Tiering, Savings Plans 30/10/2025 30/10/2025 AWS Journey Friday - Comprehensive Practice: Build a sample full-stack architecture\n- Evaluate according to 5 Well-Architected Framework criteria\n- Write weekly summary report 31/10/2025 31/10/2025 AWS Journey 3. Technical Implementation Details 3.1 Security Architecture Review (The Security Pillar) Defense in Depth: Designed a multi-layer security model: Edge: AWS WAF \u0026amp; Shield (DDoS protection). VPC: Public/Private subnets, NACLs (Stateless), Security Groups (Stateful). Identity: IAM Users with MFA, Roles for service access, Principle of Least Privilege. Data: KMS for encryption at rest (EBS/S3/RDS), TLS/ACM for encryption in transit. 3.2 Reliability \u0026amp; Resilience (The Reliability Pillar) High Availability (HA): Architected a Multi-AZ deployment for EC2 (via ASG) and RDS (Primary/Standby). Disaster Recovery (DR): Reviewed the 4 DR strategies: Backup \u0026amp; Restore (Cheapest, highest RTO). Pilot Light. Warm Standby. Multi-Site Active/Active (Most expensive, lowest RTO). 3.3 Performance \u0026amp; Cost (Efficiency \u0026amp; Optimization Pillars) Performance: Implemented CloudFront for edge caching and Global Accelerator for optimized routing to minimize latency. Cost: Analyzed S3 Lifecycle Policies (Standard -\u0026gt; IA -\u0026gt; Glacier) to automate storage savings. Evaluated Compute Savings Plans vs. Reserved Instances for long-term workloads. Used AWS Cost Explorer to identify \u0026ldquo;zombie\u0026rdquo; resources (unattached EIPs, idle ELBs). 3.4 Capstone Architecture Practice Designed a 3-Tier Web Application: Presentation Tier: CloudFront + S3 (Static assets) / ALB (Dynamic requests). Logic Tier: EC2 Auto Scaling Group in Private Subnets. Data Tier: RDS Multi-AZ + DynamoDB. Conducted a self-assessment using the AWS Well-Architected Tool to identify risks (High/Medium Risk Issues). 4. Achievements By the end of Week 8, the following outcomes were accomplished:\n✔ Conceptual Mastery Gained deep understanding and systematized knowledge of the AWS Well-Architected Framework. Able to articulate trade-offs between Cost, Performance, and Reliability. ✔ Technical Consolidation Consolidated 4 core architecture domains: Security, Resilience, Performance, and Cost Optimization. Mastered the interaction between core services (e.g., how CloudWatch triggers Lambda for remediation). ✔ Practical Application Practiced designing a complete, industry-standard infrastructure from scratch. Learned to perform self-evaluation and architectural reviews. 5. Challenges Encountered \u0026amp; Resolutions Challenge 1: Trade-off Analysis\nIssue: Difficulty choosing between \u0026ldquo;Maximum Performance\u0026rdquo; and \u0026ldquo;Lowest Cost\u0026rdquo; (e.g., DynamoDB Provisioned vs. On-Demand). Resolution: Used the Well-Architected Framework to prioritize business requirements (e.g., if traffic is unpredictable, On-Demand is better despite potentially higher unit cost). Challenge 2: Complexity of DR Strategies\nIssue: Confusing the nuances between \u0026ldquo;Pilot Light\u0026rdquo; and \u0026ldquo;Warm Standby\u0026rdquo;. Resolution: Created a comparison matrix focusing on RTO/RPO targets and active resource count to distinguish them clearly. Challenge 3: Security Group vs. NACL Conflicts\nIssue: Troubleshooting connectivity issues when NACLs blocked return traffic (ephemeral ports). Resolution: Reinforced the understanding that NACLs are stateless and require explicit allow rules for both inbound and outbound traffic. "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/1-worklog/1.9-week9/",
	"title": "Week 9 Worklog",
	"tags": [],
	"description": "",
	"content": "📘 Week 9 Worklog – AWS Journey 1. Weekly Objectives During Week 9, the focus expanded into the Data \u0026amp; Analytics domain. The primary goal was to understand the end-to-end data pipeline on AWS, from ingestion to visualization. Key objectives included:\nData Lake Architecture – Building a scalable storage repository using Amazon S3. ETL \u0026amp; Data Cataloging – Using AWS Glue to discover and catalog metadata. Serverless Querying – Analyzing data directly in S3 using Amazon Athena (SQL). Business Intelligence (BI) – Visualizing insights using Amazon QuickSight. This week demonstrates how to turn raw data into actionable business intelligence using serverless analytics tools.\n2. Detailed Work Summary 🗂 Table of Activities Day Tasks Start Date Completion Date Reference Monday - Introduction to Data \u0026amp; Analytics ecosystem on AWS\n- Understand Data Lake concepts, ETL pipeline, and how to connect data sources 03/11/2025 03/11/2025 AWS Journey Tuesday - Create Data Lake on Amazon S3\n- Configure directory structure, access permissions\n- Set up AWS Glue Crawler to identify data schema 04/11/2025 04/11/2025 AWS Journey Wednesday - Practice AWS Athena to query data in Data Lake\n- Write basic SQL queries and export results to S3 05/11/2025 05/11/2025 AWS Journey Thursday - Introduction and practice with Amazon QuickSight\n- Connect QuickSight with Athena\n- Create simple dashboard with charts and tables 06/11/2025 06/11/2025 AWS Journey Friday - Review \u0026amp; consolidate weekly knowledge (Data collection → processing → analysis)\n- Compare Glue/Athena vs traditional tools\n- Write summary report 07/11/2025 07/11/2025 AWS Journey 3. Technical Implementation Details 3.1 Data Lake Setup (Amazon S3) Storage Strategy: Created an S3 bucket with a logical folder structure (e.g., raw-data/, processed-data/). Data Ingestion: Uploaded sample datasets (CSV/JSON files representing sales logs) to the raw-data folder. Security: Applied \u0026ldquo;Block Public Access\u0026rdquo; and ensured IAM roles were ready for Glue and Athena access. 3.2 Metadata Discovery (AWS Glue) Glue Crawler: Configured a Crawler to scan the S3 bucket. IAM Role: Created a service role granting Glue permissions to read S3 and write to the Glue Data Catalog. Data Catalog: Successfully ran the crawler, which automatically detected the schema (columns, data types) and created a table definition in the Glue Database. 3.3 Serverless Querying (Amazon Athena) Configuration: Set up a query result location in S3 (s3://my-bucket/athena-results/). SQL Operations: Executed standard SQL queries against the Glue table: SELECT product_category, SUM(amount) as total_sales FROM sales_data GROUP BY product_category; Verification: Verified that Athena could query the CSV data directly without loading it into a database server. 3.4 Data Visualization (Amazon QuickSight) Dataset Creation: Selected Athena as the data source and imported the table created in the previous steps. SPICE: Imported data into SPICE (Super-fast, Parallel, In-memory Calculation Engine) for faster rendering. Dashboarding: Built a dashboard containing: Bar Chart: Sales by Region. Pie Chart: Customer Demographics. KPI: Total Revenue. 4. Achievements By the end of Week 9, the following outcomes were accomplished:\n✔ Functional Successes Successfully built a Serverless Data Lake using S3. Automated schema discovery using AWS Glue Crawlers. Performed ad-hoc SQL analysis using Athena without provisioning servers. Created a visual BI Dashboard in QuickSight to present data insights. ✔ Skill Development Understood the separation of Compute (Athena) and Storage (S3). Gained experience with Schema-on-Read concepts vs. traditional Schema-on-Write. Learned IAM permission management between Analytics services (QuickSight \u0026lt;-\u0026gt; Athena \u0026lt;-\u0026gt; S3). 5. Challenges Encountered \u0026amp; Resolutions Challenge 1: Athena Output Location Error\nIssue: Query failed with \u0026ldquo;No output location provided\u0026rdquo;. Fix: Configured the \u0026ldquo;Query Result Location\u0026rdquo; in Athena settings to point to a valid S3 bucket folder. Challenge 2: QuickSight Permissions\nIssue: QuickSight could not access the S3 bucket data. Fix: Went to QuickSight \u0026ldquo;Manage QuickSight\u0026rdquo; \u0026gt; \u0026ldquo;Security \u0026amp; Permissions\u0026rdquo; and explicitly ticked the checkbox for the S3 bucket containing the data. Challenge 3: Glue Crawler Classification\nIssue: Crawler classified CSV data incorrectly due to header issues. Fix: Edited the custom classifier in Glue to properly recognize the CSV header row. "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/1-worklog/",
	"title": "Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1: Getting Familiar with AWS and Core Services\nWeek 2: Building a Static Website with S3 and Connecting to RDS\nWeek 3: Optimizing Performance with CloudFront, DynamoDB, and ElastiCache\nWeek 4: Migration and Disaster Recovery with DMS and EDR\nWeek 5: Infrastructure as Code with CloudFormation, CDK, and Systems Manager\nWeek 6: Security and Cost Management with IAM Policies, KMS, Secrets Manager, Billing Dashboard, and AWS Budgets\nWeek 7: Enhancing System Scalability with Auto Scaling, Load Balancer, SQS/SNS, and VPC Flow Logs\nWeek 8: Reviewing the AWS Well-Architected Framework and Consolidating Core Service Knowledge\nWeek 9: Hands-on with Data \u0026amp; Analytics Services: Data Lake with S3, Glue, Athena, and QuickSight\nWeek 10: Hands-on with AI/ML Services: SageMaker, Rekognition, Comprehend, and Kendra\nWeek 11: Proposal Writing and Capstone Project Completion\nWeek 12: Capstone Project – Knowledge Wrap-up and Building a Complete AWS Solution\n"
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/5-workshop/5.4-s3-onprem/5.4.2-create-interface-enpoint/",
	"title": "Create an S3 Interface endpoint",
	"tags": [],
	"description": "",
	"content": "In this section you will create and test an S3 interface endpoint using the simulated on-premises environment deployed as part of this workshop.\nReturn to the Amazon VPC menu. In the navigation pane, choose Endpoints, then click Create Endpoint.\nIn Create endpoint console:\nName the interface endpoint In Service category, choose aws services In the Search box, type S3 and press Enter. Select the endpoint named com.amazonaws.us-east-1.s3. Ensure that the Type column indicates Interface. For VPC, select VPC Cloud from the drop-down. Make sure to choose \u0026ldquo;VPC Cloud\u0026rdquo; and not \u0026ldquo;VPC On-prem\u0026rdquo;\nExpand Additional settings and ensure that Enable DNS name is not selected (we will use this in the next part of the workshop) Select 2 subnets in the following AZs: us-east-1a and us-east-1b For Security group, choose SGforS3Endpoint: Keep the default policy - full access and click Create endpoint Congratulation on successfully creating S3 interface endpoint. In the next step, we will test the interface endpoint.\n"
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/4-eventparticipated/4.2-event2/",
	"title": "Event 2",
	"tags": [],
	"description": "",
	"content": "Workshop: Data Science on AWS Time: 9:30 – 11:45, October 16, 2025 Location: Hall A - FPT University HCMC (FPTU HCMC) Role: Attendee\nObjectives Understand the complete Landscape of AWS AI/ML services. Learn how to build, train, and deploy real-world Machine Learning models. Network with experts from the AWS User Group community. Speakers Van Hoang Kha - Cloud Solutions Architect, AWS User Group Leader Bach Doan Vuong - Cloud DevOps Engineer, AWS Community Builder Key Highlights 1. The AWS AI/ML Stack The speakers clearly introduced the 3-layer structure of the AWS AI/ML Stack:\nAI Services (Top layer): Ready-to-use services requiring no deep ML expertise (Vision, Speech, Chatbots, etc.). ML Services (Middle layer): Amazon SageMaker platform for building, training, and deploying models. Frameworks \u0026amp; Infrastructure (Bottom layer): For experts needing deep control (PyTorch, TensorFlow, EC2 GPU\u0026hellip;). 2. Featured AI Services \u0026amp; Demos I found this section incredibly interesting due to its high applicability for student projects:\nVision: Amazon Rekognition for face recognition, video analysis, PPE Detection, and Content Moderation. Speech \u0026amp; Audio: The combination of Amazon Polly (Text-to-Speech with lifelike voices) and Amazon Transcribe (Speech-to-Text, supporting real-time calls). Natural Language Processing (NLP): Amazon Translate: Fast, multi-language translation. Amazon Textract: Quickly extract data from scanned documents/invoices. Amazon Lex: Build intelligent conversational Chatbots. Personalization: Amazon Personalize creates real-time product recommendations without needing deep ML expertise. 3. Machine Learning Workflow with SageMaker The speakers demonstrated the process from Feature Engineering (raw data processing) to Training and Tuning parameters. Specifically, they showed how to bring existing Python code (Scikit-learn, TensorFlow) to run on AWS\u0026rsquo;s powerful infrastructure.\nPersonal Experience This workshop was very practical and relevant for students like me.\nI was particularly impressed by the sharing on Amazon Personalize. I used to think building a Recommendation System was extremely difficult and required complex algorithms, but AWS makes it much simpler. A huge plus was the speaker\u0026rsquo;s practical advice on Cost Management. The \u0026ldquo;Monitoring Cost Daily\u0026rdquo; slide made me realize that working with Cloud isn\u0026rsquo;t just about making code run, but also about optimizing costs to avoid \u0026ldquo;wallet shock\u0026rdquo; at the end of the month. The atmosphere at FPTU was energetic. Hearing experiences from industry seniors gave me more motivation to study for my upcoming AWS certifications. Event Photos Summary: This event truly \u0026ldquo;enlightened\u0026rdquo; me about the AI Services toolkit. Instead of building models from scratch, I now know how to leverage AWS\u0026rsquo;s available APIs to solve problems faster and more effectively.\n"
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/5-workshop/5.2-prerequiste/",
	"title": "Prerequiste",
	"tags": [],
	"description": "",
	"content": "IAM permissions Add the following IAM permission policy to your user account to deploy and cleanup this workshop.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cloudformation:*\u0026#34;, \u0026#34;cloudwatch:*\u0026#34;, \u0026#34;ec2:AcceptTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:AcceptTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:AllocateAddress\u0026#34;, \u0026#34;ec2:AssociateAddress\u0026#34;, \u0026#34;ec2:AssociateIamInstanceProfile\u0026#34;, \u0026#34;ec2:AssociateRouteTable\u0026#34;, \u0026#34;ec2:AssociateSubnetCidrBlock\u0026#34;, \u0026#34;ec2:AssociateTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:AssociateVpcCidrBlock\u0026#34;, \u0026#34;ec2:AttachInternetGateway\u0026#34;, \u0026#34;ec2:AttachNetworkInterface\u0026#34;, \u0026#34;ec2:AttachVolume\u0026#34;, \u0026#34;ec2:AttachVpnGateway\u0026#34;, \u0026#34;ec2:AuthorizeSecurityGroupEgress\u0026#34;, \u0026#34;ec2:AuthorizeSecurityGroupIngress\u0026#34;, \u0026#34;ec2:CreateClientVpnEndpoint\u0026#34;, \u0026#34;ec2:CreateClientVpnRoute\u0026#34;, \u0026#34;ec2:CreateCustomerGateway\u0026#34;, \u0026#34;ec2:CreateDhcpOptions\u0026#34;, \u0026#34;ec2:CreateFlowLogs\u0026#34;, \u0026#34;ec2:CreateInternetGateway\u0026#34;, \u0026#34;ec2:CreateLaunchTemplate\u0026#34;, \u0026#34;ec2:CreateNetworkAcl\u0026#34;, \u0026#34;ec2:CreateNetworkInterface\u0026#34;, \u0026#34;ec2:CreateNetworkInterfacePermission\u0026#34;, \u0026#34;ec2:CreateRoute\u0026#34;, \u0026#34;ec2:CreateRouteTable\u0026#34;, \u0026#34;ec2:CreateSecurityGroup\u0026#34;, \u0026#34;ec2:CreateSubnet\u0026#34;, \u0026#34;ec2:CreateSubnetCidrReservation\u0026#34;, \u0026#34;ec2:CreateTags\u0026#34;, \u0026#34;ec2:CreateTransitGateway\u0026#34;, \u0026#34;ec2:CreateTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:CreateTransitGatewayPrefixListReference\u0026#34;, \u0026#34;ec2:CreateTransitGatewayRoute\u0026#34;, \u0026#34;ec2:CreateTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:CreateTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:CreateVpc\u0026#34;, \u0026#34;ec2:CreateVpcEndpoint\u0026#34;, \u0026#34;ec2:CreateVpcEndpointConnectionNotification\u0026#34;, \u0026#34;ec2:CreateVpcEndpointServiceConfiguration\u0026#34;, \u0026#34;ec2:CreateVpnConnection\u0026#34;, \u0026#34;ec2:CreateVpnConnectionRoute\u0026#34;, \u0026#34;ec2:CreateVpnGateway\u0026#34;, \u0026#34;ec2:DeleteCustomerGateway\u0026#34;, \u0026#34;ec2:DeleteFlowLogs\u0026#34;, \u0026#34;ec2:DeleteInternetGateway\u0026#34;, \u0026#34;ec2:DeleteNetworkInterface\u0026#34;, \u0026#34;ec2:DeleteNetworkInterfacePermission\u0026#34;, \u0026#34;ec2:DeleteRoute\u0026#34;, \u0026#34;ec2:DeleteRouteTable\u0026#34;, \u0026#34;ec2:DeleteSecurityGroup\u0026#34;, \u0026#34;ec2:DeleteSubnet\u0026#34;, \u0026#34;ec2:DeleteSubnetCidrReservation\u0026#34;, \u0026#34;ec2:DeleteTags\u0026#34;, \u0026#34;ec2:DeleteTransitGateway\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayPrefixListReference\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayRoute\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:DeleteVpc\u0026#34;, \u0026#34;ec2:DeleteVpcEndpoints\u0026#34;, \u0026#34;ec2:DeleteVpcEndpointServiceConfigurations\u0026#34;, \u0026#34;ec2:DeleteVpnConnection\u0026#34;, \u0026#34;ec2:DeleteVpnConnectionRoute\u0026#34;, \u0026#34;ec2:Describe*\u0026#34;, \u0026#34;ec2:DetachInternetGateway\u0026#34;, \u0026#34;ec2:DisassociateAddress\u0026#34;, \u0026#34;ec2:DisassociateRouteTable\u0026#34;, \u0026#34;ec2:GetLaunchTemplateData\u0026#34;, \u0026#34;ec2:GetTransitGatewayAttachmentPropagations\u0026#34;, \u0026#34;ec2:ModifyInstanceAttribute\u0026#34;, \u0026#34;ec2:ModifySecurityGroupRules\u0026#34;, \u0026#34;ec2:ModifyTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:ModifyVpcAttribute\u0026#34;, \u0026#34;ec2:ModifyVpcEndpoint\u0026#34;, \u0026#34;ec2:ReleaseAddress\u0026#34;, \u0026#34;ec2:ReplaceRoute\u0026#34;, \u0026#34;ec2:RevokeSecurityGroupEgress\u0026#34;, \u0026#34;ec2:RevokeSecurityGroupIngress\u0026#34;, \u0026#34;ec2:RunInstances\u0026#34;, \u0026#34;ec2:StartInstances\u0026#34;, \u0026#34;ec2:StopInstances\u0026#34;, \u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsEgress\u0026#34;, \u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsIngress\u0026#34;, \u0026#34;iam:AddRoleToInstanceProfile\u0026#34;, \u0026#34;iam:AttachRolePolicy\u0026#34;, \u0026#34;iam:CreateInstanceProfile\u0026#34;, \u0026#34;iam:CreatePolicy\u0026#34;, \u0026#34;iam:CreateRole\u0026#34;, \u0026#34;iam:DeleteInstanceProfile\u0026#34;, \u0026#34;iam:DeletePolicy\u0026#34;, \u0026#34;iam:DeleteRole\u0026#34;, \u0026#34;iam:DeleteRolePolicy\u0026#34;, \u0026#34;iam:DetachRolePolicy\u0026#34;, \u0026#34;iam:GetInstanceProfile\u0026#34;, \u0026#34;iam:GetPolicy\u0026#34;, \u0026#34;iam:GetRole\u0026#34;, \u0026#34;iam:GetRolePolicy\u0026#34;, \u0026#34;iam:ListPolicyVersions\u0026#34;, \u0026#34;iam:ListRoles\u0026#34;, \u0026#34;iam:PassRole\u0026#34;, \u0026#34;iam:PutRolePolicy\u0026#34;, \u0026#34;iam:RemoveRoleFromInstanceProfile\u0026#34;, \u0026#34;lambda:CreateFunction\u0026#34;, \u0026#34;lambda:DeleteFunction\u0026#34;, \u0026#34;lambda:DeleteLayerVersion\u0026#34;, \u0026#34;lambda:GetFunction\u0026#34;, \u0026#34;lambda:GetLayerVersion\u0026#34;, \u0026#34;lambda:InvokeFunction\u0026#34;, \u0026#34;lambda:PublishLayerVersion\u0026#34;, \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:DeleteLogGroup\u0026#34;, \u0026#34;logs:DescribeLogGroups\u0026#34;, \u0026#34;logs:PutRetentionPolicy\u0026#34;, \u0026#34;route53:ChangeTagsForResource\u0026#34;, \u0026#34;route53:CreateHealthCheck\u0026#34;, \u0026#34;route53:CreateHostedZone\u0026#34;, \u0026#34;route53:CreateTrafficPolicy\u0026#34;, \u0026#34;route53:DeleteHostedZone\u0026#34;, \u0026#34;route53:DisassociateVPCFromHostedZone\u0026#34;, \u0026#34;route53:GetHostedZone\u0026#34;, \u0026#34;route53:ListHostedZones\u0026#34;, \u0026#34;route53domains:ListDomains\u0026#34;, \u0026#34;route53domains:ListOperations\u0026#34;, \u0026#34;route53domains:ListTagsForDomain\u0026#34;, \u0026#34;route53resolver:AssociateResolverEndpointIpAddress\u0026#34;, \u0026#34;route53resolver:AssociateResolverRule\u0026#34;, \u0026#34;route53resolver:CreateResolverEndpoint\u0026#34;, \u0026#34;route53resolver:CreateResolverRule\u0026#34;, \u0026#34;route53resolver:DeleteResolverEndpoint\u0026#34;, \u0026#34;route53resolver:DeleteResolverRule\u0026#34;, \u0026#34;route53resolver:DisassociateResolverEndpointIpAddress\u0026#34;, \u0026#34;route53resolver:DisassociateResolverRule\u0026#34;, \u0026#34;route53resolver:GetResolverEndpoint\u0026#34;, \u0026#34;route53resolver:GetResolverRule\u0026#34;, \u0026#34;route53resolver:ListResolverEndpointIpAddresses\u0026#34;, \u0026#34;route53resolver:ListResolverEndpoints\u0026#34;, \u0026#34;route53resolver:ListResolverRuleAssociations\u0026#34;, \u0026#34;route53resolver:ListResolverRules\u0026#34;, \u0026#34;route53resolver:ListTagsForResource\u0026#34;, \u0026#34;route53resolver:UpdateResolverEndpoint\u0026#34;, \u0026#34;route53resolver:UpdateResolverRule\u0026#34;, \u0026#34;s3:AbortMultipartUpload\u0026#34;, \u0026#34;s3:CreateBucket\u0026#34;, \u0026#34;s3:DeleteBucket\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34;, \u0026#34;s3:GetAccountPublicAccessBlock\u0026#34;, \u0026#34;s3:GetBucketAcl\u0026#34;, \u0026#34;s3:GetBucketOwnershipControls\u0026#34;, \u0026#34;s3:GetBucketPolicy\u0026#34;, \u0026#34;s3:GetBucketPolicyStatus\u0026#34;, \u0026#34;s3:GetBucketPublicAccessBlock\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:GetObjectVersion\u0026#34;, \u0026#34;s3:GetBucketVersioning\u0026#34;, \u0026#34;s3:ListAccessPoints\u0026#34;, \u0026#34;s3:ListAccessPointsForObjectLambda\u0026#34;, \u0026#34;s3:ListAllMyBuckets\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;s3:ListBucketMultipartUploads\u0026#34;, \u0026#34;s3:ListBucketVersions\u0026#34;, \u0026#34;s3:ListJobs\u0026#34;, \u0026#34;s3:ListMultipartUploadParts\u0026#34;, \u0026#34;s3:ListMultiRegionAccessPoints\u0026#34;, \u0026#34;s3:ListStorageLensConfigurations\u0026#34;, \u0026#34;s3:PutAccountPublicAccessBlock\u0026#34;, \u0026#34;s3:PutBucketAcl\u0026#34;, \u0026#34;s3:PutBucketPolicy\u0026#34;, \u0026#34;s3:PutBucketPublicAccessBlock\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;secretsmanager:CreateSecret\u0026#34;, \u0026#34;secretsmanager:DeleteSecret\u0026#34;, \u0026#34;secretsmanager:DescribeSecret\u0026#34;, \u0026#34;secretsmanager:GetSecretValue\u0026#34;, \u0026#34;secretsmanager:ListSecrets\u0026#34;, \u0026#34;secretsmanager:ListSecretVersionIds\u0026#34;, \u0026#34;secretsmanager:PutResourcePolicy\u0026#34;, \u0026#34;secretsmanager:TagResource\u0026#34;, \u0026#34;secretsmanager:UpdateSecret\u0026#34;, \u0026#34;sns:ListTopics\u0026#34;, \u0026#34;ssm:DescribeInstanceProperties\u0026#34;, \u0026#34;ssm:DescribeSessions\u0026#34;, \u0026#34;ssm:GetConnectionStatus\u0026#34;, \u0026#34;ssm:GetParameters\u0026#34;, \u0026#34;ssm:ListAssociations\u0026#34;, \u0026#34;ssm:ResumeSession\u0026#34;, \u0026#34;ssm:StartSession\u0026#34;, \u0026#34;ssm:TerminateSession\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Provision resources using CloudFormation In this lab, we will use N.Virginia region (us-east-1).\nTo prepare the workshop environment, deploy this CloudFormation Template (click link): PrivateLinkWorkshop . Accept all of the defaults when deploying the template.\nTick 2 acknowledgement boxes Choose Create stack The ClouddFormation deployment requires about 15 minutes to complete.\n2 VPCs have been created 3 EC2s have been created "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/2-proposal/",
	"title": "Proposal",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nIn this section, you need to summarize the contents of the workshop that you plan to conduct.\nIoT Weather Platform for Lab Research A Unified AWS Serverless Solution for Real-Time Weather Monitoring 1. Executive Summary The IoT Weather Platform is designed for the ITea Lab team in Ho Chi Minh City to enhance weather data collection and analysis. It supports up to 5 weather stations, with potential scalability to 10-15, utilizing Raspberry Pi edge devices with ESP32 sensors to transmit data via MQTT. The platform leverages AWS Serverless services to deliver real-time monitoring, predictive analytics, and cost efficiency, with access restricted to 5 lab members via Amazon Cognito.\n2. Problem Statement What’s the Problem? Current weather stations require manual data collection, becoming unmanageable with multiple units. There is no centralized system for real-time data or analytics, and third-party platforms are costly and overly complex.\nThe Solution The platform uses AWS IoT Core to ingest MQTT data, AWS Lambda and API Gateway for processing, Amazon S3 for storage (including a data lake), and AWS Glue Crawlers and ETL jobs to extract, transform, and load data from the S3 data lake to another S3 bucket for analysis. AWS Amplify with Next.js provides the web interface, and Amazon Cognito ensures secure access. Similar to Thingsboard and CoreIoT, users can register new devices and manage connections, though this platform operates on a smaller scale and is designed for private use. Key features include real-time dashboards, trend analysis, and low operational costs.\nBenefits and Return on Investment The solution establishes a foundational resource for lab members to develop a larger IoT platform, serving as a study resource, and provides a data foundation for AI enthusiasts for model training or analysis. It reduces manual reporting for each station via a centralized platform, simplifying management and maintenance, and improves data reliability. Monthly costs are $0.66 USD per the AWS Pricing Calculator, with a 12-month total of $7.92 USD. All IoT equipment costs are covered by the existing weather station setup, eliminating additional development expenses. The break-even period of 6-12 months is achieved through significant time savings from reduced manual work.\n3. Solution Architecture The platform employs a serverless AWS architecture to manage data from 5 Raspberry Pi-based stations, scalable to 15. Data is ingested via AWS IoT Core, stored in an S3 data lake, and processed by AWS Glue Crawlers and ETL jobs to transform and load it into another S3 bucket for analysis. Lambda and API Gateway handle additional processing, while Amplify with Next.js hosts the dashboard, secured by Cognito. The architecture is detailed below:\nAWS Services Used AWS IoT Core: Ingests MQTT data from 5 stations, scalable to 15. AWS Lambda: Processes data and triggers Glue jobs (two functions). Amazon API Gateway: Facilitates web app communication. Amazon S3: Stores raw data in a data lake and processed outputs (two buckets). AWS Glue: Crawlers catalog data, and ETL jobs transform and load it. AWS Amplify: Hosts the Next.js web interface. Amazon Cognito: Secures access for lab users. Component Design Edge Devices: Raspberry Pi collects and filters sensor data, sending it to IoT Core. Data Ingestion: AWS IoT Core receives MQTT messages from the edge devices. Data Storage: Raw data is stored in an S3 data lake; processed data is stored in another S3 bucket. Data Processing: AWS Glue Crawlers catalog the data, and ETL jobs transform it for analysis. Web Interface: AWS Amplify hosts a Next.js app for real-time dashboards and analytics. User Management: Amazon Cognito manages user access, allowing up to 5 active accounts. 4. Technical Implementation Implementation Phases This project has two parts—setting up weather edge stations and building the weather platform—each following 4 phases:\nBuild Theory and Draw Architecture: Research Raspberry Pi setup with ESP32 sensors and design the AWS serverless architecture (1 month pre-internship) Calculate Price and Check Practicality: Use AWS Pricing Calculator to estimate costs and adjust if needed (Month 1). Fix Architecture for Cost or Solution Fit: Tweak the design (e.g., optimize Lambda with Next.js) to stay cost-effective and usable (Month 2). Develop, Test, and Deploy: Code the Raspberry Pi setup, AWS services with CDK/SDK, and Next.js app, then test and release to production (Months 2-3). Technical Requirements\nWeather Edge Station: Sensors (temperature, humidity, rainfall, wind speed), a microcontroller (ESP32), and a Raspberry Pi as the edge device. Raspberry Pi runs Raspbian, handles Docker for filtering, and sends 1 MB/day per station via MQTT over Wi-Fi. Weather Platform: Practical knowledge of AWS Amplify (hosting Next.js), Lambda (minimal use due to Next.js), AWS Glue (ETL), S3 (two buckets), IoT Core (gateway and rules), and Cognito (5 users). Use AWS CDK/SDK to code interactions (e.g., IoT Core rules to S3). Next.js reduces Lambda workload for the fullstack web app. 5. Timeline \u0026amp; Milestones Project Timeline\nPre-Internship (Month 0): 1 month for planning and old station review. Internship (Months 1-3): 3 months. Month 1: Study AWS and upgrade hardware. Month 2: Design and adjust architecture. Month 3: Implement, test, and launch. Post-Launch: Up to 1 year for research. 6. Budget Estimation You can find the budget estimation on the AWS Pricing Calculator.\nOr you can download the Budget Estimation File.\nInfrastructure Costs AWS Services: AWS Lambda: $0.00/month (1,000 requests, 512 MB storage). S3 Standard: $0.15/month (6 GB, 2,100 requests, 1 GB scanned). Data Transfer: $0.02/month (1 GB inbound, 1 GB outbound). AWS Amplify: $0.35/month (256 MB, 500 ms requests). Amazon API Gateway: $0.01/month (2,000 requests). AWS Glue ETL Jobs: $0.02/month (2 DPUs). AWS Glue Crawlers: $0.07/month (1 crawler). MQTT (IoT Core): $0.08/month (5 devices, 45,000 messages). Total: $0.7/month, $8.40/12 months\nHardware: $265 one-time (Raspberry Pi 5 and sensors). 7. Risk Assessment Risk Matrix Network Outages: Medium impact, medium probability. Sensor Failures: High impact, low probability. Cost Overruns: Medium impact, low probability. Mitigation Strategies Network: Local storage on Raspberry Pi with Docker. Sensors: Regular checks and spares. Cost: AWS budget alerts and optimization. Contingency Plans Revert to manual methods if AWS fails. Use CloudFormation for cost-related rollbacks. 8. Expected Outcomes Technical Improvements: Real-time data and analytics replace manual processes.\nScalable to 10-15 stations.\nLong-term Value 1-year data foundation for AI research.\nReusable for future projects.\n"
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/5-workshop/5.3-s3-vpc/5.3.2-test-gwe/",
	"title": "Test the Gateway Endpoint",
	"tags": [],
	"description": "",
	"content": "Create S3 bucket Navigate to S3 management console In the Bucket console, choose Create bucket In the Create bucket console Name the bucket: choose a name that hasn\u0026rsquo;t been given to any bucket globally (hint: lab number and your name) Leave other fields as they are (default) Scroll down and choose Create bucket Successfully create S3 bucket. Connect to EC2 with session manager For this workshop, you will use AWS Session Manager to access several EC2 instances. Session Manager is a fully managed AWS Systems Manager capability that allows you to manage your Amazon EC2 instances and on-premises virtual machines (VMs) through an interactive one-click browser-based shell. Session Manager provides secure and auditable instance management without the need to open inbound ports, maintain bastion hosts, or manage SSH keys.\nFirst cloud journey Lab for indepth understanding of Session manager.\nIn the AWS Management Console, start typing Systems Manager in the quick search box and press Enter: From the Systems Manager menu, find Node Management in the left menu and click Session Manager: Click Start Session, and select the EC2 instance named Test-Gateway-Endpoint. This EC2 instance is already running in \u0026ldquo;VPC Cloud\u0026rdquo; and will be used to test connectivity to Amazon S3 through the Gateway endpoint you just created (s3-gwe).\nSession Manager will open a new browser tab with a shell prompt: sh-4.2 $\nYou have successfully start a session - connect to the EC2 instance in VPC cloud. In the next step, we will create a S3 bucket and a file in it.\nCreate a file and upload to s3 bucket Change to the ssm-user\u0026rsquo;s home directory by typing cd ~ in the CLI Create a new file to use for testing with the command fallocate -l 1G testfile.xyz, which will create a file of 1GB size named \u0026ldquo;testfile.xyz\u0026rdquo;. Upload file to S3 bucket with command aws s3 cp testfile.xyz s3://your-bucket-name. Replace your-bucket-name with the name of S3 bucket that you created earlier. You have successfully uploaded the file to your S3 bucket. You can now terminate the session.\nCheck object in S3 bucket Navigate to S3 console. Click the name of your s3 bucket In the Bucket console, you will see the file you have uploaded to your S3 bucket Section summary Congratulation on completing access to S3 from VPC. In this section, you created a Gateway endpoint for Amazon S3, and used the AWS CLI to upload an object. The upload worked because the Gateway endpoint allowed communication to S3, without needing an Internet Gateway attached to \u0026ldquo;VPC Cloud\u0026rdquo;. This demonstrates the functionality of the Gateway endpoint as a secure path to S3 without traversing the Public Internet.\n"
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/1-worklog/1.10-week10/",
	"title": "Week 10 Worklog",
	"tags": [],
	"description": "",
	"content": "📘 Week 10 Worklog – AWS Journey 1. Weekly Objectives During Week 10, the exploration extended into the Artificial Intelligence (AI) and Machine Learning (ML) domain of AWS. The primary goal was to distinguish between building custom models and using pre-trained AI services. Key objectives included:\nAWS AI Ecosystem – Understanding the landscape of ML services (SageMaker) vs. AI Services (Rekognition, Comprehend, Kendra). Amazon SageMaker – Experiencing the full ML lifecycle: Build, Train, and Deploy. Computer Vision \u0026amp; NLP – Implementing image analysis and natural language processing without deep data science expertise using AWS APIs. Intelligent Search – Setting up enterprise search capabilities with Amazon Kendra. This week highlights how AWS democratizes AI, allowing developers to add intelligence to applications via API calls or build custom models with managed infrastructure.\n2. Detailed Work Summary 🗂 Table of Activities Day Tasks Start Date Completion Date Reference Monday - Overview of AI/ML on AWS\n- Learn about ML support services: SageMaker, Rekognition, Comprehend, Kendra, Translate, Polly 10/11/2025 10/11/2025 AWS Journey Tuesday - Practice with Amazon SageMaker:\n- Create Notebook Instance\n- Train simple models (Linear Regression/Image Classification)\n- Deploy endpoint and test predictions 11/11/2025 11/11/2025 AWS Journey Wednesday - Get familiar with Amazon Rekognition\n- Demo face and object recognition in images/videos\n- Integrate Rekognition API into a small web application 12/11/2025 12/11/2025 AWS Journey Thursday - Practice Amazon Comprehend (NLP)\n- Experiment with Amazon Kendra (contextual intelligent search)\n- Compare advantages and limitations of each service 13/11/2025 13/11/2025 AWS Journey Friday - Summarize Week 10 knowledge (AI/ML development process)\n- Real-world applications of AI/ML in business\n- Write practice results report and expansion directions 14/11/2025 14/11/2025 AWS Journey 3. Technical Implementation Details 3.1 Amazon SageMaker (Custom ML) Notebook Instance: Launched a ml.t2.medium Jupyter Notebook instance. Training: Used the built-in XGBoost algorithm to train a model on a sample dataset (e.g., predicting house prices or MNIST). Deployment: Deployed the trained model to a real-time HTTPS Endpoint. Inference: Invoked the endpoint using Python (Boto3) to generate predictions from new data. 3.2 Amazon Rekognition (Computer Vision) Image Analysis: Uploaded photos to S3 and used the DetectLabels API to identify objects (e.g., \u0026ldquo;Car\u0026rdquo;, \u0026ldquo;Tree\u0026rdquo;, \u0026ldquo;Person\u0026rdquo;) with confidence scores. Facial Analysis: Used DetectFaces to estimate age range, emotion, and gender. Integration: Wrote a simple script that triggers a Lambda function when an image is uploaded to S3 to tag it automatically using Rekognition. 3.3 Amazon Comprehend (NLP) Sentiment Analysis: Processed customer review text to determine sentiment (Positive, Negative, Neutral). Entity Recognition: Extracted specific entities (Dates, Locations, Names) from unstructured text documents. 3.4 Amazon Kendra (Intelligent Search) Index Creation: Created a Kendra Index (Developer Edition). Data Source: Connected an S3 bucket containing PDF manuals as the knowledge base. Querying: Tested natural language queries (e.g., \u0026ldquo;How do I reset my device?\u0026rdquo;) in the search console and received precise answers extracted from the documents. 4. Achievements By the end of Week 10, the following outcomes were accomplished:\n✔ Functional Successes Successfully trained and hosted a Machine Learning model using Amazon SageMaker. Implemented Computer Vision features (Face/Object detection) via API calls. extracted insights from text using Amazon Comprehend. Set up a functional document search engine using Amazon Kendra. ✔ Skill Development Understood the distinction between AI Services (High-level APIs for developers) and ML Services (SageMaker for Data Scientists). Gained experience in Model Inference costs and endpoint management. Learned how to integrate AI capabilities into existing applications using AWS SDKs. 5. Challenges Encountered \u0026amp; Resolutions Challenge 1: SageMaker Cost Management\nIssue: SageMaker Notebooks and Endpoints charge per hour even when idle. Fix: Created a \u0026ldquo;Cleanup Script\u0026rdquo; to delete Endpoints and Stop Notebook instances immediately after lab completion to avoid unexpected bills. Challenge 2: IAM Permissions for Rekognition\nIssue: AccessDeniedException when Rekognition tried to read images from the S3 bucket. Fix: Updated the Bucket Policy and IAM Role to explicitly grant s3:GetObject permission to the user/role calling the Rekognition API. Challenge 3: Kendra Indexing Time\nIssue: Kendra took significant time (30+ minutes) to create an index and sync data. Fix: Learned that Kendra is an enterprise-grade service with provisioning time; planned tasks to allow for \u0026ldquo;waiting time\u0026rdquo; or worked on Comprehend while Kendra was initializing. "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/1-worklog/1.11-week11/",
	"title": "Week 11 Worklog",
	"tags": [],
	"description": "",
	"content": "📘 Week 11 Worklog – AWS Journey 1. Weekly Objectives Week 11 focused on Application Modernization via Serverless Architecture. The primary goal was to shift from managing infrastructure (EC2) to focusing on code and business logic using event-driven services. Key objectives included:\nServerless Paradigm – Understanding the shift from Monolithic to Microservices and the benefits of \u0026ldquo;No-Ops\u0026rdquo;. Core Serverless Stack – Mastering AWS Lambda (Compute), API Gateway (Interface), and DynamoDB (NoSQL Data). Security \u0026amp; Auth – Implementing user identity management with Amazon Cognito. Infrastructure as Code – Using the AWS Serverless Application Model (SAM) to define and deploy serverless resources. This week provides the skillset to build highly scalable, cost-effective applications that scale to zero when not in use.\n2. Detailed Work Summary 🗂 Table of Activities Day Tasks Start Date Completion Date Reference Monday - Introduction to Modernization and Serverless concepts\n- Compare Monolithic vs. Microservices architectures\n- Analyze benefits: Cost, Scalability, Operational overhead 17/11/2025 17/11/2025 AWS Journey Tuesday - Practice AWS Lambda: create functions, configure triggers (S3/APIGW)\n- Deploy basic API processing logic\n- Monitor logs via CloudWatch Logs 18/11/2025 18/11/2025 AWS Journey Wednesday - Integrate API Gateway with Lambda (REST API)\n- Connect logic to DynamoDB (CRUD operations)\n- Test API endpoints using Postman 19/11/2025 19/11/2025 AWS Journey Thursday - Configure Cognito for user authentication (User Pool)\n- Integrate Cognito Authorizer into API Gateway\n- Manage access permissions via IAM Roles 20/11/2025 20/11/2025 AWS Journey Friday - Practice deploying a complete Serverless App using AWS SAM\n- Testing, logging, and performance optimization\n- Summarize knowledge and weekly report 21/11/2025 21/11/2025 AWS Journey 3. Technical Implementation Details 3.1 AWS Lambda \u0026amp; Logic Runtime: Created Python 3.9 Lambda functions. Handler: Implemented the lambda_handler(event, context) function to parse JSON input. Triggers: Configured API Gateway as the event source. Logging: Used print() statements to send structured logs to CloudWatch for debugging execution errors. 3.2 API Gateway \u0026amp; DynamoDB Integration API Type: Built a REST API. Integration: Used Lambda Proxy Integration to pass the full HTTP request object to the function. Database: Created a DynamoDB table (Items) with ItemId as the Partition Key. Used boto3 in Lambda to perform put_item, get_item, and scan operations based on HTTP methods (POST, GET). 3.3 Security with Amazon Cognito User Pool: Created a User Pool to handle sign-up and sign-in. App Client: Generated a client ID for the application. Authorizer: Configured a Cognito User Pool Authorizer in API Gateway. Validation: Verified that API requests without a valid Authorization (JWT) token were rejected with 401 Unauthorized. 3.4 AWS SAM (Serverless Application Model) Template: Defined resources (Function, API, Table) in template.yaml. Build: Ran sam build to compile dependencies. Deploy: executed sam deploy --guided to package code to S3 and create the CloudFormation stack automatically. Local Testing: Used sam local invoke to test functions before deployment. 4. Achievements By the end of Week 11, the following outcomes were accomplished:\n✔ Functional Successes Transitioned from managing servers to deploying functions. Built a fully functional Serverless CRUD API. Secured API endpoints using JWT Tokens from Cognito. Automated deployment using AWS SAM, replacing manual console actions. ✔ Skill Development Thoroughly understood the Event-Driven Architecture model. Learned to handle Stateless compute limitations. Mastered the process of decomposing a problem into microservices (Auth service, Data service, Logic service). 5. Challenges Encountered \u0026amp; Resolutions Challenge 1: CORS Errors\nIssue: Calling the API from a browser frontend resulted in Cross-Origin Resource Sharing errors. Fix: Enabled CORS in API Gateway settings and ensured the Lambda function returned the Access-Control-Allow-Origin: * header in the response object. Challenge 2: Lambda Permissions\nIssue: AccessDeniedException when Lambda tried to write to DynamoDB. Fix: Updated the Lambda Execution Role (IAM) to include dynamodb:PutItem and dynamodb:GetItem permissions for the specific table ARN. Challenge 3: Cold Starts\nIssue: The first API call after a period of inactivity took 2-3 seconds. Fix: Acknowledged this as a trade-off of Serverless. Optimized imports in the Python code to reduce initialization time. 6. Plan for Next Week (Preview of Week 12) Comprehensive Review: Consolidate all 11 weeks of knowledge. Real-world Project: Design and implementation of a final Capstone Project. Final Report: Documentation and presentation of the AWS Journey. Certification Prep: Roadmap for AWS Certified Solutions Architect Associate (SAA). 5. Challenges Encountered \u0026amp; Resolutions Challenge 1: SageMaker Cost Management\nIssue: SageMaker Notebooks and Endpoints charge per hour even when idle. Fix: Created a \u0026ldquo;Cleanup Script\u0026rdquo; to delete Endpoints and Stop Notebook instances immediately after lab completion to avoid unexpected bills. Challenge 2: IAM Permissions for Rekognition\nIssue: AccessDeniedException when Rekognition tried to read images from the S3 bucket. Fix: Updated the Bucket Policy and IAM Role to explicitly grant s3:GetObject permission to the user/role calling the Rekognition API. Challenge 3: Kendra Indexing Time\nIssue: Kendra took significant time (30+ minutes) to create an index and sync data. Fix: Learned that Kendra is an enterprise-grade service with provisioning time; planned tasks to allow for \u0026ldquo;waiting time\u0026rdquo; or worked on Comprehend while Kendra was initializing. "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/1-worklog/1.12-week12/",
	"title": "Week 12 Worklog",
	"tags": [],
	"description": "",
	"content": "📘 Week 12 Worklog – AWS Journey 1. Weekly Objectives Week 12 marked the conclusion of the AWS learning roadmap. The primary objective was to synthesize and apply all skills acquired over the past 11 weeks into a comprehensive Capstone Project. Key objectives included:\nHolistic Review – Revisiting core services (EC2, VPC, S3, RDS, Lambda) to ensure deep understanding. Capstone Project – Designing, building, and deploying a production-ready \u0026ldquo;E-Commerce Order Processing System\u0026rdquo; from scratch. Operational Excellence – Implementing CI/CD, Monitoring, and Security best practices. Self-Assessment – Evaluating the architecture against the Well-Architected Framework and preparing for certification. This week transitions the status from \u0026ldquo;Learner\u0026rdquo; to \u0026ldquo;Cloud Practitioner/Architect\u0026rdquo; ready for real-world challenges.\n2. Detailed Work Summary 🗂 Table of Activities Day Tasks Start Date Completion Date Reference Monday - Review all core services: EC2, S3, RDS, DynamoDB, IAM, VPC, Lambda, CloudFront\n- Define requirements and architecture for the final project 24/11/2025 24/11/2025 AWS Journey Tuesday - Start project deployment:\n- Design VPC (Public/Private subnets), Security Groups\n- Configure S3 for static assets, CloudFront for CDN 25/11/2025 25/11/2025 AWS Journey Wednesday - Backend implementation:\n- Build Lambda functions and API Gateway resources\n- Connect to DynamoDB/RDS and implement business logic\n- Integrate CloudWatch Logs \u0026amp; Alarms 26/11/2025 26/11/2025 AWS Journey Thursday - Complete the project:\n- Add Cognito authentication (SignUp/SignIn)\n- Finalize CI/CD pipeline (CodePipeline/CodeBuild)\n- Perform End-to-end system testing 27/11/2025 27/11/2025 AWS Journey Friday - Write final report and documentation\n- Prepare presentation (Architecture diagram, Cost analysis, Security review)\n- Summarize the entire journey and self-evaluate 28/11/2025 28/11/2025 AWS Journey 3. Technical Implementation Details (Capstone Project) 3.1 Project Scope: \u0026ldquo;Serverless E-Commerce Backend\u0026rdquo; Architecture Overview: A 3-tier serverless web application. Frontend: Hosted on S3 + CloudFront. Backend: API Gateway + Lambda. Database: DynamoDB (Product Catalog) + RDS MySQL (Order History). Auth: Amazon Cognito. 3.2 Infrastructure Setup VPC Design: Created a custom VPC with 2 Public Subnets (NAT Gateway, Load Balancer) and 2 Private Subnets (Lambda, RDS). Security Groups: Strictly defined rules: RDS only accepts traffic from Lambda SG on port 3306. Lambda only accepts traffic from internal VPC endpoints. 3.3 Application Logic \u0026amp; Data Lambda Functions: Developed Python functions for CreateOrder, GetProducts, and ProcessPayment. Database Integration: Used Boto3 to scan DynamoDB for product availability. Used PyMySQL to transact SQL commands to RDS for order recording. Monitoring: Created a CloudWatch Dashboard to visualize API Latency and Error Rates (4xx/5xx). 3.4 Automation \u0026amp; Operations CI/CD: Built a pipeline using AWS CodePipeline: Source: GitHub. Build: AWS CodeBuild (Runs unit tests). Deploy: CloudFormation/SAM deploy. Cost Optimization: Implemented S3 Lifecycle policies for logs and set up an AWS Budget alert for the project. 4. Achievements By the end of Week 12 (and the entire course), the following outcomes were accomplished:\n✔ Project Success Successfully delivered a production-grade cloud application combining 10+ AWS services. Demonstrated ability to integrate Relational (RDS) and Non-Relational (DynamoDB) data stores in a single system. Achieved a Secure (Cognito/IAM), Reliable (Multi-AZ), and Performant (CloudFront/Caching) architecture. ✔ Professional Growth Mastered the process of building Cloud applications from Requirement Analysis → Design → Implementation → Operation. Developed strong troubleshooting skills for complex distributed systems. Completed the learning roadmap and fully prepared for the AWS Certified Solutions Architect – Associate exam. 5. Challenges Encountered \u0026amp; Resolutions Challenge 1: Lambda in VPC Connectivity\nIssue: Lambda function in Private Subnet lost internet access (could not reach public APIs or DynamoDB). Fix: Configured a NAT Gateway in the Public Subnet and updated Route Tables. Also used a VPC Endpoint for DynamoDB to keep traffic internal and reduce NAT costs. Challenge 2: CodePipeline Build Errors\nIssue: buildspec.yml failed due to missing Python dependencies. Fix: Updated the install phase in the buildspec file to execute pip install -r requirements.txt. Challenge 3: CORS with Cognito\nIssue: API Gateway rejected requests with valid tokens due to CORS headers missing on 401 responses. Fix: Configured \u0026ldquo;Gateway Responses\u0026rdquo; in API Gateway to include CORS headers even for Unauthorized errors. "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/5-workshop/5.3-s3-vpc/",
	"title": "Access S3 from VPC",
	"tags": [],
	"description": "",
	"content": "Using Gateway endpoint In this section, you will create a Gateway eendpoint to access Amazon S3 from an EC2 instance. The Gateway endpoint will allow upload an object to S3 buckets without using the Public Internet. To create an endpoint, you must specify the VPC in which you want to create the endpoint, and the service (in this case, S3) to which you want to establish the connection.\nContent Create gateway endpoint Test gateway endpoint "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/5-workshop/5.4-s3-onprem/5.4.3-test-endpoint/",
	"title": "Test the Interface Endpoint",
	"tags": [],
	"description": "",
	"content": "Get the regional DNS name of S3 interface endpoint From the Amazon VPC menu, choose Endpoints.\nClick the name of newly created endpoint: s3-interface-endpoint. Click details and save the regional DNS name of the endpoint (the first one) to your text-editor for later use.\nConnect to EC2 instance in \u0026ldquo;VPC On-prem\u0026rdquo; Navigate to Session manager by typing \u0026ldquo;session manager\u0026rdquo; in the search box\nClick Start Session, and select the EC2 instance named Test-Interface-Endpoint. This EC2 instance is running in \u0026ldquo;VPC On-prem\u0026rdquo; and will be used to test connectivty to Amazon S3 through the Interface endpoint we just created. Session Manager will open a new browser tab with a shell prompt: sh-4.2 $\nChange to the ssm-user\u0026rsquo;s home directory with command \u0026ldquo;cd ~\u0026rdquo;\nCreate a file named testfile2.xyz\nfallocate -l 1G testfile2.xyz Copy file to the same S3 bucket we created in section 3.2 aws s3 cp --endpoint-url https://bucket.\u0026lt;Regional-DNS-Name\u0026gt; testfile2.xyz s3://\u0026lt;your-bucket-name\u0026gt; This command requires the \u0026ndash;endpoint-url parameter, because you need to use the endpoint-specific DNS name to access S3 using an Interface endpoint. Do not include the leading \u0026rsquo; * \u0026rsquo; when copying/pasting the regional DNS name. Provide your S3 bucket name created earlier Now the file has been added to your S3 bucket. Let check your S3 bucket in the next step.\nCheck Object in S3 bucket Navigate to S3 console Click Buckets Click the name of your bucket and you will see testfile2.xyz has been added to your bucket "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/3-blogstranslated/",
	"title": "Translated Blogs",
	"tags": [],
	"description": "",
	"content": "Blog 1 - Model Customization, RAG, or Both: A Case Study with Amazon Nova In the rapidly evolving Generative AI landscape, choosing the optimal strategy to improve the accuracy of large language models (LLMs) for domain tools can be a big challenge. This paper conducts a case study using Amazon Nova models to compare the performance of three multivariate approaches: Model Fine-Tuning (Fine-Tuning), Retrieval Augmentation Generation (RAG), and Hybrid. Through real-world experiments on the AWS dataset, the paper provides an in-depth analysis of performance, cost, and technical complexity, helping developers and enterprises make the most informed decisions for their AI applications.\nBlog 2 - Migrating CDK Version 1 Applications to CDK Version 2 with Amazon Q Developer With AWS CDK v1 officially retired, developers are faced with an urgent need to upgrade to v2 to ensure security and take advantage of the latest features. This article details how to use Amazon Q Developer – an AI-powered programming assistant – to automate and accelerate this migration. From updating dependencies and modifying imports to debugging and documentation, you will discover how Amazon Q makes infrastructure-as-code (IaC) modernization simple, fast, and less error-prone.\nBlog 3 - Melting The Ice - How Natural Intelligence Simplified a Data Lake Migration to Apache Iceberg Migrating a large-scale, operational Data Lake to a modern tabular format like Apache Iceberg often comes with the risk of system disruption and technical complexity. This article is a success story of Natural Intelligence\u0026rsquo;s zero-downtime migration from Apache Hive to Apache Iceberg. The authors share details of an innovative hybrid migration strategy that uses automated schema and change synchronization (CDC) between the old and new systems. This is a valuable reference on architecture and processes for organizations looking to safely and efficiently modernize their big data platforms.\n"
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/5-workshop/5.4-s3-onprem/",
	"title": "Access S3 from on-premises",
	"tags": [],
	"description": "",
	"content": "Overview In this section, you will create an Interface endpoint to access Amazon S3 from a simulated on-premises environment. The Interface endpoint will allow you to route to Amazon S3 over a VPN connection from your simulated on-premises environment.\nWhy using Interface endpoint:\nGateway endpoints only work with resources running in the VPC where they are created. Interface endpoints work with resources running in VPC, and also resources running in on-premises environments. Connectivty from your on-premises environment to the cloud can be provided by AWS Site-to-Site VPN or AWS Direct Connect. Interface endpoints allow you to connect to services powered by AWS PrivateLink. These services include some AWS services, services hosted by other AWS customers and partners in their own VPCs (referred to as PrivateLink Endpoint Services), and supported AWS Marketplace Partner services. For this workshop, we will focus on connecting to Amazon S3. "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/4-eventparticipated/",
	"title": "Events Participated",
	"tags": [],
	"description": "",
	"content": " In this section, you should list and describe in detail the events you have participated in during your internship or work experience.\nEach event should be presented in the format Event 1, Event 2, Event 3…, along with the following details:\nEvent name Date and time Location (if applicable) Your role in the event (attendee, event support, speaker, etc.) A brief description of the event’s content and main activities Outcomes or value gained (lessons learned, new skills, contribution to the team/project) This listing helps demonstrate your actual participation as well as the soft skills and experience you have gained from each event. During my internship, I participated in two events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: Vietnam Cloud Day 2025 : Ho Chi Minh City Connect Edition for Builders\nDate \u0026amp; Time: Thursday, 18 September 2025, 9:00 – 17:30\nLocation: 36th floor, 2 Hai Trieu Street, Ben Nghe Ward, District 1, Ho Chi Minh City\nRole: Attendee\nDescription: A deep-dive tech event for Builders, focusing on strategic trends such as Agentic AI, Data Foundation solutions, and the AI-Driven Development Lifecycle (AI-DLC). It also covered essential security standards for Generative AI.\nOutcomes: Gained insights into the core mindset and architecture of Agentic AI; deeply understood the AI-DLC process to shift from coding to architecting and reviewing; updated key knowledge on multi-layer security for GenAI applications.\nEvent 2 Event Name: Workshop Data Science on AWS\nDate \u0026amp; Time: 16 October 2025, 9:30 – 11:45\nLocation: Hall A - FPT University HCMC (FPTU HCMC)\nRole: Attendee\nDescription: A practical workshop on Data Science on AWS platform, focusing on the AI/ML ecosystem stack, key AI Services (Vision, Speech, Text), and the model training workflow using Amazon SageMaker.\nOutcomes: Understood the big picture of AWS AI/ML Stack; learned how to quickly integrate AI Services (Rekognition, Polly, Lex) into applications; grasped the Feature Engineering process and cost optimization strategies when running models on the Cloud.\nEvent 3 Event Name: AWS Cloud Mastery Series #1: AI/ML/GenAI on AWS\nDate \u0026amp; Time: Saturday, 15 November 2025, 8:30 – 12:00\nLocation: AWS Vietnam Office, Bitexco Financial Tower, HCMC\nRole: Attendee\nDescription: An intensive series on Generative AI, focusing on Prompt Engineering techniques, RAG (Retrieval-Augmented Generation) architecture, and building AI Agents with Amazon Bedrock.\nOutcomes: Deeply understood the differences between Foundation Models (Claude, Llama, Titan); mastered Chain-of-Thought prompting; learned the Bedrock Agent architecture to build smart chatbots capable of using tools.\nEvent 4 Event Name: AWS Cloud Mastery Series #2: DevOps on AWS\nDate \u0026amp; Time: Monday, 17 November 2025, 8:30 – 17:00\nLocation: AWS Vietnam Office, Bitexco Financial Tower, HCMC\nRole: Attendee\nDescription: A full-day deep dive into DevOps culture and tools on AWS. Content covered building automated CI/CD pipelines (CodePipeline), managing infrastructure as code (IaC) with AWS CDK, Containerization techniques (ECS/EKS), and comprehensive system Observability.\nOutcomes: Shifted mindset from manual administration to \u0026ldquo;Automation is King\u0026rdquo;; mastered the AWS Developer Tools suite to eliminate manual operations; learned how to use AWS CDK to define infrastructure using programming languages; gained a clear understanding of Microservices architecture and safe deployment strategies (Blue/Green).\nEvent 5 Event Name: AWS Cloud Mastery Series #3: AWS Well-Architected Security Pillar\nDate \u0026amp; Time: Saturday, 29 November 2025, 8:30 – 12:00\nLocation: AWS Vietnam Office, Bitexco Financial Tower, HCMC\nRole: Attendee\nDescription: A workshop dedicated to the Security Pillar, covering Zero Trust principles, Modern Identity and Access Management (IAM), Detection-as-Code, and Automated Incident Response.\nOutcomes: Adopted the \u0026ldquo;No ClickOps\u0026rdquo; mindset in security (configuration must be code); deeply understood the Shared Responsibility Model; learned to use EventBridge and Lambda to build systems that automatically remediate vulnerabilities upon detection; gained confidence in designing multi-layered secure network architectures.\n"
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/5-workshop/5.4-s3-onprem/5.4.4-dns-simulation/",
	"title": "On-premises DNS Simulation",
	"tags": [],
	"description": "",
	"content": "AWS PrivateLink endpoints have a fixed IP address in each Availability Zone where they are deployed, for the life of the endpoint (until it is deleted). These IP addresses are attached to Elastic Network Interfaces (ENIs). AWS recommends using DNS to resolve the IP addresses for endpoints so that downstream applications use the latest IP addresses when ENIs are added to new AZs, or deleted over time.\nIn this section, you will create a forwarding rule to send DNS resolution requests from a simulated on-premises environment to a Route 53 Private Hosted Zone. This section leverages the infrastructure deployed by CloudFormation in the Prepare the environment section.\nCreate DNS Alias Records for the Interface endpoint Navigate to the Route 53 management console (Hosted Zones section). The CloudFormation template you deployed in the Prepare the environment section created this Private Hosted Zone. Click on the name of the Private Hosted Zone, s3.us-east-1.amazonaws.com: Create a new record in the Private Hosted Zone: Record name and record type keep default options Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor (you saved when doing section 4.3) Click Add another record, and add a second record using the following values. Click Create records when finished to create both records. Record name: *. Record type: keep default value (type A) Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor The new records appear in the Route 53 console:\nCreate a Resolver Forwarding Rule Route 53 Resolver Forwarding Rules allow you to forward DNS queries from your VPC to other sources for name resolution. Outside of a workshop environment, you might use this feature to forward DNS queries from your VPC to DNS servers running on-premises. In this section, you will simulate an on-premises conditional forwarder by creating a forwarding rule that forwards DNS queries for Amazon S3 to a Private Hosted Zone running in \u0026ldquo;VPC Cloud\u0026rdquo; in-order to resolve the PrivateLink interface endpoint regional DNS name.\nFrom the Route 53 management console, click Inbound endpoints on the left side bar In the Inbound endpoints console, click the ID of the inbound endpoint Copy the two IP addresses listed to your text editor From the Route 53 menu, choose Resolver \u0026gt; Rules, and click Create rule: In the Create rule console: Name: myS3Rule Rule type: Forward Domain name: s3.us-east-1.amazonaws.com VPC: VPC On-prem Outbound endpoint: VPCOnpremOutboundEndpoint Target IP Addresses: Enter both IP addresses from your text editor (inbound endpoint addresses) and then click Submit You have successfully created resolver forwarding rule.\nTest the on-premises DNS Simulation Connect to Test-Interface-Endpoint EC2 instance with Session manager Test DNS resolution. The dig command will return the IP addresses assigned to the VPC Interface endpoint running in VPC Cloud (your IP\u0026rsquo;s will be different): dig +short s3.us-east-1.amazonaws.com The IP addresses returned are the VPC endpoint IP addresses, NOT the Resolver IP addresses you pasted from your text editor. The IP addresses of the Resolver endpoint and the VPC endpoint look similar because they are all from the VPC Cloud CIDR block.\nNavigate to the VPC menu (Endpoints section), select the S3 Interface endpoint. Click the Subnets tab and verify that the IP addresses returned by Dig match the VPC endpoint: Return to your shell and use the AWS CLI to test listing your S3 buckets: aws s3 ls --endpoint-url https://s3.us-east-1.amazonaws.com Terminate your Session Manager session: In this section you created an Interface endpoint for Amazon S3. This endpoint can be reached from on-premises through Site-to-Site VPN or AWS Direct Connect. Route 53 Resolver outbound endpoints simulated forwarding DNS requests from on-premises to a Private Hosted Zone running the cloud. Route 53 inbound Endpoints recieved the resolution request and returned a response containing the IP addresses of the VPC interface endpoint. Using DNS to resolve the endpoint IP addresses provides high availability in-case of an Availability Zone outage.\n"
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/5-workshop/5.5-policy/",
	"title": "VPC Endpoint Policies",
	"tags": [],
	"description": "",
	"content": "When you create an interface or gateway endpoint, you can attach an endpoint policy to it that controls access to the service to which you are connecting. A VPC endpoint policy is an IAM resource policy that you attach to an endpoint. If you do not attach a policy when you create an endpoint, AWS attaches a default policy for you that allows full access to the service through the endpoint.\nYou can create a policy that restricts access to specific S3 buckets only. This is useful if you only want certain S3 Buckets to be accessible through the endpoint.\nIn this section you will create a VPC endpoint policy that restricts access to the S3 bucket specified in the VPC endpoint policy.\nConnect to an EC2 instance and verify connectivity to S3 Start a new AWS Session Manager session on the instance named Test-Gateway-Endpoint. From the session, verify that you can list the contents of the bucket you created in Part 1: Access S3 from VPC: aws s3 ls s3://\\\u0026lt;your-bucket-name\\\u0026gt; The bucket contents include the two 1 GB files uploaded in earlier.\nCreate a new S3 bucket; follow the naming pattern you used in Part 1, but add a \u0026lsquo;-2\u0026rsquo; to the name. Leave other fields as default and click create Successfully create bucket\nNavigate to: Services \u0026gt; VPC \u0026gt; Endpoints, then select the Gateway VPC endpoint you created earlier. Click the Policy tab. Click Edit policy. The default policy allows access to all S3 Buckets through the VPC endpoint.\nIn Edit Policy console, copy \u0026amp; Paste the following policy, then replace yourbucketname-2 with your 2nd bucket name. This policy will allow access through the VPC endpoint to your new bucket, but not any other bucket in Amazon S3. Click Save to apply the policy. { \u0026#34;Id\u0026#34;: \u0026#34;Policy1631305502445\u0026#34;, \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;Stmt1631305501021\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:*\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::yourbucketname-2\u0026#34;, \u0026#34;arn:aws:s3:::yourbucketname-2/*\u0026#34; ], \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34; } ] } Successfully customize policy\nFrom your session on the Test-Gateway-Endpoint instance, test access to the S3 bucket you created in Part 1: Access S3 from VPC aws s3 ls s3://\u0026lt;yourbucketname\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy:\nReturn to your home directory on your EC2 instance cd~ Create a file fallocate -l 1G test-bucket2.xyz Copy file to 2nd bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-2nd-bucket-name\u0026gt; This operation succeeds because it is permitted by the VPC endpoint policy.\nThen we test access to the first bucket by copy the file to 1st bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-1st-bucket-name\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy.\nPart 3 Summary: In this section, you created a VPC endpoint policy for Amazon S3, and used the AWS CLI to test the policy. AWS CLI actions targeted to your original S3 bucket failed because you applied a policy that only allowed access to the second bucket you created. AWS CLI actions targeted for your second bucket succeeded because the policy allowed them. These policies can be useful in situations where you need to control access to resources through VPC endpoints.\n"
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/5-workshop/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nSecure Hybrid Access to S3 using VPC Endpoints Overview AWS PrivateLink provides private connectivity to AWS services from VPCs and your on-premises networks, without exposing your traffic to the Public Internet.\nIn this lab, you will learn how to create, configure, and test VPC endpoints that enable your workloads to reach AWS services without traversing the Public Internet.\nYou will create two types of endpoints to access Amazon S3: a Gateway VPC endpoint, and an Interface VPC endpoint. These two types of VPC endpoints offer different benefits depending on if you are accessing Amazon S3 from the cloud or your on-premises location\nGateway - Create a gateway endpoint to send traffic to Amazon S3 or DynamoDB using private IP addresses.You route traffic from your VPC to the gateway endpoint using route tables. Interface - Create an interface endpoint to send traffic to endpoint services that use a Network Load Balancer to distribute traffic. Traffic destined for the endpoint service is resolved using DNS. Content Workshop overview Prerequiste Access S3 from VPC Access S3 from On-premises VPC Endpoint Policies (Bonus) Clean up "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/5-workshop/5.6-cleanup/",
	"title": "Clean up",
	"tags": [],
	"description": "",
	"content": "Congratulations on completing this workshop! In this workshop, you learned architecture patterns for accessing Amazon S3 without using the Public Internet.\nBy creating a gateway endpoint, you enabled direct communication between EC2 resources and Amazon S3, without traversing an Internet Gateway. By creating an interface endpoint you extended S3 connectivity to resources running in your on-premises data center via AWS Site-to-Site VPN or Direct Connect. clean up Navigate to Hosted Zones on the left side of Route 53 console. Click the name of s3.us-east-1.amazonaws.com zone. Click Delete and confirm deletion by typing delete. Disassociate the Route 53 Resolver Rule - myS3Rule from \u0026ldquo;VPC Onprem\u0026rdquo; and Delete it. Open the CloudFormation console and delete the two CloudFormation Stacks that you created for this lab: PLOnpremSetup PLCloudSetup Delete S3 buckets Open S3 console Choose the bucket we created for the lab, click and confirm empty. Click delete and confirm delete. "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/6-self-evaluation/",
	"title": "Self-Assessment",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.\nDuring my internship at [Company/Organization Name] from [start date] to [end date], I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in [briefly describe the main project or task], through which I improved my skills in [list skills: programming, analysis, reporting, communication, etc.].\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ✅ ☐ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ☐ ✅ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ✅ ☐ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ✅ ☐ ☐ 5 Discipline Adhering to schedules, rules, and work processes ☐ ☐ ✅ 6 Progressive mindset Willingness to receive feedback and improve oneself ☐ ✅ ☐ 7 Communication Presenting ideas and reporting work clearly ☐ ✅ ☐ 8 Teamwork Working effectively with colleagues and participating in teams ✅ ☐ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ☐ ✅ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ✅ ☐ ☐ 12 Overall General evaluation of the entire internship period ✅ ☐ ☐ Needs Improvement Strengthen discipline and strictly comply with the rules and regulations of the company or any organization Improve problem-solving thinking Enhance communication skills in both daily interactions and professional contexts, including handling situations effectively "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/7-feedback/",
	"title": "Sharing and Feedback",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nHere, you can freely share your personal opinions about your experience participating in the First Cloud Journey program. This will help the FCJ team improve any shortcomings based on the following aspects:\nOverall Evaluation 1. Working Environment\nThe working environment is very friendly and open. FCJ members are always willing to help whenever I encounter difficulties, even outside working hours. The workspace is tidy and comfortable, helping me focus better. However, I think it would be nice to have more social gatherings or team bonding activities to strengthen relationships.\n2. Support from Mentor / Team Admin\nThe mentor provides very detailed guidance, explains clearly when I don’t understand, and always encourages me to ask questions. The admin team supports administrative tasks, provides necessary documents, and creates favorable conditions for me to work effectively. I especially appreciate that the mentor allows me to try and solve problems myself instead of just giving the answer.\n3. Relevance of Work to Academic Major\nThe tasks I was assigned align well with the knowledge I learned at university, while also introducing me to new areas I had never encountered before. This allowed me to both strengthen my foundational knowledge and gain practical skills.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learned many new skills such as using project management tools, teamwork skills, and professional communication in a corporate environment. The mentor also shared valuable real-world experiences that helped me better plan my career path.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other, works seriously but still keeps things enjoyable. When there are urgent projects, everyone works together and supports one another regardless of their position. This made me feel like a real part of the team, even as an intern.\n6. Internship Policies / Benefits\nThe company provides an internship allowance and offers flexible working hours when needed. In addition, having the opportunity to join internal training sessions is a big plus.\nAdditional Questions What did you find most satisfying during your internship? What do you think the company should improve for future interns? If recommending to a friend, would you suggest they intern here? Why or why not? Suggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience? Would you like to continue this program in the future? Any other comments (free sharing): "
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost:1313/Le-Minh-Tuan-FCJ-AWS-FA25-Hugo/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]